{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Training Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# %pip install bert_race/requirements.txt -qqq\n",
    "%pip install transformers==4.11.3 -qqq\n",
    "!pip install pytorch_pretrained_bert==0.4.0 -qqq\n",
    "# %conda install -c conda-forge ipywidgets\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %pip install transformers --upgrade\n",
    "\n",
    "!pip install wandb -qqq\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from loss import LossCriterion, LOSS_REGISTRY\n",
    "\n",
    "from module.bert_optim import RAdam\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# del variables\n",
    "gc.collect()\n",
    "\n",
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from utils.tokenization_utils import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: hgwhyik9\n",
      "Sweep URL: https://wandb.ai/wassimboubaker/master-thesis/sweeps/hgwhyik9\n"
     ]
    }
   ],
   "source": [
    "bucket = 'support-bert-data'\n",
    "data_location = f's3://{bucket}'\n",
    "\n",
    "\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "      'name': 'regularized batch loss',\n",
    "      'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'max_seq_length': {\n",
    "            'values': [128]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [2, 3]\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [2]\n",
    "        },\n",
    "        'adv_train':{\n",
    "            'values': [0,1]\n",
    "        },\n",
    "        'adv_k': {\n",
    "            'values': [1, 3, 5]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'values': [5e-4, 1e-5, 3e-5, 5e-5]\n",
    "        },\n",
    "        'adv_p_norm': {\n",
    "            'values': ['inf']#, 'l1', 'l2']\n",
    "        },\n",
    "        'adv_alpha': {\n",
    "            'values': [0.1, 1, 3, 5]\n",
    "        },\n",
    "        'adv_loss': {\n",
    "            'values': ['LossCriterion.SymKlCriterion', 'LossCriterion.KlCriterion']\n",
    "        },\n",
    "        'grad_accumulation_step': {\n",
    "            'values': [1, 2]\n",
    "        },\n",
    "        'scheduler_type':{\n",
    "            'values': ['ms','exp']\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'radam']\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"master-thesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r bert_race/requirements.txt\n",
    "# %cp bert_race/pytorch_pretrained_bert/tokenization.py utils/tokenization.py\n",
    "# %cp bert_race/pytorch_pretrained_bert/file_utils.py utils/file_utils.py\n",
    "\n",
    "from utils.tokenization import BertTokenizer \n",
    "from utils.tokenization_utils import convert_examples_to_features\n",
    "from data_utils.utils import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "do_lower_case = True\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# %cp bert_race/pytorch_pretrained_bert/modeling.py utils/modeling.py\n",
    "# %cp bert_race/pytorch_pretrained_bert/optimization.py utils/optimization.py\n",
    "\n",
    "import torch\n",
    "from utils.modeling import BertForMultipleChoice, BertConfig, BertEmbeddings\n",
    "from utils.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from utils.optimization import BertAdam, WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset, IterableDataset)\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tokenization_utils import build_tensor\n",
    "def select_field(features, field):\n",
    "    return [\n",
    "        [\n",
    "            choice[field]\n",
    "            for choice in feature.choices_features\n",
    "        ]\n",
    "        for feature in features\n",
    "    ]\n",
    "\n",
    "def build_tensor(features):\n",
    "    all_input_ids = torch.tensor(select_field(features, 'input_ids'),\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(select_field(features, 'input_mask'),\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(select_field(features, 'segment_ids'),\n",
    "                                   dtype=torch.long)\n",
    "    all_label = torch.tensor([f.label for f in features],\n",
    "                             dtype=torch.long)\n",
    "    return TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "seed = 30\n",
    "do_lower_case = True\n",
    "bert_model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "print(n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare optimizer\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "\n",
    "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#         ]\n",
    "    \n",
    "\n",
    "# global_step = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sorted: support-bert-data/train/0.txt\n",
      "After sorted: support-bert-data/dev/0.txt\n",
      "After sorted: support-bert-data/test/0.txt\n",
      "3289 race_id: support-bert-data/train/0:0, context_sentence: when tech opens info session and when he tries to close it, it hangs and will not close. i have included a screen shot as to what process  hangs in the device manager. i have tried to uninstall and reinstall the pdf application but it did not help, start_ending: what kind of issue occurred?, ending_0: a performance issue., ending_1: [NOA], ending_2: a protocol issue., label: 0\n",
      "0\n",
      "557 race_id: support-bert-data/dev/0:0, context_sentence: [AN56] data is showing as queued, almost all to [AN16], [AN31] seems to transmit okay. please see screen shot and [AN25] requirements log. i also submitted error report from admin client on device., start_ending: how would you describe your issue?, ending_0: the {[AN56]} protocols were transferred to {[AN31]} but could not be saved on your device (pc/{[AN1]})., ending_1: the {[AN56]} protocols are not sent., ending_2: [NOA], label: 1\n",
      "0\n",
      "1025 race_id: support-bert-data/test/0:0, context_sentence: [AN56] data is showing as queued, almost all to [AN16], [AN31] seems to transmit okay. please see screen shot and [AN25] requirements log. i also submitted error report from admin client on device., start_ending: how can we help you?, ending_0: [NOA], ending_1: i have a question or i would like to know how to perform a specific action or task related to the usage of {[AN25]}., ending_2: i have an issue when using {[AN25]} and i need support., label: 2\n",
      "0\n",
      "3289 race_id: support-bert-data/train/0:0, context_sentence: when tech opens info session and when he tries to close it, it hangs and will not close. i have included a screen shot as to what process  hangs in the device manager. i have tried to uninstall and reinstall the pdf application but it did not help, start_ending: what kind of issue occurred?, ending_0: a performance issue., ending_1: [NOA], ending_2: a protocol issue., label: 0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_examples = read_data(data_location,'train')\n",
    "dev_examples = read_data(data_location,'dev')\n",
    "test_examples = read_data(data_location,'test')\n",
    "\n",
    "dev_train_examples = train_examples[:16]\n",
    "\n",
    "def build_dataset(config):\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, tokenizer, config['max_seq_length'], True)\n",
    "    dev_features = convert_examples_to_features(\n",
    "                dev_examples, tokenizer, config['max_seq_length'], True)\n",
    "    test_features = convert_examples_to_features(\n",
    "                test_examples, tokenizer, config['max_seq_length'], True)\n",
    "    dev_train_features = convert_examples_to_features(\n",
    "        train_examples, tokenizer, config['max_seq_length'], True)\n",
    "\n",
    "    train_data = build_tensor(train_features)\n",
    "    dev_data = build_tensor(dev_features)\n",
    "    test_data = build_tensor(test_features)\n",
    "    dev_train_data = build_tensor(dev_train_features)\n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=config['batch_size'])\n",
    "    \n",
    "    eval_sampler = SequentialSampler(dev_data)\n",
    "    eval_dataloader = DataLoader(dev_data,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=2)    \n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data,\n",
    "                                 sampler=test_sampler,\n",
    "                                 batch_size=32)\n",
    "    \n",
    "    dev_train_sampler = SequentialSampler(dev_train_data)\n",
    "    dev_train_dataloader = DataLoader(dev_data,\n",
    "                                 sampler=dev_train_sampler,\n",
    "                                 batch_size=2) \n",
    "    \n",
    "    return train_dataloader, eval_dataloader, test_dataloader, dev_train_dataloader\n",
    "[train_dataloader,\n",
    " eval_dataloader,\n",
    " test_dataloader,\n",
    " dev_train_dataloader] = build_dataset({'max_seq_length': 128,\n",
    "                                        'batch_size':2})\n",
    "\n",
    "model = BertForMultipleChoice.from_pretrained(bert_model,\n",
    "                                              cache_dir=os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(-1)),\n",
    "                                              num_choices=3)\n",
    "    \n",
    "\n",
    "def create_model(device):\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        ln = 24\n",
    "        if name.startswith('bert.encoder'):\n",
    "        \tl = name.split('.')\n",
    "        \tln = int(l[3])\n",
    "      \n",
    "        if name.startswith('bert.embeddings') or ln < 6:\n",
    "#         \tprint(name)  \n",
    "        \tparam.requires_grad = False\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def _model_init(config, model, device, state_dict=None, num_train_step=-1):\n",
    "        total_param = sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
    "        if config['cuda']:\n",
    "            if config['local_rank'] != -1:\n",
    "                model = model.to(device)\n",
    "            else:\n",
    "                model = model.to(device)\n",
    "        network = model\n",
    "        if state_dict:\n",
    "            missing_keys, unexpected_keys = network.load_state_dict(state_dict['state'], strict=False)\n",
    "\n",
    "        optimizer_parameters = _get_param_groups(network)\n",
    "#         try:\n",
    "        optimizer, scheduler = _setup_optim(config,optimizer_parameters, state_dict, num_train_step, network)\n",
    "#         except Exceptio#work: {network}')\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        #if self.config[\"local_rank\"] not in [-1, 0]:\n",
    "        #    torch.distributed.barrier()\n",
    "\n",
    "        if config['local_rank'] != -1:\n",
    "            mnetwork = torch.nn.parallel.DistributedDataParallel(network, device_ids=[self.config[\"local_rank\"]], output_device=self.config[\"local_rank\"], find_unused_parameters=True)\n",
    "        elif config['multi_gpu_on']:\n",
    "            mnetwork = torch.nn.DataParallel(network, device_ids=[0, 1, 2])\n",
    "        else:\n",
    "            mnetwork = network\n",
    "        task_loss_criterion = _setup_lossmap(config)\n",
    "        adv_task_loss_criterion = _setup_adv_lossmap(config)\n",
    "        adv_teacher = _setup_adv_training(config, adv_task_loss_criterion)\n",
    "        \n",
    "        return [mnetwork,\n",
    "                task_loss_criterion,\n",
    "                adv_task_loss_criterion,\n",
    "                adv_teacher,\n",
    "                optimizer_parameters,\n",
    "                optimizer,\n",
    "                scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perturbation import SmartPerturbation\n",
    "\n",
    "def _setup_adv_training(config, adv_task_loss_criterion):\n",
    "        adv_teacher = None\n",
    "        if config.get('adv_train', False):\n",
    "            adv_teacher = SmartPerturbation(config['adv_epsilon'],\n",
    "                    config['multi_gpu_on'],\n",
    "                    config['adv_step_size'],\n",
    "                    config['adv_noise_var'],\n",
    "                    config['adv_p_norm'],\n",
    "                    config['adv_k'],\n",
    "                    config['fp16'],\n",
    "                    config['encoder_type'],\n",
    "                    loss_map=adv_task_loss_criterion,\n",
    "                    norm_level=config['adv_norm_level'])\n",
    "        return adv_teacher\n",
    "            \n",
    "def _setup_adv_lossmap(config):\n",
    "        task_def_list: List[TaskDef] = config['task_def_list']\n",
    "        adv_task_loss_criterion = []\n",
    "        if config.get('adv_train', False):\n",
    "            cs = config['adv_loss']\n",
    "            assert cs in ['LossCriterion.SymKlCriterion', 'LossCriterion.KlCriterion']\n",
    "            if cs == 'LossCriterion.SymKlCriterion':\n",
    "                lc = LOSS_REGISTRY[LossCriterion.SymKlCriterion](name='Adv Loss func of task {}: {}'.format(0, cs))\n",
    "                adv_task_loss_criterion.append(lc)\n",
    "            else:\n",
    "                lc = LOSS_REGISTRY[LossCriterion.KlCriterion](name='Adv Loss func of task {}: {}'.format(0, cs))\n",
    "                adv_task_loss_criterion.append(lc)\n",
    "            return adv_task_loss_criterion\n",
    "            \n",
    "def _setup_lossmap(config):\n",
    "        task_def_list: List[TaskDef] = config['task_def_list']\n",
    "        task_loss_criterion = []\n",
    "        cs = config['loss'] # this loss has later to be passed through config file\n",
    "        if cs=='LossCriterion.CeCriterion':\n",
    "            lc = LOSS_REGISTRY[LossCriterion.CeCriterion](name='Loss func of task {}: {}'.format(0, cs)) \n",
    "            task_loss_criterion.append(lc)\n",
    "        \n",
    "        elif cs=='LossCriterion.KlCriterion':\n",
    "            lc = LOSS_REGISTRY[LossCriterion.KlCriterion](name='Loss func of task {}: {}'.format(0, cs)) \n",
    "            task_loss_criterion.append(lc)\n",
    "        elif cs=='LossCriterion.SymKlCriterion':\n",
    "            lc = LOSS_REGISTRY[LossCriterion.KlCriterion](name='Loss func of task {}: {}'.format(0, cs)) \n",
    "            task_loss_criterion.append(lc)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return task_loss_criterion\n",
    "            \n",
    "def _get_param_groups(network):\n",
    "        param_optimizer = [n for n in list(network.named_parameters()) if 'pooler' not in n[0]]\n",
    "        no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    \n",
    "from pytorch_pretrained_bert import BertAdam as Adam\n",
    "\n",
    "def _setup_optim(config, optimizer_parameters, state_dict=None, num_train_step=-1, network=None):\n",
    "    if config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(optimizer_parameters, config['learning_rate'],\n",
    "                                   weight_decay=config['weight_decay'])\n",
    "\n",
    "    elif config['optimizer'] == 'adamax':\n",
    "        optimizer = Adamax(optimizer_parameters,\n",
    "                                config['learning_rate'],\n",
    "                                warmup=config['warmup'],\n",
    "                                t_total=num_train_step,\n",
    "                                max_grad_norm=config['grad_clipping'],\n",
    "                                schedule=config['warmup_schedule'],\n",
    "                                weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "    elif config['optimizer'] == 'radam':\n",
    "        optimizer = RAdam(optimizer_parameters,\n",
    "                                config['learning_rate'],\n",
    "                                warmup=config['warmup'],\n",
    "                                t_total=num_train_step,\n",
    "                                max_grad_norm=config['grad_clipping'],\n",
    "                                schedule=config['warmup_schedule'],\n",
    "                                eps=config['adam_eps'],\n",
    "                                weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "        # The current radam does not support FP16.\n",
    "        config['fp16'] = False\n",
    "    elif config['optimizer'] == 'adam':\n",
    "        optimizer = Adam(optimizer_parameters,\n",
    "                              lr=config['learning_rate'],\n",
    "                              warmup=config['warmup'],\n",
    "                              t_total=num_train_step,\n",
    "                              max_grad_norm=config['grad_clipping'],\n",
    "                              schedule=config['warmup_schedule'],\n",
    "                              weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "    else:\n",
    "        raise RuntimeError('Unsupported optimizer: %s' % opt['optimizer'])\n",
    "\n",
    "    if state_dict and 'optimizer' in state_dict:\n",
    "        optimizer.load_state_dict(state_dict['optimizer'])\n",
    "\n",
    "    if config['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "            global amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(network, optimizer, opt_level=config['fp16_opt_level'])\n",
    "        network = model\n",
    "        optimizer = optimizer\n",
    "\n",
    "    if config.get('have_lr_scheduler', False):\n",
    "        if config.get('scheduler_type', 'rop') == 'rop':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=config['lr_gamma'], patience=3)\n",
    "        elif config.get('scheduler_type', 'rop') == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=config.get('lr_gamma', 0.95))\n",
    "        else:\n",
    "            milestones = [int(step) for step in config.get('multi_step_lr', '10,20,30').split(',')]\n",
    "            scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=config.get('lr_gamma'))\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return optimizer, scheduler\n",
    "\n",
    "def perturbated_loss():\n",
    "    smartPerturbation = SmartPerturbation(epsilon=1e-6,\n",
    "                    multi_gpu_on=False,\n",
    "                    step_size=1e-3,\n",
    "                    noise_var=1e-5,\n",
    "                    norm_p='inf',\n",
    "                    k=1,\n",
    "                    fp16=False,\n",
    "                    encoder_type=EncoderModelType.BERT,\n",
    "                    loss_map=[SymKlCriterion],\n",
    "                    norm_level=0)\n",
    "    \n",
    "    return smartPerturbation\n",
    "\n",
    "def _norm_grad(grad, norm_p, epsilon, eff_grad=None, sentence_level=False):\n",
    "        eff_direction = 0\n",
    "        if norm_p == 'l2':\n",
    "            if sentence_level:\n",
    "                direction = grad / (torch.norm(grad, dim=(-2, -1), keepdim=True) + epsilon)\n",
    "            else:\n",
    "                direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + epsilon)\n",
    "        elif norm_p == 'l1':\n",
    "            direction = grad.sign()\n",
    "        else:\n",
    "            if sentence_level:\n",
    "                direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + epsilon)\n",
    "            else:\n",
    "                direction = grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
    "                eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
    "        return direction, eff_direction\n",
    "    \n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    #print(outputs,outputs == labels)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "from loss import stable_kl\n",
    "\n",
    "def generate_noise(embed, mask, epsilon=1e-5):\n",
    "    noise = embed.data.new(embed.size()).normal_(0, 1) *  epsilon\n",
    "    noise.detach()\n",
    "    noise.requires_grad_()\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_save_ckp(metric, best_metric, model, optimizer, save_path, epoch, metric_name='train_loss'):\n",
    "    if metric < best_metric:\n",
    "        wandb.summary[f\"best_{metric_name}\"] = metric\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, save_path)\n",
    "        return metric\n",
    "    return best_metric\n",
    "    \n",
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  model,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  run_id,\n",
    "                  device,\n",
    "                  save_path=''):\n",
    "    \n",
    "    # preliminaries for training loop\n",
    "    loss_scale=0\n",
    "    output_dir=\"large_models\"\n",
    "    %mkdir large_models\n",
    "    output_train_file = os.path.join(output_dir, \"train_results.txt\")\n",
    "    output_smart_train_file = os.path.join(output_dir, \"smart_train_results.txt\")\n",
    "    loss_writer = open(output_train_file, \"w\",1)\n",
    "    smart_loss_writer = open(output_smart_train_file, \"w\",1)\n",
    "\n",
    "    train_loss_meter = AverageMeter()\n",
    "    adv_loss_meter = AverageMeter()\n",
    "    emb_val_meter = AverageMeter()\n",
    "    eff_perturb_meter = AverageMeter()\n",
    "    val_loss_meter = AverageMeter()\n",
    "    extracted_std_loss_meter = AverageMeter()\n",
    "    \n",
    "    local_updates = 0\n",
    "    updates =0\n",
    "    best_loss = np.inf\n",
    "    last_es_criterion = np.inf\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_std_tr_loss = 0\n",
    "    overall_wrong_confidence_scores = []\n",
    "    overall_correct_confidence_scores = []\n",
    "    trigger_times = 0\n",
    "    stopping = False\n",
    "    best_metrics = {}\n",
    "    \n",
    "    try:\n",
    "        adv_schedule_step = int(len(train_dataloader)/len(config['adv_alpha_schedule']))\n",
    "    except ZeroDivisionError:\n",
    "        adv_schedule_step = 1\n",
    "\n",
    "    for epoch in trange(int(config['epochs']), desc=\"Epoch\"):\n",
    "                tr_loss = 0\n",
    "                last_tr_loss = 0\n",
    "                std_loss = 0\n",
    "\n",
    "                nb_tr_examples, nb_tr_steps = 0, 0\n",
    "                for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                    if len(config['adv_alpha_schedule']) > 0 and (step % adv_schedule_step == 0):\n",
    "                        config['adv_alpha'] = config['adv_alpha_schedule'].pop()\n",
    "                    batch = tuple(t.to(device) for t in batch)\n",
    "                    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                    logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "                    # compute loss\n",
    "                    if config['loss'] is not 'LossCriterion.KlCriterion':\n",
    "                        label_logits = create_true_logits(logits, label_ids)\n",
    "                        loss = task_loss_criterion[0](logits, label_logits, config['weight'], ignore_index=-1)\n",
    "                    else:\n",
    "                        loss = task_loss_criterion[0](logits, label_ids, config['weight'], ignore_index=-1)\n",
    "                    \n",
    "                    if config['adv_train']:\n",
    "                        if config['multi_gpu_on']:\n",
    "                            embed = torch.stack([model.module.bert.embeddings(ids, mask) for ids, mask in zip(input_ids, input_mask)]) # recover batch-embeddings\n",
    "                        else:\n",
    "                            embed = torch.stack([model.bert.embeddings(ids, mask) for ids, mask in zip(input_ids, input_mask)]) # recover batch-embeddings\n",
    "                        noise = generate_noise(embed, input_mask, config['adv_noise_var'])\n",
    "                        for step in range(config['adv_k']):\n",
    "                            adv_logits = torch.stack([model(input_ids=ids,\n",
    "                                                               token_type_ids=mask,\n",
    "                                                               attention_mask=segment,\n",
    "                                                               inputs_embeds=in_embed)[-1] for ids, mask, segment, in_embed in zip(input_ids,\n",
    "                                                                                                                                   input_mask,\n",
    "                                                                                                                                   segment_ids,\n",
    "                                                                                                                                   noise+embed)\n",
    "                                                     ]\n",
    "                                                    )\n",
    "                            adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
    "                            delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
    "                            norm = delta_grad.norm()\n",
    "                            if (torch.isnan(norm) or torch.isinf(norm)):\n",
    "                                adv_loss = emb_val = eff_perturbation = 0\n",
    "                                break\n",
    "                            eff_delta_grad = delta_grad * config['adv_step_size']\n",
    "                            delta_grad = noise + delta_grad * config['adv_step_size']\n",
    "                            noise, eff_noise = _norm_grad(delta_grad,\n",
    "                                                          norm_p=config['adv_p_norm'],\n",
    "                                                          epsilon=config['adv_epsilon'],\n",
    "                                                          eff_grad=eff_delta_grad,\n",
    "                                                          sentence_level=config['adv_norm_level'])\n",
    "                            noise = noise.detach()\n",
    "                            noise.requires_grad_()\n",
    "                        adv_logits = torch.stack([model(input_ids=ids,\n",
    "                                               token_type_ids=mask,\n",
    "                                               attention_mask=segment,\n",
    "                                               inputs_embeds=in_embed)[-1] for ids, mask, segment, in_embed in zip(input_ids,\n",
    "                                                                                                                   input_mask,\n",
    "                                                                                                                   segment_ids,\n",
    "                                                                                                                   noise+embed)\n",
    "                                                 ]\n",
    "                                                )\n",
    "                        if config['klcriterion']=='flip':\n",
    "                            adv_loss = adv_task_loss_criterion[-1](adv_logits, logits, ignore_index=-1)\n",
    "                        else:\n",
    "                            adv_loss = adv_task_loss_criterion[-1](logits, adv_logits, ignore_index=-1)\n",
    "                        emb_val = embed.detach().abs().mean()\n",
    "                        try:\n",
    "                            eff_perturb = eff_noise.detach().abs().mean()\n",
    "                            eff_perturb_meter.update(eff_perturb.item(), config['batch_size'])\n",
    "                        except AttributeError:\n",
    "                            eff_perturb = 0\n",
    "                            eff_perturb_meter.update(0, config['batch_size'])\n",
    "\n",
    "                        loss += config['adv_alpha']*adv_loss\n",
    "                        adv_loss_meter.update(adv_loss.item(), config['batch_size'])\n",
    "                        emb_val_meter.update(emb_val.item(), config['batch_size'])\n",
    "                    \n",
    "                    else:\n",
    "                        adv_loss = 0\n",
    "                        emb_val = 0\n",
    "                        adv_loss_meter.update(adv_loss, config['batch_size'])\n",
    "                        emb_val_meter.update(emb_val, config['batch_size'])\n",
    "                    train_loss_meter.update(loss.item(), config['batch_size'])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    loss = loss /config.get('grad_accumulation_step', 1)\n",
    "                    if config['fp16']:\n",
    "                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                            scaled_loss.backward()\n",
    "\n",
    "                    local_updates += 1\n",
    "                    if local_updates % config.get('grad_accumulation_step', 1) == 0:\n",
    "                        if config['global_grad_clipping'] > 0:\n",
    "                            if config['fp16']:\n",
    "                                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                                               config['global_grad_clipping'])\n",
    "                            else:\n",
    "                                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                                               config['global_grad_clipping'])\n",
    "                        updates += 1\n",
    "                        # reset number of the grad accumulation\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad() \n",
    "                    \n",
    "    \n",
    "#                     accumulated_tr_loss+=train_loss_meter.val\n",
    "#                     accumulated_std_tr_loss+=train_loss_meter.val-adv_loss_meter.val\n",
    "                    if (updates) % (config['log_per_updates']) == 0 or updates == 1:\n",
    "        \n",
    "                        print('evaluating model......')\n",
    "                        wandb.log({\"avg standard loss\": train_loss_meter.avg - (config['adv_alpha']*adv_loss_meter.avg)})\n",
    "                        wandb.log({\"avg regularized loss\": train_loss_meter.avg})\n",
    "                                                \n",
    "#                         wandb.log({\"moving avg standard batch loss\": accumulated_std_tr_loss/config['log_per_updates']})\n",
    "#                         wandb.log({\"moving avg regularized batch loss\": accumulated_tr_loss/config['log_per_updates']})\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        if config['early_stopping_loss']=='train_loss':\n",
    "                            es_criterion = train_loss_meter.avg\n",
    "                        \n",
    "#                         accumulated_tr_loss = 0\n",
    "#                         accumulated_std_tr_loss = 0\n",
    "                        \n",
    "                        best_loss=check_save_ckp(train_loss_meter.val,\n",
    "                                                 best_loss,\n",
    "                                                 model,\n",
    "                                                 optimizer,\n",
    "                                                 f'model_ckp{run_id}.pt',\n",
    "                                                 epoch,\n",
    "                                                 'train_loss')\n",
    "                        \n",
    "                        [extracted_std_tr_loss,\n",
    "                         _,\n",
    "                         metrics,\n",
    "                         scores] = evaluation(model,\n",
    "                                              train_dataloader,\n",
    "                                              device,\n",
    "                                              mode='train')\n",
    "                        extracted_std_loss_meter.update(extracted_std_tr_loss,\n",
    "                                                        config['batch_size'])\n",
    "                        wandb.log({'extracted train std loss': extracted_std_loss_meter.avg})\n",
    "                \n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'train {key}': val})\n",
    "                            if val > best_metrics.get(f'best train {key}', 0):\n",
    "                                wandb.summary[f'best train {key}'] = val\n",
    "                                best_metrics[f'best train {key}'] = val\n",
    "                            \n",
    "                        [avg_val_loss,\n",
    "                         _,\n",
    "                         metrics,\n",
    "                         scores] = evaluation(model,\n",
    "                                              eval_dataloader,\n",
    "                                              device,\n",
    "                                              mode='val')\n",
    "                        \n",
    "                        val_loss_meter.update(avg_val_loss,\n",
    "                                              config['batch_size'])\n",
    "                        wandb.log({\"avg eval loss\": val_loss_meter.avg})\n",
    "                        \n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'val {key}': val})\n",
    "                            if val > best_metrics.get(f'best val {key}', 0):\n",
    "                                wandb.summary[f'best val {key}'] = val\n",
    "                                best_metrics[f'best val {key}'] = val\n",
    "                        \n",
    "                        \n",
    "                        if val_loss_meter.avg < best_metrics.get('best_avg_val_loss', np.inf):\n",
    "                            wandb.summary[f\"best eval loss\"] = val_loss_meter.avg\n",
    "                            best_metrics['best_avg_val_loss'] = val_loss_meter.avg\n",
    "                        \n",
    "                        # test\n",
    "                        [_,\n",
    "                         _,\n",
    "                         metrics,\n",
    "                         scores] = evaluation(model,\n",
    "                                              test_dataloader,\n",
    "                                              device,\n",
    "                                              mode='test')\n",
    "                        \n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'test {key}': val})\n",
    "                            if val > best_metrics.get(f'best test {key}', 0):\n",
    "                                wandb.summary[f'best test {key}'] = val\n",
    "                                best_metrics[f'best test {key}'] = val\n",
    "                            \n",
    "                        if config['early_stopping_loss']!='train_loss':\n",
    "                            es_criterion = val_loss_meter.avg\n",
    "                        \n",
    "                        \n",
    "                        if es_criterion > last_es_criterion:\n",
    "                            trigger_times += 1\n",
    "\n",
    "                            if trigger_times >= config['patience']:\n",
    "                                stopping=True\n",
    "                                break\n",
    "                                \n",
    "\n",
    "                        else:\n",
    "                            trigger_times = 0\n",
    "\n",
    "                        last_es_criterion = es_criterion\n",
    "                    \n",
    "                        test_correct_confidence_scores = scores['correct_confidence']\n",
    "                        test_wrong_confidence_scores = scores['wrong_confidence']\n",
    "                        \n",
    "                    overall_wrong_confidence_scores = []\n",
    "                    overall_correct_confidence_scores = []\n",
    "                    \n",
    "                    wandb.log({'batch adv_loss': train_loss_meter.avg})\n",
    "                    wandb.log({'batch std_loss': train_loss_meter.avg - adv_loss_meter.avg})\n",
    "                        \n",
    "                    \n",
    "                try:\n",
    "                    test_correct_confidence_scores = [[sigmoid(s)] for s in test_correct_confidence_scores]\n",
    "                    test_wrong_confidence_scores = [[sigmoid(s)] for s in test_wrong_confidence_scores]\n",
    "                    correct_table = wandb.Table(data=test_correct_confidence_scores, columns=[\"confidence scores\"])\n",
    "                    wrong_table = wandb.Table(data=test_wrong_confidence_scores, columns=[\"confidence scores\"])\n",
    "\n",
    "                    wandb.log({f'correct_pred_scores epoch {epoch}': wandb.plot.histogram(correct_table,\n",
    "                                                                    \"confidence scores\",\n",
    "                                                                    title=\"(Correct) Prediction Score Distribution\")})\n",
    "                    wandb.log({f'wrong_pred_scores epoch {epoch}': wandb.plot.histogram(wrong_table,\n",
    "                                                                \"confidence scores\",\n",
    "                                                                title=\"(Wrong) Prediction Score Distribution\")})\n",
    "                except Exception as e:\n",
    "                    print(f'EXCEPTION {e}')\n",
    "                    print(test_correct_confidence_scores)\n",
    "                    print(test_wrong_confidence_scores)\n",
    "                    \n",
    "                if stopping==True:\n",
    "                    print(f'STOPPING at epoch {epoch}.')\n",
    "                    return model\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "def calc_metrics(predictions, labels):\n",
    "    return {'acc': np.round(100*accuracy_score(labels, predictions)),\n",
    "            'f1': np.round(100*f1_score(labels, predictions, average=\"weighted\"), 2)}\n",
    "     \n",
    "\n",
    "def evaluation(model, dataloader, device, mode='val'):\n",
    "    assert mode in ['test', 'val', 'dev', 'train']\n",
    "    with torch.no_grad():\n",
    "        [metrics,\n",
    "         predictions,\n",
    "         loss,\n",
    "         batch_size,\n",
    "         scores] = eval_model(model, dataloader, device)\n",
    "    return loss.item(), batch_size, metrics, scores\n",
    "                         \n",
    "def eval_model(model, dataloader, device):\n",
    "    overall_predictions = []\n",
    "    overall_labels = []\n",
    "    golds = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    overall_metrics = {}\n",
    "    \n",
    "    \n",
    "    prediction_confidence_summary = {'correct_confidence': [],\n",
    "                            'wrong_confidence': []}\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss, logits = model(input_ids,\n",
    "                             segment_ids,\n",
    "                             input_mask,\n",
    "                             label_ids,\n",
    "                             return_logits=True)\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        \n",
    "        normalized_confidence = softmax(logits)\n",
    "        prediction_confidence = np.max(normalized_confidence, axis=1)\n",
    "        \n",
    "        correct_label_prediction = prediction_confidence[predictions==label_ids]\n",
    "        wrong_label_prediction = prediction_confidence[predictions!=label_ids]\n",
    "        \n",
    "        \n",
    "        prediction_confidence_summary['correct_confidence'].extend(list(correct_label_prediction))\n",
    "        prediction_confidence_summary['wrong_confidence'].extend((wrong_label_prediction))\n",
    "        overall_predictions.extend(predictions)\n",
    "        overall_labels.extend(label_ids)\n",
    "    \n",
    "    metrics = calc_metrics(overall_predictions, list(overall_labels))\n",
    "    prediction_confidence_summary = {'correct_confidence': [],\n",
    "                            'wrong_confidence': []} \n",
    "    return metrics, predictions, loss, len(predictions), prediction_confidence_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset\n",
    "# train_dataloader, eval_dataloader = build_dataset(wandb.config)\n",
    "\n",
    "def train():\n",
    "    default_config = {\n",
    "             'multi_gpu_on':False,\n",
    "             'max_seq_length': 128,\n",
    "             'adam_eps': 6, \n",
    "             'adv_epsilon': 1e-5, # 1e-6\n",
    "             'adv_train': 1,\n",
    "             'adv_noise_var': 1e-5,\n",
    "             'adv_norm_level': 0,\n",
    "             'adv_step_size': 1e-3, #e-5 was bad\n",
    "             'bin_on': False,\n",
    "             'cuda': 1,\n",
    "             'encoder_type': None,\n",
    "             'fp16': True,\n",
    "             'fp16_opt_level': 'O1',\n",
    "             'global_grad_clipping': 1.0,\n",
    "             'grad_accumulation_step': 1,\n",
    "             'grad_clipping': 1,\n",
    "             'local_rank': -1,\n",
    "             'mkd_opt': 0,\n",
    "             'scheduler_type': 'exp',\n",
    "             'task_def_list': None,\n",
    "             'warmup': 0.1,\n",
    "             'warmup_schedule': 'warmup_linear',\n",
    "             'weight_decay': 0,\n",
    "             'weighted_on': False,\n",
    "             'state_dict': None,\n",
    "             'loss': 'LossCriterion.SymKlCriterion',\n",
    "             'epochs': 5,\n",
    "             'batch_size': 2,\n",
    "             'adv_k': 1,\n",
    "             'learning_rate':2e-5,\n",
    "             'adv_p_norm': 'inf',\n",
    "             'adv_alpha': 1,\n",
    "             'optimizer': 'adam',\n",
    "             'adv_loss': 'LossCriterion.SymKlCriterion',\n",
    "             'klcriterion': '-',\n",
    "             'weight': 0,\n",
    "             'log_per_updates': 128,\n",
    "             'have_lr_scheduler': True,\n",
    "             'patience': 3,\n",
    "             'early_stopping_loss': 'train_loss',\n",
    "             'adv_alpha_schedule': []#[5, 3, 1, 0.1]#[0.1, 1, 3, 5] #[5, 3, 1, 0.1]\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"master-thesis\", config=default_config)\n",
    "    \n",
    "    cuda = default_config['cuda']\n",
    "    if cuda:\n",
    "        device='cuda'\n",
    "    \n",
    "    config = wandb.config\n",
    "    run_id = wandb.run.id\n",
    "    \n",
    "#     # build_dataset\n",
    "#     train_dataloader, eval_dataloader = build_dataset(config)\n",
    "    \n",
    "    # create model\n",
    "    model = create_model(device)\n",
    "    \n",
    "    # initialize model and losses\n",
    "    [mnetwork,\n",
    "     task_loss_criterion,\n",
    "     adv_task_loss_criterion,\n",
    "     adv_teacher,\n",
    "     optimizer_parameters,\n",
    "     optimizer,\n",
    "     scheduler] = _model_init(config=config, model=model, num_train_step=len(train_dataloader), device=device)\n",
    "    \n",
    "    # training mode ON\n",
    "    mnetwork.train()\n",
    "    \n",
    "    # wandb watch\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # train/eval\n",
    "    training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  run_id,\n",
    "                  device=device)\n",
    "    \n",
    "    # stop wandb\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/wassimboubaker/master-thesis/runs/2h4d1rhw\" target=\"_blank\">eternal-silence-555</a></strong> to <a href=\"https://wandb.ai/wassimboubaker/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "mkdir: cannot create directory ‘large_models’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1645 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 1/1645 [04:09<113:54:32, 249.44s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/1645 [04:10<79:49:03, 174.89s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 3/1645 [04:11<55:58:04, 122.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 4/1645 [04:12<39:16:55, 86.18s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 5/1645 [04:13<27:35:37, 60.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 6/1645 [04:14<19:26:21, 42.70s/it]\u001b[A\n",
      "Iteration:   0%|          | 7/1645 [04:14<13:42:49, 30.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 8/1645 [04:16<9:44:57, 21.44s/it] \u001b[A\n",
      "Iteration:   1%|          | 9/1645 [04:17<6:58:36, 15.35s/it]\u001b[A\n",
      "Iteration:   1%|          | 10/1645 [04:18<5:01:13, 11.05s/it]\u001b[A\n",
      "Iteration:   1%|          | 11/1645 [04:19<3:39:21,  8.05s/it]\u001b[A\n",
      "Iteration:   1%|          | 12/1645 [04:20<2:42:43,  5.98s/it]\u001b[A\n",
      "Iteration:   1%|          | 13/1645 [04:21<2:02:54,  4.52s/it]\u001b[A\n",
      "Iteration:   1%|          | 14/1645 [04:22<1:34:42,  3.48s/it]\u001b[A\n",
      "Iteration:   1%|          | 15/1645 [04:23<1:14:13,  2.73s/it]\u001b[A\n",
      "Iteration:   1%|          | 16/1645 [04:24<1:01:04,  2.25s/it]\u001b[A\n",
      "Iteration:   1%|          | 17/1645 [04:25<51:52,  1.91s/it]  \u001b[A\n",
      "Iteration:   1%|          | 18/1645 [04:26<45:22,  1.67s/it]\u001b[A\n",
      "Iteration:   1%|          | 19/1645 [04:28<40:55,  1.51s/it]\u001b[A\n",
      "Iteration:   1%|          | 20/1645 [04:29<37:40,  1.39s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 21/1645 [04:30<35:22,  1.31s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 22/1645 [04:31<33:56,  1.25s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 23/1645 [04:32<32:49,  1.21s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 24/1645 [04:33<32:08,  1.19s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 25/1645 [04:34<31:34,  1.17s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 26/1645 [04:35<31:06,  1.15s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 27/1645 [04:37<30:50,  1.14s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 28/1645 [04:38<29:19,  1.09s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 29/1645 [04:39<29:30,  1.10s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 30/1645 [04:40<29:46,  1.11s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 31/1645 [04:41<30:06,  1.12s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 32/1645 [04:42<30:16,  1.13s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 33/1645 [04:43<30:09,  1.12s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 34/1645 [04:44<30:05,  1.12s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 35/1645 [04:45<30:08,  1.12s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 36/1645 [04:46<28:56,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 37/1645 [04:47<28:12,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 38/1645 [04:49<28:53,  1.08s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 39/1645 [04:50<28:31,  1.07s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 40/1645 [04:51<28:08,  1.05s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 41/1645 [04:52<27:51,  1.04s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 42/1645 [04:53<28:12,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 43/1645 [04:54<28:53,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 44/1645 [04:55<28:26,  1.07s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 45/1645 [04:56<28:01,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 46/1645 [04:57<28:38,  1.07s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 47/1645 [04:58<28:11,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 48/1645 [04:59<28:37,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 49/1645 [05:00<29:23,  1.11s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 50/1645 [05:01<28:50,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 51/1645 [05:02<28:38,  1.08s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 52/1645 [05:03<28:12,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 53/1645 [05:04<27:56,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 54/1645 [05:05<27:46,  1.05s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 55/1645 [05:07<28:26,  1.07s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 56/1645 [05:08<28:07,  1.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 57/1645 [05:09<28:16,  1.07s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 58/1645 [05:10<28:00,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 59/1645 [05:11<27:49,  1.05s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 60/1645 [05:12<27:43,  1.05s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 61/1645 [05:13<27:34,  1.04s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 62/1645 [05:14<27:20,  1.04s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 63/1645 [05:15<27:54,  1.06s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 64/1645 [05:16<28:33,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 65/1645 [05:17<28:58,  1.10s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 66/1645 [05:18<28:18,  1.08s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 67/1645 [05:19<27:27,  1.04s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 68/1645 [05:20<26:47,  1.02s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 69/1645 [05:21<26:31,  1.01s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 70/1645 [05:22<26:37,  1.01s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 71/1645 [05:23<26:39,  1.02s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 72/1645 [05:24<26:38,  1.02s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 73/1645 [05:25<26:41,  1.02s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 74/1645 [05:26<27:29,  1.05s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 75/1645 [05:28<28:03,  1.07s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 76/1645 [05:29<28:26,  1.09s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 77/1645 [05:30<28:42,  1.10s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 78/1645 [05:31<28:52,  1.11s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 79/1645 [05:32<28:59,  1.11s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 80/1645 [05:33<29:06,  1.12s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 81/1645 [05:34<29:04,  1.12s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 82/1645 [05:35<29:11,  1.12s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 83/1645 [05:37<29:18,  1.13s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 84/1645 [05:38<29:18,  1.13s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 85/1645 [05:39<28:40,  1.10s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 86/1645 [05:40<27:59,  1.08s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 87/1645 [05:41<27:38,  1.06s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 88/1645 [05:42<27:50,  1.07s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 89/1645 [05:43<28:26,  1.10s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 90/1645 [05:44<28:24,  1.10s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 91/1645 [05:45<28:30,  1.10s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 92/1645 [05:46<27:54,  1.08s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 93/1645 [05:47<27:31,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 94/1645 [05:48<27:23,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 95/1645 [05:49<27:55,  1.08s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 96/1645 [05:51<27:32,  1.07s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 97/1645 [05:52<27:11,  1.05s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 98/1645 [05:53<27:14,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 99/1645 [05:54<27:45,  1.08s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 100/1645 [05:55<28:00,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 101/1645 [05:56<28:21,  1.10s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 102/1645 [05:57<28:32,  1.11s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 103/1645 [05:58<27:57,  1.09s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 104/1645 [05:59<27:28,  1.07s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 105/1645 [06:00<27:08,  1.06s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 106/1645 [06:01<26:54,  1.05s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 107/1645 [06:02<26:39,  1.04s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 108/1645 [06:03<26:46,  1.05s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 109/1645 [06:04<27:24,  1.07s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 110/1645 [06:06<27:43,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 111/1645 [06:07<28:06,  1.10s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 112/1645 [06:08<27:39,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 113/1645 [06:09<27:58,  1.10s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 114/1645 [06:10<28:04,  1.10s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 115/1645 [06:11<26:59,  1.06s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 116/1645 [06:12<26:13,  1.03s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 117/1645 [06:13<26:20,  1.03s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 118/1645 [06:14<27:06,  1.07s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 119/1645 [06:15<26:46,  1.05s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 120/1645 [06:16<27:38,  1.09s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 121/1645 [06:17<27:58,  1.10s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 122/1645 [06:18<27:20,  1.08s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 123/1645 [06:20<27:45,  1.09s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 124/1645 [06:21<27:11,  1.07s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 125/1645 [06:22<26:54,  1.06s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 126/1645 [06:23<27:00,  1.07s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 127/1645 [06:24<26:42,  1.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   8%|▊         | 128/1645 [10:43<33:07:05, 78.59s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 129/1645 [10:44<23:17:30, 55.31s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 130/1645 [10:45<16:25:19, 39.02s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 131/1645 [10:46<11:36:37, 27.61s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 132/1645 [10:47<8:14:39, 19.62s/it] \u001b[A\n",
      "Iteration:   8%|▊         | 133/1645 [10:48<5:53:17, 14.02s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 134/1645 [10:49<4:14:25, 10.10s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 135/1645 [10:50<3:05:13,  7.36s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 136/1645 [10:51<2:16:55,  5.44s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 137/1645 [10:52<1:43:10,  4.11s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 138/1645 [10:53<1:19:42,  3.17s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 139/1645 [10:54<1:03:25,  2.53s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 140/1645 [10:55<53:12,  2.12s/it]  \u001b[A\n",
      "Iteration:   9%|▊         | 141/1645 [10:56<45:21,  1.81s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 142/1645 [10:57<40:36,  1.62s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 143/1645 [10:59<37:22,  1.49s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 144/1645 [11:00<33:50,  1.35s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 145/1645 [11:01<31:28,  1.26s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 146/1645 [11:02<29:41,  1.19s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 147/1645 [11:03<28:34,  1.14s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 148/1645 [11:04<27:54,  1.12s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 149/1645 [11:05<27:15,  1.09s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 150/1645 [11:06<26:40,  1.07s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 151/1645 [11:07<26:22,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 152/1645 [11:08<27:22,  1.10s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 153/1645 [11:09<26:50,  1.08s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 154/1645 [11:10<26:30,  1.07s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 155/1645 [11:11<26:17,  1.06s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 156/1645 [11:12<26:46,  1.08s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 157/1645 [11:13<26:22,  1.06s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 158/1645 [11:14<26:06,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 159/1645 [11:15<25:58,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 160/1645 [11:17<25:55,  1.05s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 161/1645 [11:18<26:57,  1.09s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 162/1645 [11:19<27:11,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 163/1645 [11:20<27:15,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 164/1645 [11:21<27:20,  1.11s/it]\u001b[A\n",
      "Iteration:  10%|█         | 165/1645 [11:22<27:13,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|█         | 166/1645 [11:23<26:44,  1.08s/it]\u001b[A\n",
      "Iteration:  10%|█         | 167/1645 [11:24<26:24,  1.07s/it]\u001b[A\n",
      "Iteration:  10%|█         | 168/1645 [11:25<27:16,  1.11s/it]\u001b[A\n",
      "Iteration:  10%|█         | 169/1645 [11:27<27:47,  1.13s/it]\u001b[A\n",
      "Iteration:  10%|█         | 170/1645 [11:28<27:04,  1.10s/it]\u001b[A\n",
      "Iteration:  10%|█         | 171/1645 [11:29<26:35,  1.08s/it]\u001b[A\n",
      "Iteration:  10%|█         | 172/1645 [11:30<26:14,  1.07s/it]\u001b[A\n",
      "Iteration:  11%|█         | 173/1645 [11:31<27:47,  1.13s/it]\u001b[A\n",
      "Iteration:  11%|█         | 174/1645 [11:32<27:01,  1.10s/it]\u001b[A\n",
      "Iteration:  11%|█         | 175/1645 [11:33<26:27,  1.08s/it]\u001b[A\n",
      "Iteration:  11%|█         | 176/1645 [11:34<26:12,  1.07s/it]\u001b[A\n",
      "Iteration:  11%|█         | 177/1645 [11:35<25:53,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 178/1645 [11:36<26:46,  1.10s/it]\u001b[A\n",
      "Iteration:  11%|█         | 179/1645 [11:37<26:20,  1.08s/it]\u001b[A\n",
      "Iteration:  11%|█         | 180/1645 [11:38<25:55,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 181/1645 [11:39<25:41,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 182/1645 [11:41<26:16,  1.08s/it]\u001b[A\n",
      "Iteration:  11%|█         | 183/1645 [11:42<25:51,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 184/1645 [11:43<25:36,  1.05s/it]\u001b[A\n",
      "Iteration:  11%|█         | 185/1645 [11:44<25:23,  1.04s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 186/1645 [11:45<25:41,  1.06s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 187/1645 [11:46<26:32,  1.09s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 188/1645 [11:47<26:45,  1.10s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 189/1645 [11:48<26:13,  1.08s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 190/1645 [11:49<27:03,  1.12s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 191/1645 [11:50<27:44,  1.14s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 192/1645 [11:52<28:09,  1.16s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 193/1645 [11:53<27:44,  1.15s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 194/1645 [11:54<27:39,  1.14s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 195/1645 [11:55<27:36,  1.14s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 196/1645 [11:56<26:22,  1.09s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 197/1645 [11:57<26:03,  1.08s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 198/1645 [11:58<25:38,  1.06s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 199/1645 [11:59<25:19,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 200/1645 [12:00<25:13,  1.05s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 201/1645 [12:01<25:03,  1.04s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 202/1645 [12:02<24:57,  1.04s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 203/1645 [12:03<24:52,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 204/1645 [12:04<24:50,  1.03s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 205/1645 [12:05<25:22,  1.06s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 206/1645 [12:07<25:49,  1.08s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 207/1645 [12:08<26:09,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 208/1645 [12:09<26:20,  1.10s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 209/1645 [12:10<26:25,  1.10s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 210/1645 [12:11<26:29,  1.11s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 211/1645 [12:12<26:29,  1.11s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 212/1645 [12:13<25:52,  1.08s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 213/1645 [12:14<25:28,  1.07s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 214/1645 [12:15<25:09,  1.05s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 215/1645 [12:16<25:38,  1.08s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 216/1645 [12:17<26:03,  1.09s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 217/1645 [12:19<26:12,  1.10s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 218/1645 [12:20<26:22,  1.11s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 219/1645 [12:21<26:24,  1.11s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 220/1645 [12:22<26:08,  1.10s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 221/1645 [12:23<26:20,  1.11s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 222/1645 [12:24<26:30,  1.12s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 223/1645 [12:25<26:31,  1.12s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 224/1645 [12:26<26:31,  1.12s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 225/1645 [12:28<26:28,  1.12s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 226/1645 [12:29<26:28,  1.12s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 227/1645 [12:30<26:23,  1.12s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 228/1645 [12:31<25:51,  1.09s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 229/1645 [12:32<25:23,  1.08s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 230/1645 [12:33<25:00,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 231/1645 [12:34<24:40,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 232/1645 [12:35<24:38,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 233/1645 [12:36<24:27,  1.04s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 234/1645 [12:37<25:14,  1.07s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 235/1645 [12:38<24:54,  1.06s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 236/1645 [12:39<24:34,  1.05s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 237/1645 [12:40<25:02,  1.07s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 238/1645 [12:41<25:28,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 239/1645 [12:42<24:53,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 240/1645 [12:44<25:21,  1.08s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 241/1645 [12:45<24:58,  1.07s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 242/1645 [12:46<24:52,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 243/1645 [12:47<25:45,  1.10s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 244/1645 [12:48<25:58,  1.11s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 245/1645 [12:49<26:01,  1.12s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 246/1645 [12:50<26:03,  1.12s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 247/1645 [12:51<25:20,  1.09s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 248/1645 [12:52<25:08,  1.08s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 249/1645 [12:53<24:46,  1.06s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 250/1645 [12:54<25:31,  1.10s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 251/1645 [12:56<25:59,  1.12s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 252/1645 [12:57<26:27,  1.14s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 253/1645 [12:58<26:21,  1.14s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 254/1645 [12:59<26:12,  1.13s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 255/1645 [13:00<26:04,  1.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  16%|█▌        | 256/1645 [16:45<26:17:03, 68.12s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 257/1645 [16:46<18:31:06, 48.03s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 258/1645 [16:47<13:05:09, 33.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 259/1645 [16:48<9:17:10, 24.12s/it] \u001b[A\n",
      "Iteration:  16%|█▌        | 260/1645 [16:49<6:37:38, 17.23s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 261/1645 [16:50<4:45:19, 12.37s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 262/1645 [16:51<3:26:46,  8.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 263/1645 [16:52<2:31:47,  6.59s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 264/1645 [16:53<1:53:17,  4.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 265/1645 [16:54<1:26:30,  3.76s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 266/1645 [16:56<1:08:38,  2.99s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 267/1645 [16:57<54:58,  2.39s/it]  \u001b[A\n",
      "Iteration:  16%|█▋        | 268/1645 [16:58<45:27,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 269/1645 [16:59<38:51,  1.69s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 270/1645 [17:00<34:18,  1.50s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 271/1645 [17:01<31:00,  1.35s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 272/1645 [17:02<28:50,  1.26s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 273/1645 [17:03<27:11,  1.19s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 274/1645 [17:04<25:59,  1.14s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 275/1645 [17:05<25:17,  1.11s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 276/1645 [17:06<24:39,  1.08s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 277/1645 [17:07<24:51,  1.09s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 278/1645 [17:08<25:09,  1.10s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 279/1645 [17:09<25:14,  1.11s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 280/1645 [17:10<24:13,  1.06s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 281/1645 [17:11<23:35,  1.04s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 282/1645 [17:12<23:30,  1.04s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 283/1645 [17:13<23:36,  1.04s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 284/1645 [17:14<23:35,  1.04s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 285/1645 [17:15<23:29,  1.04s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 286/1645 [17:16<23:24,  1.03s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 287/1645 [17:17<24:16,  1.07s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 288/1645 [17:19<24:59,  1.10s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 289/1645 [17:20<25:03,  1.11s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 290/1645 [17:21<24:28,  1.08s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 291/1645 [17:22<24:38,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 292/1645 [17:23<24:57,  1.11s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 293/1645 [17:24<24:10,  1.07s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 294/1645 [17:25<23:56,  1.06s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 295/1645 [17:26<24:19,  1.08s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 296/1645 [17:27<24:32,  1.09s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 297/1645 [17:28<24:38,  1.10s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 298/1645 [17:30<24:47,  1.10s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 299/1645 [17:31<25:35,  1.14s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 300/1645 [17:32<25:22,  1.13s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 301/1645 [17:33<25:14,  1.13s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 302/1645 [17:34<25:11,  1.13s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 303/1645 [17:35<24:33,  1.10s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 304/1645 [17:36<24:46,  1.11s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 305/1645 [17:37<24:47,  1.11s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 306/1645 [17:39<24:51,  1.11s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 307/1645 [17:40<24:18,  1.09s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 308/1645 [17:41<24:28,  1.10s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 309/1645 [17:42<24:40,  1.11s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 310/1645 [17:43<24:55,  1.12s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 311/1645 [17:44<24:51,  1.12s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 312/1645 [17:45<25:05,  1.13s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 313/1645 [17:46<24:19,  1.10s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 314/1645 [17:47<23:38,  1.07s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 315/1645 [17:48<23:06,  1.04s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 316/1645 [17:49<23:06,  1.04s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 317/1645 [17:50<23:00,  1.04s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 318/1645 [17:51<22:51,  1.03s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 319/1645 [17:52<22:46,  1.03s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 320/1645 [17:53<22:41,  1.03s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 321/1645 [17:54<22:40,  1.03s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 322/1645 [17:55<22:58,  1.04s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 323/1645 [17:57<23:33,  1.07s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 324/1645 [17:58<23:50,  1.08s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 325/1645 [17:59<24:04,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 326/1645 [18:00<24:23,  1.11s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 327/1645 [18:01<24:36,  1.12s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 328/1645 [18:02<24:39,  1.12s/it]\u001b[A\n",
      "Iteration:  20%|██        | 329/1645 [18:03<24:42,  1.13s/it]\u001b[A\n",
      "Iteration:  20%|██        | 330/1645 [18:04<24:35,  1.12s/it]\u001b[A\n",
      "Iteration:  20%|██        | 331/1645 [18:06<23:57,  1.09s/it]\u001b[A\n",
      "Iteration:  20%|██        | 332/1645 [18:07<23:33,  1.08s/it]\u001b[A\n",
      "Iteration:  20%|██        | 333/1645 [18:08<23:16,  1.06s/it]\u001b[A\n",
      "Iteration:  20%|██        | 334/1645 [18:09<23:02,  1.05s/it]\u001b[A\n",
      "Iteration:  20%|██        | 335/1645 [18:10<22:48,  1.04s/it]\u001b[A\n",
      "Iteration:  20%|██        | 336/1645 [18:11<23:19,  1.07s/it]\u001b[A\n",
      "Iteration:  20%|██        | 337/1645 [18:12<23:36,  1.08s/it]\u001b[A\n",
      "Iteration:  21%|██        | 338/1645 [18:13<23:49,  1.09s/it]\u001b[A\n",
      "Iteration:  21%|██        | 339/1645 [18:14<23:59,  1.10s/it]\u001b[A\n",
      "Iteration:  21%|██        | 340/1645 [18:15<23:31,  1.08s/it]\u001b[A\n",
      "Iteration:  21%|██        | 341/1645 [18:16<23:09,  1.07s/it]\u001b[A\n",
      "Iteration:  21%|██        | 342/1645 [18:17<22:55,  1.06s/it]\u001b[A\n",
      "Iteration:  21%|██        | 343/1645 [18:18<22:42,  1.05s/it]\u001b[A\n",
      "Iteration:  21%|██        | 344/1645 [18:19<22:34,  1.04s/it]\u001b[A\n",
      "Iteration:  21%|██        | 345/1645 [18:20<23:22,  1.08s/it]\u001b[A\n",
      "Iteration:  21%|██        | 346/1645 [18:21<23:05,  1.07s/it]\u001b[A\n",
      "Iteration:  21%|██        | 347/1645 [18:23<22:53,  1.06s/it]\u001b[A\n",
      "Iteration:  21%|██        | 348/1645 [18:24<23:19,  1.08s/it]\u001b[A\n",
      "Iteration:  21%|██        | 349/1645 [18:25<22:36,  1.05s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 350/1645 [18:26<23:01,  1.07s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 351/1645 [18:27<23:17,  1.08s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 352/1645 [18:28<22:38,  1.05s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 353/1645 [18:29<22:05,  1.03s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 354/1645 [18:30<21:41,  1.01s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 355/1645 [18:31<22:24,  1.04s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 356/1645 [18:32<22:54,  1.07s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 357/1645 [18:33<22:20,  1.04s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 358/1645 [18:34<22:16,  1.04s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 359/1645 [18:35<23:10,  1.08s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 360/1645 [18:36<23:51,  1.11s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 361/1645 [18:37<23:19,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 362/1645 [18:38<22:55,  1.07s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 363/1645 [18:40<23:18,  1.09s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 364/1645 [18:41<23:07,  1.08s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 365/1645 [18:42<22:51,  1.07s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 366/1645 [18:43<23:28,  1.10s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 367/1645 [18:44<23:52,  1.12s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 368/1645 [18:45<23:57,  1.13s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 369/1645 [18:46<23:56,  1.13s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 370/1645 [18:47<23:51,  1.12s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 371/1645 [18:49<23:46,  1.12s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 372/1645 [18:50<23:44,  1.12s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 373/1645 [18:51<23:47,  1.12s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 374/1645 [18:52<23:41,  1.12s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 375/1645 [18:53<23:41,  1.12s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 376/1645 [18:54<23:08,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 377/1645 [18:55<22:42,  1.07s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 378/1645 [18:56<22:27,  1.06s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 379/1645 [18:57<23:10,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 380/1645 [18:58<23:14,  1.10s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 381/1645 [19:00<23:22,  1.11s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 382/1645 [19:01<22:51,  1.09s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 383/1645 [19:02<22:26,  1.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  23%|██▎       | 384/1645 [23:18<27:10:46, 77.59s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 385/1645 [23:19<19:07:08, 54.63s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 386/1645 [23:20<13:28:50, 38.55s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 387/1645 [23:21<9:32:11, 27.29s/it] \u001b[A\n",
      "Iteration:  24%|██▎       | 388/1645 [23:22<6:47:18, 19.44s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 389/1645 [23:23<4:51:01, 13.90s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 390/1645 [23:24<3:30:01, 10.04s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 391/1645 [23:25<2:33:20,  7.34s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 392/1645 [23:26<1:53:38,  5.44s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 393/1645 [23:27<1:25:51,  4.11s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 394/1645 [23:28<1:07:06,  3.22s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 395/1645 [23:29<53:36,  2.57s/it]  \u001b[A\n",
      "Iteration:  24%|██▍       | 396/1645 [23:30<43:53,  2.11s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 397/1645 [23:31<37:03,  1.78s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 398/1645 [23:32<32:18,  1.55s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 399/1645 [23:33<28:56,  1.39s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 400/1645 [23:34<26:33,  1.28s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 401/1645 [23:35<24:58,  1.20s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 402/1645 [23:36<24:28,  1.18s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 403/1645 [23:37<23:25,  1.13s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 404/1645 [23:39<22:42,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 405/1645 [23:40<22:24,  1.08s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 406/1645 [23:41<22:40,  1.10s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 407/1645 [23:42<22:54,  1.11s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 408/1645 [23:43<22:59,  1.12s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 409/1645 [23:44<22:58,  1.12s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 410/1645 [23:45<23:11,  1.13s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 411/1645 [23:46<23:06,  1.12s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 412/1645 [23:47<23:07,  1.13s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 413/1645 [23:48<22:04,  1.07s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 414/1645 [23:49<21:31,  1.05s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 415/1645 [23:50<21:18,  1.04s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 416/1645 [23:51<21:12,  1.04s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 417/1645 [23:53<22:01,  1.08s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 418/1645 [23:54<21:54,  1.07s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 419/1645 [23:55<21:38,  1.06s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 420/1645 [23:56<21:23,  1.05s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 421/1645 [23:57<21:44,  1.07s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 422/1645 [23:58<21:26,  1.05s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 423/1645 [23:59<21:17,  1.05s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 424/1645 [24:00<21:12,  1.04s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 425/1645 [24:01<21:41,  1.07s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 426/1645 [24:02<21:22,  1.05s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 427/1645 [24:03<21:11,  1.04s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 428/1645 [24:04<21:38,  1.07s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 429/1645 [24:05<21:19,  1.05s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 430/1645 [24:06<21:08,  1.04s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 431/1645 [24:07<20:57,  1.04s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 432/1645 [24:08<20:49,  1.03s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 433/1645 [24:09<21:27,  1.06s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 434/1645 [24:11<21:49,  1.08s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 435/1645 [24:12<22:04,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 436/1645 [24:13<22:15,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 437/1645 [24:14<22:05,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 438/1645 [24:15<21:14,  1.06s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 439/1645 [24:16<20:39,  1.03s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 440/1645 [24:17<22:36,  1.13s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 441/1645 [24:18<22:35,  1.13s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 442/1645 [24:19<22:34,  1.13s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 443/1645 [24:21<22:29,  1.12s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 444/1645 [24:22<22:27,  1.12s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 445/1645 [24:23<21:56,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 446/1645 [24:24<21:33,  1.08s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 447/1645 [24:25<21:51,  1.09s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 448/1645 [24:26<21:59,  1.10s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 449/1645 [24:27<22:08,  1.11s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 450/1645 [24:28<22:12,  1.11s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 451/1645 [24:29<21:22,  1.07s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 452/1645 [24:30<21:01,  1.06s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 453/1645 [24:31<20:46,  1.05s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 454/1645 [24:32<20:38,  1.04s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 455/1645 [24:33<20:51,  1.05s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 456/1645 [24:35<21:32,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 457/1645 [24:36<22:05,  1.12s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 458/1645 [24:37<21:28,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 459/1645 [24:38<21:03,  1.07s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 460/1645 [24:39<20:45,  1.05s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 461/1645 [24:40<20:33,  1.04s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 462/1645 [24:41<21:24,  1.09s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 463/1645 [24:42<21:36,  1.10s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 464/1645 [24:43<21:43,  1.10s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 465/1645 [24:44<21:07,  1.07s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 466/1645 [24:45<21:32,  1.10s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 467/1645 [24:46<21:36,  1.10s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 468/1645 [24:48<21:44,  1.11s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 469/1645 [24:49<21:12,  1.08s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 470/1645 [24:50<20:49,  1.06s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 471/1645 [24:51<20:34,  1.05s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 472/1645 [24:52<21:19,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 473/1645 [24:53<21:41,  1.11s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 474/1645 [24:54<20:46,  1.06s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 475/1645 [24:55<21:02,  1.08s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 476/1645 [24:56<20:26,  1.05s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 477/1645 [24:57<20:14,  1.04s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 478/1645 [24:58<20:09,  1.04s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 479/1645 [24:59<20:03,  1.03s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 480/1645 [25:00<20:32,  1.06s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 481/1645 [25:01<20:22,  1.05s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 482/1645 [25:02<20:48,  1.07s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 483/1645 [25:04<21:08,  1.09s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 484/1645 [25:05<20:42,  1.07s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 485/1645 [25:06<21:25,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 486/1645 [25:07<21:30,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 487/1645 [25:08<21:29,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 488/1645 [25:09<21:27,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 489/1645 [25:10<21:25,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 490/1645 [25:11<21:24,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 491/1645 [25:12<21:22,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 492/1645 [25:14<21:23,  1.11s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 493/1645 [25:15<21:27,  1.12s/it]\u001b[A\n",
      "Iteration:  30%|███       | 494/1645 [25:16<21:32,  1.12s/it]\u001b[A\n",
      "Iteration:  30%|███       | 495/1645 [25:17<20:38,  1.08s/it]\u001b[A\n",
      "Iteration:  30%|███       | 496/1645 [25:18<20:22,  1.06s/it]\u001b[A\n",
      "Iteration:  30%|███       | 497/1645 [25:19<21:02,  1.10s/it]\u001b[A\n",
      "Iteration:  30%|███       | 498/1645 [25:20<21:29,  1.12s/it]\u001b[A\n",
      "Iteration:  30%|███       | 499/1645 [25:21<20:55,  1.10s/it]\u001b[A\n",
      "Iteration:  30%|███       | 500/1645 [25:22<20:33,  1.08s/it]\u001b[A\n",
      "Iteration:  30%|███       | 501/1645 [25:23<20:16,  1.06s/it]\u001b[A\n",
      "Iteration:  31%|███       | 502/1645 [25:24<20:01,  1.05s/it]\u001b[A\n",
      "Iteration:  31%|███       | 503/1645 [25:25<19:50,  1.04s/it]\u001b[A\n",
      "Iteration:  31%|███       | 504/1645 [25:26<20:18,  1.07s/it]\u001b[A\n",
      "Iteration:  31%|███       | 505/1645 [25:28<20:43,  1.09s/it]\u001b[A\n",
      "Iteration:  31%|███       | 506/1645 [25:29<20:17,  1.07s/it]\u001b[A\n",
      "Iteration:  31%|███       | 507/1645 [25:30<20:30,  1.08s/it]\u001b[A\n",
      "Iteration:  31%|███       | 508/1645 [25:31<20:08,  1.06s/it]\u001b[A\n",
      "Iteration:  31%|███       | 509/1645 [25:32<20:24,  1.08s/it]\u001b[A\n",
      "Iteration:  31%|███       | 510/1645 [25:33<21:03,  1.11s/it]\u001b[A\n",
      "Iteration:  31%|███       | 511/1645 [25:34<21:22,  1.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  31%|███       | 512/1645 [29:21<21:38:15, 68.75s/it]\u001b[A\n",
      "Iteration:  31%|███       | 513/1645 [29:22<15:14:21, 48.46s/it]\u001b[A\n",
      "Iteration:  31%|███       | 514/1645 [29:23<10:45:46, 34.26s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 515/1645 [29:24<7:37:38, 24.30s/it] \u001b[A\n",
      "Iteration:  31%|███▏      | 516/1645 [29:25<5:26:43, 17.36s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 517/1645 [29:26<3:54:15, 12.46s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 518/1645 [29:27<2:49:37,  9.03s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 519/1645 [29:28<2:05:09,  6.67s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 520/1645 [29:29<1:32:57,  4.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 521/1645 [29:30<1:10:52,  3.78s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 522/1645 [29:31<55:22,  2.96s/it]  \u001b[A\n",
      "Iteration:  32%|███▏      | 523/1645 [29:33<45:05,  2.41s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 524/1645 [29:34<37:47,  2.02s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 525/1645 [29:35<32:45,  1.75s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 526/1645 [29:36<28:15,  1.52s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 527/1645 [29:37<25:06,  1.35s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 528/1645 [29:38<23:05,  1.24s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 529/1645 [29:39<21:48,  1.17s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 530/1645 [29:40<20:56,  1.13s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 531/1645 [29:41<21:09,  1.14s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 532/1645 [29:42<21:16,  1.15s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 533/1645 [29:43<20:36,  1.11s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 534/1645 [29:44<20:03,  1.08s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 535/1645 [29:45<19:54,  1.08s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 536/1645 [29:46<19:36,  1.06s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 537/1645 [29:47<19:20,  1.05s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 538/1645 [29:48<19:13,  1.04s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 539/1645 [29:50<19:55,  1.08s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 540/1645 [29:51<20:35,  1.12s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 541/1645 [29:52<19:47,  1.08s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 542/1645 [29:53<19:10,  1.04s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 543/1645 [29:54<19:21,  1.05s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 544/1645 [29:55<19:10,  1.04s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 545/1645 [29:56<19:37,  1.07s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 546/1645 [29:57<19:54,  1.09s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 547/1645 [29:58<19:36,  1.07s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 548/1645 [29:59<19:20,  1.06s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 549/1645 [30:00<19:43,  1.08s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 550/1645 [30:01<19:07,  1.05s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 551/1645 [30:02<19:28,  1.07s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 552/1645 [30:03<19:47,  1.09s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 553/1645 [30:05<20:00,  1.10s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 554/1645 [30:06<19:15,  1.06s/it]\u001b[A\n",
      "Iteration:  34%|███▎      | 555/1645 [30:07<19:33,  1.08s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 556/1645 [30:08<19:18,  1.06s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 557/1645 [30:09<19:03,  1.05s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 558/1645 [30:10<18:51,  1.04s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 559/1645 [30:11<18:53,  1.04s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 560/1645 [30:12<18:51,  1.04s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 561/1645 [30:13<18:48,  1.04s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 562/1645 [30:14<18:45,  1.04s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 563/1645 [30:15<18:45,  1.04s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 564/1645 [30:16<18:37,  1.03s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 565/1645 [30:17<18:33,  1.03s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 566/1645 [30:18<18:31,  1.03s/it]\u001b[A\n",
      "Iteration:  34%|███▍      | 567/1645 [30:19<18:29,  1.03s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 568/1645 [30:20<18:25,  1.03s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 569/1645 [30:21<18:23,  1.03s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 570/1645 [30:22<18:48,  1.05s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 571/1645 [30:23<19:29,  1.09s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 572/1645 [30:24<19:10,  1.07s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 573/1645 [30:25<19:17,  1.08s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 574/1645 [30:26<18:58,  1.06s/it]\u001b[A\n",
      "Iteration:  35%|███▍      | 575/1645 [30:28<19:34,  1.10s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 576/1645 [30:29<19:57,  1.12s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 577/1645 [30:30<20:14,  1.14s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 578/1645 [30:31<19:38,  1.10s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 579/1645 [30:32<19:13,  1.08s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 580/1645 [30:33<19:27,  1.10s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 581/1645 [30:34<19:33,  1.10s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 582/1645 [30:35<19:32,  1.10s/it]\u001b[A\n",
      "Iteration:  35%|███▌      | 583/1645 [30:37<19:35,  1.11s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 584/1645 [30:38<19:36,  1.11s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 585/1645 [30:39<19:39,  1.11s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 586/1645 [30:40<19:05,  1.08s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 587/1645 [30:41<19:21,  1.10s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 588/1645 [30:42<19:28,  1.11s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 589/1645 [30:43<19:31,  1.11s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 590/1645 [30:44<19:31,  1.11s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 591/1645 [30:45<19:44,  1.12s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 592/1645 [30:47<19:39,  1.12s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 593/1645 [30:48<19:42,  1.12s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 594/1645 [30:49<19:42,  1.13s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 595/1645 [30:50<19:40,  1.12s/it]\u001b[A\n",
      "Iteration:  36%|███▌      | 596/1645 [30:51<19:36,  1.12s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 597/1645 [30:52<19:06,  1.09s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 598/1645 [30:53<18:48,  1.08s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 599/1645 [30:54<18:32,  1.06s/it]\u001b[A\n",
      "Iteration:  36%|███▋      | 600/1645 [30:55<18:19,  1.05s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 601/1645 [30:56<18:13,  1.05s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 602/1645 [30:57<18:53,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 603/1645 [30:59<19:18,  1.11s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 604/1645 [31:00<18:54,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 605/1645 [31:01<19:04,  1.10s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 606/1645 [31:02<19:13,  1.11s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 607/1645 [31:03<18:49,  1.09s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 608/1645 [31:04<18:40,  1.08s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 609/1645 [31:05<18:56,  1.10s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 610/1645 [31:06<19:00,  1.10s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 611/1645 [31:07<19:05,  1.11s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 612/1645 [31:08<19:10,  1.11s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 613/1645 [31:10<19:12,  1.12s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 614/1645 [31:11<19:10,  1.12s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 615/1645 [31:12<19:08,  1.12s/it]\u001b[A\n",
      "Iteration:  37%|███▋      | 616/1645 [31:13<18:41,  1.09s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 617/1645 [31:14<18:47,  1.10s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 618/1645 [31:15<18:56,  1.11s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 619/1645 [31:16<19:04,  1.12s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 620/1645 [31:17<18:18,  1.07s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 621/1645 [31:18<17:59,  1.05s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 622/1645 [31:19<18:24,  1.08s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 623/1645 [31:20<17:51,  1.05s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 624/1645 [31:21<17:34,  1.03s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 625/1645 [31:22<18:03,  1.06s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 626/1645 [31:23<17:54,  1.05s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 627/1645 [31:25<17:46,  1.05s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 628/1645 [31:26<18:04,  1.07s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 629/1645 [31:27<17:52,  1.06s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 630/1645 [31:28<17:42,  1.05s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 631/1645 [31:29<17:32,  1.04s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 632/1645 [31:30<17:55,  1.06s/it]\u001b[A\n",
      "Iteration:  38%|███▊      | 633/1645 [31:31<18:08,  1.08s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 634/1645 [31:32<18:18,  1.09s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 635/1645 [31:33<17:59,  1.07s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 636/1645 [31:34<17:45,  1.06s/it]\u001b[A\n",
      "Iteration:  39%|███▊      | 637/1645 [31:35<17:33,  1.05s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 638/1645 [31:36<17:58,  1.07s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 639/1645 [31:37<17:22,  1.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  39%|███▉      | 640/1645 [35:31<19:45:35, 70.78s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 641/1645 [35:32<13:53:48, 49.83s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 642/1645 [35:33<9:47:46, 35.16s/it] \u001b[A\n",
      "Iteration:  39%|███▉      | 643/1645 [35:34<6:56:31, 24.94s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 644/1645 [35:35<4:56:14, 17.76s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 645/1645 [35:36<3:32:13, 12.73s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 646/1645 [35:37<2:33:48,  9.24s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 647/1645 [35:38<1:52:58,  6.79s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 648/1645 [35:39<1:24:24,  5.08s/it]\u001b[A\n",
      "Iteration:  39%|███▉      | 649/1645 [35:40<1:04:25,  3.88s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 650/1645 [35:41<50:26,  3.04s/it]  \u001b[A\n",
      "Iteration:  40%|███▉      | 651/1645 [35:42<40:41,  2.46s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 652/1645 [35:43<33:52,  2.05s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 653/1645 [35:44<28:40,  1.73s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 654/1645 [35:45<25:04,  1.52s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 655/1645 [35:46<22:54,  1.39s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 656/1645 [35:47<21:23,  1.30s/it]\u001b[A\n",
      "Iteration:  40%|███▉      | 657/1645 [35:48<19:54,  1.21s/it]\u001b[A\n",
      "Iteration:  40%|████      | 658/1645 [35:49<18:50,  1.15s/it]\u001b[A\n",
      "Iteration:  40%|████      | 659/1645 [35:51<18:22,  1.12s/it]\u001b[A\n",
      "Iteration:  40%|████      | 660/1645 [35:52<18:14,  1.11s/it]\u001b[A\n",
      "Iteration:  40%|████      | 661/1645 [35:53<17:21,  1.06s/it]\u001b[A\n",
      "Iteration:  40%|████      | 662/1645 [35:53<16:44,  1.02s/it]\u001b[A\n",
      "Iteration:  40%|████      | 663/1645 [35:54<16:35,  1.01s/it]\u001b[A\n",
      "Iteration:  40%|████      | 664/1645 [35:55<16:14,  1.01it/s]\u001b[A\n",
      "Iteration:  40%|████      | 665/1645 [35:56<15:59,  1.02it/s]\u001b[A\n",
      "Iteration:  40%|████      | 666/1645 [35:57<15:48,  1.03it/s]\u001b[A\n",
      "Iteration:  41%|████      | 667/1645 [35:58<16:47,  1.03s/it]\u001b[A\n",
      "Iteration:  41%|████      | 668/1645 [36:00<17:04,  1.05s/it]\u001b[A\n",
      "Iteration:  41%|████      | 669/1645 [36:01<17:14,  1.06s/it]\u001b[A\n",
      "Iteration:  41%|████      | 670/1645 [36:02<17:21,  1.07s/it]\u001b[A\n",
      "Iteration:  41%|████      | 671/1645 [36:03<17:26,  1.07s/it]\u001b[A\n",
      "Iteration:  41%|████      | 672/1645 [36:04<17:28,  1.08s/it]\u001b[A\n",
      "Iteration:  41%|████      | 673/1645 [36:05<17:28,  1.08s/it]\u001b[A\n",
      "Iteration:  41%|████      | 674/1645 [36:06<17:31,  1.08s/it]\u001b[A\n",
      "Iteration:  41%|████      | 675/1645 [36:07<17:30,  1.08s/it]\u001b[A\n",
      "Iteration:  41%|████      | 676/1645 [36:08<17:30,  1.08s/it]\u001b[A\n",
      "Iteration:  41%|████      | 677/1645 [36:09<17:28,  1.08s/it]\u001b[A\n",
      "Iteration:  41%|████      | 678/1645 [36:10<17:01,  1.06s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 679/1645 [36:11<16:42,  1.04s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 680/1645 [36:12<16:28,  1.02s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 681/1645 [36:13<17:04,  1.06s/it]\u001b[A\n",
      "Iteration:  41%|████▏     | 682/1645 [36:14<16:42,  1.04s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 683/1645 [36:16<16:58,  1.06s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 684/1645 [36:17<17:06,  1.07s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 685/1645 [36:18<17:13,  1.08s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 686/1645 [36:19<17:14,  1.08s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 687/1645 [36:20<16:32,  1.04s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 688/1645 [36:21<16:47,  1.05s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 689/1645 [36:22<16:46,  1.05s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 690/1645 [36:23<16:28,  1.03s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 691/1645 [36:24<16:15,  1.02s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 692/1645 [36:25<16:05,  1.01s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 693/1645 [36:26<16:43,  1.05s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 694/1645 [36:27<16:28,  1.04s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 695/1645 [36:28<16:17,  1.03s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 696/1645 [36:29<16:09,  1.02s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 697/1645 [36:30<16:00,  1.01s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 698/1645 [36:31<15:55,  1.01s/it]\u001b[A\n",
      "Iteration:  42%|████▏     | 699/1645 [36:32<15:51,  1.01s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 700/1645 [36:33<15:47,  1.00s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 701/1645 [36:34<15:47,  1.00s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 702/1645 [36:35<16:25,  1.05s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 703/1645 [36:36<16:40,  1.06s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 704/1645 [36:37<16:07,  1.03s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 705/1645 [36:38<15:41,  1.00s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 706/1645 [36:39<15:37,  1.00it/s]\u001b[A\n",
      "Iteration:  43%|████▎     | 707/1645 [36:40<16:19,  1.04s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 708/1645 [36:42<16:49,  1.08s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 709/1645 [36:43<17:05,  1.10s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 710/1645 [36:44<17:02,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 711/1645 [36:45<17:13,  1.11s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 712/1645 [36:46<16:44,  1.08s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 713/1645 [36:47<16:48,  1.08s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 714/1645 [36:48<16:51,  1.09s/it]\u001b[A\n",
      "Iteration:  43%|████▎     | 715/1645 [36:49<16:26,  1.06s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 716/1645 [36:50<16:32,  1.07s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 717/1645 [36:51<16:37,  1.07s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 718/1645 [36:52<16:00,  1.04s/it]\u001b[A\n",
      "Iteration:  44%|████▎     | 719/1645 [36:53<15:35,  1.01s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 720/1645 [36:54<15:16,  1.01it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 721/1645 [36:55<15:01,  1.03it/s]\u001b[A\n",
      "Iteration:  44%|████▍     | 722/1645 [36:56<15:30,  1.01s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 723/1645 [36:57<15:51,  1.03s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 724/1645 [36:58<15:40,  1.02s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 725/1645 [36:59<15:34,  1.02s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 726/1645 [37:00<15:30,  1.01s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 727/1645 [37:01<15:54,  1.04s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 728/1645 [37:02<15:42,  1.03s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 729/1645 [37:03<15:32,  1.02s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 730/1645 [37:04<15:25,  1.01s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 731/1645 [37:05<15:47,  1.04s/it]\u001b[A\n",
      "Iteration:  44%|████▍     | 732/1645 [37:06<16:04,  1.06s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 733/1645 [37:07<15:32,  1.02s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 734/1645 [37:09<15:49,  1.04s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 735/1645 [37:10<15:59,  1.05s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 736/1645 [37:11<15:25,  1.02s/it]\u001b[A\n",
      "Iteration:  45%|████▍     | 737/1645 [37:11<15:04,  1.00it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 738/1645 [37:12<15:02,  1.00it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 739/1645 [37:13<15:03,  1.00it/s]\u001b[A\n",
      "Iteration:  45%|████▍     | 740/1645 [37:14<15:03,  1.00it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 741/1645 [37:15<15:00,  1.00it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 742/1645 [37:16<14:59,  1.00it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 743/1645 [37:17<14:57,  1.00it/s]\u001b[A\n",
      "Iteration:  45%|████▌     | 744/1645 [37:19<15:21,  1.02s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 745/1645 [37:20<15:11,  1.01s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 746/1645 [37:21<15:33,  1.04s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 747/1645 [37:22<15:45,  1.05s/it]\u001b[A\n",
      "Iteration:  45%|████▌     | 748/1645 [37:23<15:53,  1.06s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 749/1645 [37:24<15:17,  1.02s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 750/1645 [37:25<15:35,  1.04s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 751/1645 [37:26<15:47,  1.06s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 752/1645 [37:27<15:13,  1.02s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 753/1645 [37:28<14:49,  1.00it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 754/1645 [37:29<14:47,  1.00it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 755/1645 [37:30<14:49,  1.00it/s]\u001b[A\n",
      "Iteration:  46%|████▌     | 756/1645 [37:31<15:33,  1.05s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 757/1645 [37:32<15:20,  1.04s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 758/1645 [37:33<15:33,  1.05s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 759/1645 [37:34<15:46,  1.07s/it]\u001b[A\n",
      "Iteration:  46%|████▌     | 760/1645 [37:35<15:12,  1.03s/it]\u001b[A\n",
      "Iteration:  46%|████▋     | 761/1645 [37:36<14:49,  1.01s/it]\u001b[A\n",
      "Iteration:  46%|████▋     | 762/1645 [37:37<14:34,  1.01it/s]\u001b[A\n",
      "Iteration:  46%|████▋     | 763/1645 [37:38<14:59,  1.02s/it]\u001b[A\n",
      "Iteration:  46%|████▋     | 764/1645 [37:39<15:18,  1.04s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 765/1645 [37:40<15:29,  1.06s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 766/1645 [37:41<15:38,  1.07s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 767/1645 [37:42<15:43,  1.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  47%|████▋     | 768/1645 [41:36<17:16:33, 70.92s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 769/1645 [41:37<12:09:08, 49.94s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 770/1645 [41:38<8:34:10, 35.26s/it] \u001b[A\n",
      "Iteration:  47%|████▋     | 771/1645 [41:39<6:04:19, 25.01s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 772/1645 [41:41<4:19:29, 17.83s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 773/1645 [41:42<3:06:11, 12.81s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 774/1645 [41:43<2:14:33,  9.27s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 775/1645 [41:44<1:38:24,  6.79s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 776/1645 [41:45<1:13:14,  5.06s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 777/1645 [41:46<56:10,  3.88s/it]  \u001b[A\n",
      "Iteration:  47%|████▋     | 778/1645 [41:47<44:06,  3.05s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 779/1645 [41:48<35:11,  2.44s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 780/1645 [41:49<29:32,  2.05s/it]\u001b[A\n",
      "Iteration:  47%|████▋     | 781/1645 [41:50<25:34,  1.78s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 782/1645 [41:51<22:13,  1.55s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 783/1645 [41:52<20:26,  1.42s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 784/1645 [41:53<19:11,  1.34s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 785/1645 [41:54<17:45,  1.24s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 786/1645 [41:56<17:20,  1.21s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 787/1645 [41:57<17:02,  1.19s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 788/1645 [41:58<16:11,  1.13s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 789/1645 [41:59<16:11,  1.14s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 790/1645 [42:00<16:11,  1.14s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 791/1645 [42:01<16:12,  1.14s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 792/1645 [42:02<15:58,  1.12s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 793/1645 [42:03<15:14,  1.07s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 794/1645 [42:04<15:04,  1.06s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 795/1645 [42:05<15:10,  1.07s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 796/1645 [42:06<15:15,  1.08s/it]\u001b[A\n",
      "Iteration:  48%|████▊     | 797/1645 [42:08<15:17,  1.08s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 798/1645 [42:09<15:18,  1.08s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 799/1645 [42:10<15:20,  1.09s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 800/1645 [42:11<15:19,  1.09s/it]\u001b[A\n",
      "Iteration:  49%|████▊     | 801/1645 [42:12<15:18,  1.09s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 802/1645 [42:13<15:19,  1.09s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 803/1645 [42:14<14:39,  1.04s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 804/1645 [42:15<14:27,  1.03s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 805/1645 [42:16<14:16,  1.02s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 806/1645 [42:17<14:11,  1.01s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 807/1645 [42:18<14:28,  1.04s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 808/1645 [42:19<14:42,  1.05s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 809/1645 [42:20<14:49,  1.06s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 810/1645 [42:21<14:55,  1.07s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 811/1645 [42:22<14:36,  1.05s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 812/1645 [42:23<14:24,  1.04s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 813/1645 [42:24<14:48,  1.07s/it]\u001b[A\n",
      "Iteration:  49%|████▉     | 814/1645 [42:26<15:04,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 815/1645 [42:27<15:03,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 816/1645 [42:28<15:02,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 817/1645 [42:29<15:00,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 818/1645 [42:30<14:59,  1.09s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 819/1645 [42:31<14:24,  1.05s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 820/1645 [42:32<14:32,  1.06s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 821/1645 [42:33<14:40,  1.07s/it]\u001b[A\n",
      "Iteration:  50%|████▉     | 822/1645 [42:34<14:42,  1.07s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 823/1645 [42:35<14:22,  1.05s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 824/1645 [42:36<14:06,  1.03s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 825/1645 [42:37<13:55,  1.02s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 826/1645 [42:38<13:49,  1.01s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 827/1645 [42:39<13:43,  1.01s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 828/1645 [42:40<13:38,  1.00s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 829/1645 [42:41<14:01,  1.03s/it]\u001b[A\n",
      "Iteration:  50%|█████     | 830/1645 [42:42<14:13,  1.05s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 831/1645 [42:43<14:03,  1.04s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 832/1645 [42:44<13:53,  1.03s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 833/1645 [42:45<14:00,  1.04s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 834/1645 [42:46<13:52,  1.03s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 835/1645 [42:47<13:42,  1.02s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 836/1645 [42:48<13:58,  1.04s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 837/1645 [42:49<13:34,  1.01s/it]\u001b[A\n",
      "Iteration:  51%|█████     | 838/1645 [42:50<13:16,  1.01it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 839/1645 [42:51<13:02,  1.03it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 840/1645 [42:52<12:54,  1.04it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 841/1645 [42:53<12:48,  1.05it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 842/1645 [42:54<12:42,  1.05it/s]\u001b[A\n",
      "Iteration:  51%|█████     | 843/1645 [42:55<12:40,  1.06it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 844/1645 [42:56<13:12,  1.01it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 845/1645 [42:57<13:05,  1.02it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 846/1645 [42:58<12:54,  1.03it/s]\u001b[A\n",
      "Iteration:  51%|█████▏    | 847/1645 [42:59<12:45,  1.04it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 848/1645 [43:00<12:52,  1.03it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 849/1645 [43:01<13:19,  1.00s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 850/1645 [43:02<13:15,  1.00s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 851/1645 [43:03<13:13,  1.00it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 852/1645 [43:04<13:10,  1.00it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 853/1645 [43:05<13:09,  1.00it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 854/1645 [43:06<13:10,  1.00it/s]\u001b[A\n",
      "Iteration:  52%|█████▏    | 855/1645 [43:07<13:43,  1.04s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 856/1645 [43:08<13:30,  1.03s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 857/1645 [43:09<13:22,  1.02s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 858/1645 [43:10<13:15,  1.01s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 859/1645 [43:11<13:11,  1.01s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 860/1645 [43:12<13:06,  1.00s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 861/1645 [43:13<13:26,  1.03s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 862/1645 [43:14<13:41,  1.05s/it]\u001b[A\n",
      "Iteration:  52%|█████▏    | 863/1645 [43:15<13:27,  1.03s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 864/1645 [43:16<13:39,  1.05s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 865/1645 [43:17<13:24,  1.03s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 866/1645 [43:18<13:36,  1.05s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 867/1645 [43:20<13:43,  1.06s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 868/1645 [43:21<13:49,  1.07s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 869/1645 [43:22<13:54,  1.08s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 870/1645 [43:23<13:22,  1.04s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 871/1645 [43:24<13:33,  1.05s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 872/1645 [43:25<13:39,  1.06s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 873/1645 [43:26<13:22,  1.04s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 874/1645 [43:27<13:10,  1.03s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 875/1645 [43:28<13:01,  1.02s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 876/1645 [43:29<12:55,  1.01s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 877/1645 [43:30<12:55,  1.01s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 878/1645 [43:31<12:50,  1.00s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 879/1645 [43:32<12:48,  1.00s/it]\u001b[A\n",
      "Iteration:  53%|█████▎    | 880/1645 [43:33<12:48,  1.00s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 881/1645 [43:34<12:47,  1.00s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 882/1645 [43:35<13:05,  1.03s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 883/1645 [43:36<13:17,  1.05s/it]\u001b[A\n",
      "Iteration:  54%|█████▎    | 884/1645 [43:37<13:19,  1.05s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 885/1645 [43:38<13:27,  1.06s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 886/1645 [43:39<13:31,  1.07s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 887/1645 [43:40<13:33,  1.07s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 888/1645 [43:41<13:36,  1.08s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 889/1645 [43:42<13:37,  1.08s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 890/1645 [43:44<13:36,  1.08s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 891/1645 [43:45<13:04,  1.04s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 892/1645 [43:46<12:57,  1.03s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 893/1645 [43:46<12:34,  1.00s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 894/1645 [43:47<12:31,  1.00s/it]\u001b[A\n",
      "Iteration:  54%|█████▍    | 895/1645 [43:48<12:27,  1.00it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  54%|█████▍    | 896/1645 [47:56<15:35:45, 74.96s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 897/1645 [47:57<10:58:16, 52.80s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 898/1645 [47:58<7:44:15, 37.29s/it] \u001b[A\n",
      "Iteration:  55%|█████▍    | 899/1645 [47:59<5:28:38, 26.43s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 900/1645 [48:00<3:53:47, 18.83s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 901/1645 [48:01<2:47:28, 13.51s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 902/1645 [48:03<2:01:06,  9.78s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 903/1645 [48:03<1:28:09,  7.13s/it]\u001b[A\n",
      "Iteration:  55%|█████▍    | 904/1645 [48:04<1:05:05,  5.27s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 905/1645 [48:05<48:57,  3.97s/it]  \u001b[A\n",
      "Iteration:  55%|█████▌    | 906/1645 [48:06<37:40,  3.06s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 907/1645 [48:07<30:03,  2.44s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 908/1645 [48:08<25:01,  2.04s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 909/1645 [48:09<21:11,  1.73s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 910/1645 [48:10<18:30,  1.51s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 911/1645 [48:12<17:06,  1.40s/it]\u001b[A\n",
      "Iteration:  55%|█████▌    | 912/1645 [48:13<15:36,  1.28s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 913/1645 [48:14<14:35,  1.20s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 914/1645 [48:15<13:49,  1.13s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 915/1645 [48:16<13:16,  1.09s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 916/1645 [48:17<13:28,  1.11s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 917/1645 [48:18<13:35,  1.12s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 918/1645 [48:19<13:56,  1.15s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 919/1645 [48:20<13:52,  1.15s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 920/1645 [48:21<13:50,  1.15s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 921/1645 [48:22<13:47,  1.14s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 922/1645 [48:24<13:33,  1.13s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 923/1645 [48:25<13:24,  1.11s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 924/1645 [48:26<13:19,  1.11s/it]\u001b[A\n",
      "Iteration:  56%|█████▌    | 925/1645 [48:27<12:53,  1.07s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 926/1645 [48:28<12:36,  1.05s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 927/1645 [48:29<12:24,  1.04s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 928/1645 [48:30<12:25,  1.04s/it]\u001b[A\n",
      "Iteration:  56%|█████▋    | 929/1645 [48:31<12:37,  1.06s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 930/1645 [48:32<12:10,  1.02s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 931/1645 [48:33<12:25,  1.04s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 932/1645 [48:34<12:35,  1.06s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 933/1645 [48:35<12:08,  1.02s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 934/1645 [48:36<12:00,  1.01s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 935/1645 [48:37<12:26,  1.05s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 936/1645 [48:38<12:42,  1.08s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 937/1645 [48:39<12:54,  1.09s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 938/1645 [48:40<12:55,  1.10s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 939/1645 [48:41<12:35,  1.07s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 940/1645 [48:42<12:19,  1.05s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 941/1645 [48:43<12:07,  1.03s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 942/1645 [48:45<12:28,  1.07s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 943/1645 [48:46<12:33,  1.07s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 944/1645 [48:47<12:34,  1.08s/it]\u001b[A\n",
      "Iteration:  57%|█████▋    | 945/1645 [48:48<12:36,  1.08s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 946/1645 [48:49<12:07,  1.04s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 947/1645 [48:50<12:15,  1.05s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 948/1645 [48:51<11:50,  1.02s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 949/1645 [48:52<11:32,  1.00it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 950/1645 [48:53<11:20,  1.02it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 951/1645 [48:54<11:10,  1.03it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 952/1645 [48:55<11:05,  1.04it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 953/1645 [48:56<10:59,  1.05it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 954/1645 [48:57<11:07,  1.04it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 955/1645 [48:58<11:13,  1.02it/s]\u001b[A\n",
      "Iteration:  58%|█████▊    | 956/1645 [48:59<11:36,  1.01s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 957/1645 [49:00<11:50,  1.03s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 958/1645 [49:01<11:32,  1.01s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 959/1645 [49:02<11:28,  1.00s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 960/1645 [49:03<11:29,  1.01s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 961/1645 [49:04<11:33,  1.01s/it]\u001b[A\n",
      "Iteration:  58%|█████▊    | 962/1645 [49:05<11:28,  1.01s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 963/1645 [49:06<11:53,  1.05s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 964/1645 [49:07<12:00,  1.06s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 965/1645 [49:08<11:48,  1.04s/it]\u001b[A\n",
      "Iteration:  59%|█████▊    | 966/1645 [49:09<11:39,  1.03s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 967/1645 [49:10<11:32,  1.02s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 968/1645 [49:11<11:28,  1.02s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 969/1645 [49:12<11:24,  1.01s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 970/1645 [49:13<11:48,  1.05s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 971/1645 [49:14<12:04,  1.08s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 972/1645 [49:15<12:05,  1.08s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 973/1645 [49:16<12:05,  1.08s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 974/1645 [49:17<12:06,  1.08s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 975/1645 [49:19<12:05,  1.08s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 976/1645 [49:20<12:05,  1.08s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 977/1645 [49:21<11:48,  1.06s/it]\u001b[A\n",
      "Iteration:  59%|█████▉    | 978/1645 [49:22<12:03,  1.08s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 979/1645 [49:23<11:43,  1.06s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 980/1645 [49:24<11:31,  1.04s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 981/1645 [49:25<11:39,  1.05s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 982/1645 [49:26<11:44,  1.06s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 983/1645 [49:27<11:47,  1.07s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 984/1645 [49:28<11:23,  1.03s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 985/1645 [49:29<11:06,  1.01s/it]\u001b[A\n",
      "Iteration:  60%|█████▉    | 986/1645 [49:30<11:04,  1.01s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 987/1645 [49:31<11:02,  1.01s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 988/1645 [49:32<11:18,  1.03s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 989/1645 [49:33<10:59,  1.01s/it]\u001b[A\n",
      "Iteration:  60%|██████    | 990/1645 [49:34<10:46,  1.01it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 991/1645 [49:35<10:35,  1.03it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 992/1645 [49:36<10:28,  1.04it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 993/1645 [49:37<10:21,  1.05it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 994/1645 [49:38<10:46,  1.01it/s]\u001b[A\n",
      "Iteration:  60%|██████    | 995/1645 [49:39<11:03,  1.02s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 996/1645 [49:40<11:14,  1.04s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 997/1645 [49:41<11:07,  1.03s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 998/1645 [49:42<11:18,  1.05s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 999/1645 [49:43<10:56,  1.02s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1000/1645 [49:45<12:41,  1.18s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1001/1645 [49:46<12:06,  1.13s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1002/1645 [49:47<11:46,  1.10s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1003/1645 [49:48<11:44,  1.10s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1004/1645 [49:49<11:34,  1.08s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1005/1645 [49:50<11:37,  1.09s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1006/1645 [49:51<11:39,  1.09s/it]\u001b[A\n",
      "Iteration:  61%|██████    | 1007/1645 [49:52<11:40,  1.10s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1008/1645 [49:53<11:44,  1.11s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1009/1645 [49:54<11:27,  1.08s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1010/1645 [49:55<11:26,  1.08s/it]\u001b[A\n",
      "Iteration:  61%|██████▏   | 1011/1645 [49:56<11:39,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1012/1645 [49:58<11:49,  1.12s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1013/1645 [49:59<11:45,  1.12s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1014/1645 [50:00<11:42,  1.11s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1015/1645 [50:01<11:38,  1.11s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1016/1645 [50:02<11:39,  1.11s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1017/1645 [50:03<11:36,  1.11s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1018/1645 [50:04<11:26,  1.10s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1019/1645 [50:05<11:10,  1.07s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1020/1645 [50:06<11:01,  1.06s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1021/1645 [50:07<10:51,  1.04s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1022/1645 [50:08<10:44,  1.04s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1023/1645 [50:09<10:45,  1.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  62%|██████▏   | 1024/1645 [53:51<11:35:27, 67.19s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1025/1645 [53:52<8:09:18, 47.35s/it] \u001b[A\n",
      "Iteration:  62%|██████▏   | 1026/1645 [53:53<5:45:13, 33.46s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1027/1645 [53:54<4:04:25, 23.73s/it]\u001b[A\n",
      "Iteration:  62%|██████▏   | 1028/1645 [53:55<2:53:56, 16.92s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1029/1645 [53:56<2:04:40, 12.14s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1030/1645 [53:57<1:30:14,  8.80s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1031/1645 [53:58<1:06:28,  6.50s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1032/1645 [53:59<49:25,  4.84s/it]  \u001b[A\n",
      "Iteration:  63%|██████▎   | 1033/1645 [54:00<37:30,  3.68s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1034/1645 [54:01<29:09,  2.86s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1035/1645 [54:02<23:19,  2.30s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1036/1645 [54:03<19:23,  1.91s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1037/1645 [54:04<16:36,  1.64s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1038/1645 [54:05<14:49,  1.47s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1039/1645 [54:06<13:16,  1.31s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1040/1645 [54:07<12:09,  1.21s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1041/1645 [54:08<11:23,  1.13s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1042/1645 [54:09<10:49,  1.08s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1043/1645 [54:10<10:41,  1.07s/it]\u001b[A\n",
      "Iteration:  63%|██████▎   | 1044/1645 [54:11<10:20,  1.03s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 1045/1645 [54:12<10:06,  1.01s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 1046/1645 [54:13<10:22,  1.04s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 1047/1645 [54:14<10:32,  1.06s/it]\u001b[A\n",
      "Iteration:  64%|██████▎   | 1048/1645 [54:15<10:40,  1.07s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1049/1645 [54:16<10:46,  1.09s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1050/1645 [54:17<10:53,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1051/1645 [54:19<10:53,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1052/1645 [54:20<10:54,  1.10s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1053/1645 [54:21<10:39,  1.08s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1054/1645 [54:22<10:44,  1.09s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1055/1645 [54:23<10:44,  1.09s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1056/1645 [54:24<10:21,  1.05s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1057/1645 [54:25<10:04,  1.03s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1058/1645 [54:26<09:53,  1.01s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1059/1645 [54:27<10:09,  1.04s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1060/1645 [54:28<10:20,  1.06s/it]\u001b[A\n",
      "Iteration:  64%|██████▍   | 1061/1645 [54:29<09:59,  1.03s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1062/1645 [54:30<09:46,  1.01s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1063/1645 [54:31<09:49,  1.01s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1064/1645 [54:32<09:48,  1.01s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1065/1645 [54:33<09:49,  1.02s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1066/1645 [54:34<09:48,  1.02s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1067/1645 [54:35<09:48,  1.02s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1068/1645 [54:36<09:45,  1.01s/it]\u001b[A\n",
      "Iteration:  65%|██████▍   | 1069/1645 [54:37<09:45,  1.02s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1070/1645 [54:38<09:41,  1.01s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1071/1645 [54:39<09:42,  1.02s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1072/1645 [54:40<09:50,  1.03s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1073/1645 [54:41<09:38,  1.01s/it]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1074/1645 [54:42<09:29,  1.00it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1075/1645 [54:43<09:23,  1.01it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1076/1645 [54:44<09:16,  1.02it/s]\u001b[A\n",
      "Iteration:  65%|██████▌   | 1077/1645 [54:45<09:28,  1.00s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1078/1645 [54:46<09:46,  1.03s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1079/1645 [54:47<09:35,  1.02s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1080/1645 [54:48<09:31,  1.01s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1081/1645 [54:49<09:32,  1.01s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1082/1645 [54:50<09:30,  1.01s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1083/1645 [54:51<09:29,  1.01s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1084/1645 [54:52<09:30,  1.02s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1085/1645 [54:53<09:47,  1.05s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1086/1645 [54:54<09:56,  1.07s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1087/1645 [54:55<09:36,  1.03s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1088/1645 [54:56<09:24,  1.01s/it]\u001b[A\n",
      "Iteration:  66%|██████▌   | 1089/1645 [54:57<09:15,  1.00it/s]\u001b[A\n",
      "Iteration:  66%|██████▋   | 1090/1645 [54:58<09:32,  1.03s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 1091/1645 [55:00<09:42,  1.05s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 1092/1645 [55:00<09:25,  1.02s/it]\u001b[A\n",
      "Iteration:  66%|██████▋   | 1093/1645 [55:01<09:22,  1.02s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1094/1645 [55:02<09:20,  1.02s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1095/1645 [55:03<09:16,  1.01s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1096/1645 [55:05<09:15,  1.01s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1097/1645 [55:06<09:14,  1.01s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1098/1645 [55:07<09:39,  1.06s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1099/1645 [55:08<09:30,  1.05s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1100/1645 [55:09<09:24,  1.04s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1101/1645 [55:10<09:20,  1.03s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1102/1645 [55:11<09:29,  1.05s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1103/1645 [55:12<09:38,  1.07s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1104/1645 [55:13<09:45,  1.08s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1105/1645 [55:14<09:34,  1.06s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1106/1645 [55:15<09:27,  1.05s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1107/1645 [55:16<09:23,  1.05s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1108/1645 [55:17<09:20,  1.04s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1109/1645 [55:18<09:18,  1.04s/it]\u001b[A\n",
      "Iteration:  67%|██████▋   | 1110/1645 [55:19<09:14,  1.04s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1111/1645 [55:20<09:12,  1.03s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1112/1645 [55:21<09:10,  1.03s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1113/1645 [55:22<09:07,  1.03s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1114/1645 [55:23<09:04,  1.03s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1115/1645 [55:24<09:02,  1.02s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1116/1645 [55:25<09:14,  1.05s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1117/1645 [55:27<09:21,  1.06s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1118/1645 [55:28<09:25,  1.07s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1119/1645 [55:29<09:18,  1.06s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1120/1645 [55:30<09:13,  1.05s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1121/1645 [55:31<08:56,  1.02s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1122/1645 [55:32<08:44,  1.00s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1123/1645 [55:33<08:36,  1.01it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1124/1645 [55:34<08:39,  1.00it/s]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1125/1645 [55:35<08:56,  1.03s/it]\u001b[A\n",
      "Iteration:  68%|██████▊   | 1126/1645 [55:36<09:09,  1.06s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 1127/1645 [55:37<09:15,  1.07s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 1128/1645 [55:38<09:07,  1.06s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 1129/1645 [55:39<09:01,  1.05s/it]\u001b[A\n",
      "Iteration:  69%|██████▊   | 1130/1645 [55:40<08:55,  1.04s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1131/1645 [55:41<08:53,  1.04s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1132/1645 [55:42<09:02,  1.06s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1133/1645 [55:43<08:57,  1.05s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1134/1645 [55:44<08:51,  1.04s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1135/1645 [55:45<08:52,  1.04s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1136/1645 [55:46<08:46,  1.04s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1137/1645 [55:47<08:42,  1.03s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1138/1645 [55:48<08:39,  1.02s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1139/1645 [55:49<08:38,  1.02s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1140/1645 [55:50<08:35,  1.02s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1141/1645 [55:52<08:57,  1.07s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1142/1645 [55:53<09:12,  1.10s/it]\u001b[A\n",
      "Iteration:  69%|██████▉   | 1143/1645 [55:54<09:18,  1.11s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1144/1645 [55:55<09:04,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1145/1645 [55:56<08:54,  1.07s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1146/1645 [55:57<08:46,  1.05s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1147/1645 [55:58<08:43,  1.05s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1148/1645 [55:59<08:51,  1.07s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1149/1645 [56:00<08:55,  1.08s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1150/1645 [56:01<08:57,  1.09s/it]\u001b[A\n",
      "Iteration:  70%|██████▉   | 1151/1645 [56:02<09:01,  1.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  70%|███████   | 1152/1645 [59:56<9:41:03, 70.72s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1153/1645 [59:57<6:48:28, 49.81s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1154/1645 [59:58<4:47:55, 35.18s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1155/1645 [59:59<3:23:59, 24.98s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1156/1645 [1:00:00<2:25:04, 17.80s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1157/1645 [1:00:01<1:43:48, 12.76s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1158/1645 [1:00:02<1:15:01,  9.24s/it]\u001b[A\n",
      "Iteration:  70%|███████   | 1159/1645 [1:00:03<54:57,  6.78s/it]  \u001b[A\n",
      "Iteration:  71%|███████   | 1160/1645 [1:00:04<41:11,  5.10s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1161/1645 [1:00:05<31:38,  3.92s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1162/1645 [1:00:06<24:50,  3.09s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1163/1645 [1:00:08<19:59,  2.49s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1164/1645 [1:00:09<16:35,  2.07s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1165/1645 [1:00:10<14:14,  1.78s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1166/1645 [1:00:11<12:36,  1.58s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1167/1645 [1:00:12<11:27,  1.44s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1168/1645 [1:00:13<10:36,  1.33s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1169/1645 [1:00:14<10:14,  1.29s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1170/1645 [1:00:15<09:46,  1.23s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1171/1645 [1:00:16<09:28,  1.20s/it]\u001b[A\n",
      "Iteration:  71%|███████   | 1172/1645 [1:00:18<09:14,  1.17s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 1173/1645 [1:00:19<08:49,  1.12s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 1174/1645 [1:00:20<08:33,  1.09s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 1175/1645 [1:00:21<08:34,  1.09s/it]\u001b[A\n",
      "Iteration:  71%|███████▏  | 1176/1645 [1:00:22<08:36,  1.10s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1177/1645 [1:00:23<08:35,  1.10s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1178/1645 [1:00:24<08:35,  1.10s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1179/1645 [1:00:25<08:36,  1.11s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1180/1645 [1:00:26<08:34,  1.11s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1181/1645 [1:00:27<08:31,  1.10s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1182/1645 [1:00:28<08:11,  1.06s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1183/1645 [1:00:29<07:57,  1.03s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1184/1645 [1:00:30<07:46,  1.01s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1185/1645 [1:00:31<07:36,  1.01it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1186/1645 [1:00:32<07:29,  1.02it/s]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1187/1645 [1:00:33<07:44,  1.02s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1188/1645 [1:00:34<07:49,  1.03s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1189/1645 [1:00:35<07:46,  1.02s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1190/1645 [1:00:36<07:42,  1.02s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1191/1645 [1:00:37<07:40,  1.01s/it]\u001b[A\n",
      "Iteration:  72%|███████▏  | 1192/1645 [1:00:38<07:39,  1.01s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1193/1645 [1:00:39<07:39,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1194/1645 [1:00:40<07:37,  1.01s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1195/1645 [1:00:41<07:37,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1196/1645 [1:00:42<07:35,  1.01s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1197/1645 [1:00:43<07:37,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1198/1645 [1:00:44<07:35,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1199/1645 [1:00:45<07:36,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1200/1645 [1:00:46<07:33,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1201/1645 [1:00:47<07:32,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1202/1645 [1:00:48<07:33,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1203/1645 [1:00:49<07:30,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1204/1645 [1:00:51<07:31,  1.02s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1205/1645 [1:00:52<07:32,  1.03s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1206/1645 [1:00:53<07:41,  1.05s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1207/1645 [1:00:54<07:47,  1.07s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1208/1645 [1:00:55<07:50,  1.08s/it]\u001b[A\n",
      "Iteration:  73%|███████▎  | 1209/1645 [1:00:56<07:53,  1.09s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 1210/1645 [1:00:57<07:55,  1.09s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 1211/1645 [1:00:58<07:54,  1.09s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 1212/1645 [1:00:59<07:42,  1.07s/it]\u001b[A\n",
      "Iteration:  74%|███████▎  | 1213/1645 [1:01:00<07:53,  1.10s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1214/1645 [1:01:01<07:42,  1.07s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1215/1645 [1:01:03<07:53,  1.10s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1216/1645 [1:01:04<07:41,  1.08s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1217/1645 [1:01:05<07:32,  1.06s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1218/1645 [1:01:06<07:26,  1.04s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1219/1645 [1:01:07<07:32,  1.06s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1220/1645 [1:01:08<07:37,  1.08s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1221/1645 [1:01:09<07:41,  1.09s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1222/1645 [1:01:10<07:42,  1.09s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1223/1645 [1:01:11<07:44,  1.10s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1224/1645 [1:01:12<07:32,  1.07s/it]\u001b[A\n",
      "Iteration:  74%|███████▍  | 1225/1645 [1:01:13<07:23,  1.06s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1226/1645 [1:01:14<07:16,  1.04s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1227/1645 [1:01:15<07:10,  1.03s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1228/1645 [1:01:16<07:07,  1.02s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1229/1645 [1:01:17<07:23,  1.07s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1230/1645 [1:01:19<07:34,  1.10s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1231/1645 [1:01:20<07:41,  1.11s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1232/1645 [1:01:21<07:46,  1.13s/it]\u001b[A\n",
      "Iteration:  75%|███████▍  | 1233/1645 [1:01:22<07:48,  1.14s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1234/1645 [1:01:23<07:42,  1.13s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1235/1645 [1:01:24<07:39,  1.12s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1236/1645 [1:01:25<07:32,  1.11s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1237/1645 [1:01:26<07:13,  1.06s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1238/1645 [1:01:27<07:01,  1.04s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1239/1645 [1:01:28<06:53,  1.02s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1240/1645 [1:01:29<06:45,  1.00s/it]\u001b[A\n",
      "Iteration:  75%|███████▌  | 1241/1645 [1:01:30<06:38,  1.01it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1242/1645 [1:01:31<06:35,  1.02it/s]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1243/1645 [1:01:32<06:48,  1.02s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1244/1645 [1:01:33<06:57,  1.04s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1245/1645 [1:01:34<07:05,  1.06s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1246/1645 [1:01:35<07:11,  1.08s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1247/1645 [1:01:37<07:14,  1.09s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1248/1645 [1:01:38<07:14,  1.10s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1249/1645 [1:01:39<07:14,  1.10s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1250/1645 [1:01:40<07:15,  1.10s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1251/1645 [1:01:41<07:16,  1.11s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1252/1645 [1:01:42<07:17,  1.11s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1253/1645 [1:01:43<07:16,  1.11s/it]\u001b[A\n",
      "Iteration:  76%|███████▌  | 1254/1645 [1:01:44<07:14,  1.11s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 1255/1645 [1:01:46<07:14,  1.11s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 1256/1645 [1:01:47<07:11,  1.11s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 1257/1645 [1:01:48<07:21,  1.14s/it]\u001b[A\n",
      "Iteration:  76%|███████▋  | 1258/1645 [1:01:49<07:18,  1.13s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1259/1645 [1:01:50<07:14,  1.13s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1260/1645 [1:01:51<07:10,  1.12s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1261/1645 [1:01:52<07:06,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1262/1645 [1:01:53<07:03,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1263/1645 [1:01:54<07:02,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1264/1645 [1:01:56<07:02,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1265/1645 [1:01:57<07:01,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1266/1645 [1:01:58<07:00,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1267/1645 [1:01:59<06:58,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1268/1645 [1:02:00<06:56,  1.10s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1269/1645 [1:02:01<06:56,  1.11s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1270/1645 [1:02:02<06:53,  1.10s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1271/1645 [1:02:03<06:43,  1.08s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1272/1645 [1:02:04<06:35,  1.06s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1273/1645 [1:02:05<06:38,  1.07s/it]\u001b[A\n",
      "Iteration:  77%|███████▋  | 1274/1645 [1:02:06<06:42,  1.09s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1275/1645 [1:02:08<06:45,  1.09s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1276/1645 [1:02:09<06:45,  1.10s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1277/1645 [1:02:10<06:45,  1.10s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1278/1645 [1:02:11<06:46,  1.11s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1279/1645 [1:02:12<06:45,  1.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  78%|███████▊  | 1280/1645 [1:05:57<6:56:07, 68.40s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1281/1645 [1:05:59<4:52:38, 48.24s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1282/1645 [1:06:00<3:26:25, 34.12s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1283/1645 [1:06:01<2:25:57, 24.19s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1284/1645 [1:06:02<1:43:43, 17.24s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1285/1645 [1:06:03<1:14:25, 12.40s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1286/1645 [1:06:04<53:56,  9.01s/it]  \u001b[A\n",
      "Iteration:  78%|███████▊  | 1287/1645 [1:06:05<39:36,  6.64s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1288/1645 [1:06:06<29:37,  4.98s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1289/1645 [1:06:07<22:37,  3.81s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1290/1645 [1:06:08<17:30,  2.96s/it]\u001b[A\n",
      "Iteration:  78%|███████▊  | 1291/1645 [1:06:09<14:10,  2.40s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 1292/1645 [1:06:11<11:50,  2.01s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 1293/1645 [1:06:12<10:00,  1.71s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 1294/1645 [1:06:13<09:04,  1.55s/it]\u001b[A\n",
      "Iteration:  79%|███████▊  | 1295/1645 [1:06:14<08:16,  1.42s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1296/1645 [1:06:15<07:42,  1.33s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1297/1645 [1:06:16<07:19,  1.26s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1298/1645 [1:06:17<07:01,  1.21s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1299/1645 [1:06:18<06:48,  1.18s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1300/1645 [1:06:19<06:39,  1.16s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1301/1645 [1:06:20<06:32,  1.14s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1302/1645 [1:06:21<06:12,  1.08s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1303/1645 [1:06:22<05:59,  1.05s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1304/1645 [1:06:23<05:49,  1.02s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1305/1645 [1:06:24<05:43,  1.01s/it]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1306/1645 [1:06:25<05:38,  1.00it/s]\u001b[A\n",
      "Iteration:  79%|███████▉  | 1307/1645 [1:06:26<05:47,  1.03s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1308/1645 [1:06:28<05:54,  1.05s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1309/1645 [1:06:29<05:55,  1.06s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1310/1645 [1:06:30<05:59,  1.07s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1311/1645 [1:06:31<06:01,  1.08s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1312/1645 [1:06:32<05:59,  1.08s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1313/1645 [1:06:33<05:45,  1.04s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1314/1645 [1:06:34<05:36,  1.02s/it]\u001b[A\n",
      "Iteration:  80%|███████▉  | 1315/1645 [1:06:35<05:44,  1.05s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1316/1645 [1:06:36<05:50,  1.06s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1317/1645 [1:06:37<05:53,  1.08s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1318/1645 [1:06:38<05:47,  1.06s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1319/1645 [1:06:39<05:44,  1.06s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1320/1645 [1:06:40<05:39,  1.04s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1321/1645 [1:06:41<05:35,  1.03s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1322/1645 [1:06:42<05:34,  1.03s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1323/1645 [1:06:43<05:46,  1.07s/it]\u001b[A\n",
      "Iteration:  80%|████████  | 1324/1645 [1:06:45<05:53,  1.10s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1325/1645 [1:06:46<05:57,  1.12s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1326/1645 [1:06:47<05:48,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1327/1645 [1:06:48<05:39,  1.07s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1328/1645 [1:06:49<05:33,  1.05s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1329/1645 [1:06:50<05:42,  1.08s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1330/1645 [1:06:51<05:49,  1.11s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1331/1645 [1:06:52<05:54,  1.13s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1332/1645 [1:06:53<05:51,  1.12s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1333/1645 [1:06:54<05:38,  1.09s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1334/1645 [1:06:55<05:31,  1.06s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1335/1645 [1:06:56<05:27,  1.06s/it]\u001b[A\n",
      "Iteration:  81%|████████  | 1336/1645 [1:06:58<05:30,  1.07s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 1337/1645 [1:06:59<05:33,  1.08s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 1338/1645 [1:07:00<05:24,  1.06s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 1339/1645 [1:07:01<05:20,  1.05s/it]\u001b[A\n",
      "Iteration:  81%|████████▏ | 1340/1645 [1:07:02<05:15,  1.04s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1341/1645 [1:07:03<05:13,  1.03s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1342/1645 [1:07:04<05:19,  1.05s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1343/1645 [1:07:05<05:22,  1.07s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1344/1645 [1:07:06<05:24,  1.08s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1345/1645 [1:07:07<05:26,  1.09s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1346/1645 [1:07:08<05:27,  1.10s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1347/1645 [1:07:09<05:28,  1.10s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1348/1645 [1:07:10<05:15,  1.06s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1349/1645 [1:07:11<05:06,  1.03s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1350/1645 [1:07:12<04:57,  1.01s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1351/1645 [1:07:13<05:14,  1.07s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1352/1645 [1:07:15<05:21,  1.10s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1353/1645 [1:07:16<05:14,  1.08s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1354/1645 [1:07:17<05:08,  1.06s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1355/1645 [1:07:18<05:04,  1.05s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1356/1645 [1:07:19<05:00,  1.04s/it]\u001b[A\n",
      "Iteration:  82%|████████▏ | 1357/1645 [1:07:20<04:56,  1.03s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1358/1645 [1:07:21<04:55,  1.03s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1359/1645 [1:07:22<04:54,  1.03s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1360/1645 [1:07:23<04:52,  1.03s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1361/1645 [1:07:24<04:51,  1.03s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1362/1645 [1:07:25<04:49,  1.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1363/1645 [1:07:26<04:48,  1.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1364/1645 [1:07:27<04:47,  1.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1365/1645 [1:07:28<04:45,  1.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1366/1645 [1:07:29<04:44,  1.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1367/1645 [1:07:30<04:42,  1.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1368/1645 [1:07:31<04:42,  1.02s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1369/1645 [1:07:32<04:43,  1.03s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1370/1645 [1:07:33<04:52,  1.06s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1371/1645 [1:07:34<04:59,  1.09s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1372/1645 [1:07:35<04:58,  1.10s/it]\u001b[A\n",
      "Iteration:  83%|████████▎ | 1373/1645 [1:07:37<04:59,  1.10s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 1374/1645 [1:07:38<04:58,  1.10s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 1375/1645 [1:07:39<04:58,  1.11s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 1376/1645 [1:07:40<04:57,  1.11s/it]\u001b[A\n",
      "Iteration:  84%|████████▎ | 1377/1645 [1:07:41<04:57,  1.11s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1378/1645 [1:07:42<04:56,  1.11s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1379/1645 [1:07:43<04:55,  1.11s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1380/1645 [1:07:44<04:42,  1.07s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1381/1645 [1:07:45<04:35,  1.04s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1382/1645 [1:07:46<04:29,  1.02s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1383/1645 [1:07:47<04:42,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1384/1645 [1:07:48<04:36,  1.06s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1385/1645 [1:07:49<04:35,  1.06s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1386/1645 [1:07:50<04:32,  1.05s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1387/1645 [1:07:52<04:35,  1.07s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1388/1645 [1:07:53<04:37,  1.08s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1389/1645 [1:07:54<04:38,  1.09s/it]\u001b[A\n",
      "Iteration:  84%|████████▍ | 1390/1645 [1:07:55<04:39,  1.10s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1391/1645 [1:07:56<04:39,  1.10s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1392/1645 [1:07:57<04:32,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1393/1645 [1:07:58<04:33,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1394/1645 [1:07:59<04:23,  1.05s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1395/1645 [1:08:00<04:14,  1.02s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1396/1645 [1:08:01<04:09,  1.00s/it]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1397/1645 [1:08:02<04:06,  1.00it/s]\u001b[A\n",
      "Iteration:  85%|████████▍ | 1398/1645 [1:08:03<04:07,  1.00s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1399/1645 [1:08:04<04:08,  1.01s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1400/1645 [1:08:05<04:06,  1.01s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1401/1645 [1:08:06<04:05,  1.01s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1402/1645 [1:08:07<04:14,  1.05s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1403/1645 [1:08:08<04:21,  1.08s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1404/1645 [1:08:09<04:22,  1.09s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1405/1645 [1:08:10<04:14,  1.06s/it]\u001b[A\n",
      "Iteration:  85%|████████▌ | 1406/1645 [1:08:11<04:10,  1.05s/it]\u001b[A\n",
      "Iteration:  86%|████████▌ | 1407/1645 [1:08:12<04:07,  1.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-61f854c1d053>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcreate_true_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "def create_true_logits(logits, labels):\n",
    "    out = torch.zeros_like(logits)\n",
    "    for i, label in enumerate(labels):\n",
    "        out[i, label]=1\n",
    "    return out\n",
    "\n",
    "create_true_logits(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,1,2 classes (irrelevant - misleading)\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    #print(outputs,outputs == labels)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "# group data by question -> we want to look at the accuracy and f1 score for each question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "# model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distil_model():\n",
    "    \n",
    "    for name, param in distil_model.named_parameters():\n",
    "#         param.requires_grad = False\n",
    "        ln = 24\n",
    "        if name.startswith('distilbert.encoder'):\n",
    "        \tl = name.split('.')\n",
    "        \tln = int(l[3])\n",
    "      \n",
    "        if name.startswith('distilbert.embeddings') or ln < 6:\n",
    "#         \tprint(name)  \n",
    "        \tparam.requires_grad = False\n",
    "    \n",
    "    distil_model.to(device)\n",
    "    \n",
    "    return distil_model\n",
    "\n",
    "# from utils.tokenization import BertTokenizer tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modeling import BertEmbeddings\n",
    "\n",
    "class RaceDistilBert(DistilBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(RaceDistilBert, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = self.transformer\n",
    "    \n",
    "    def forward(self, input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=None,\n",
    "                output_all_encoded_layers=True,\n",
    "                inputs_embeds=None):\n",
    "        '''overwrite forward method'''\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        P_att = torch.zeros_like(input_ids)\n",
    "        Q_att = torch.zeros_like(input_ids)\n",
    "        A_att = torch.zeros_like(input_ids)\n",
    "        token_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        P_att[token_type_ids==0] = 1\n",
    "        Q_att[token_type_ids==1] = 1\n",
    "        A_att[token_type_ids==2] = 1\n",
    "\n",
    "        token_ids[token_type_ids > 0] = 1\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        P_att = P_att.unsqueeze(1).unsqueeze(2)\n",
    "        P_att = P_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        P_att = (1.0 - P_att) * -10000.0\n",
    "\n",
    "        Q_att = Q_att.unsqueeze(1).unsqueeze(2)\n",
    "        Q_att = Q_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        Q_att = (1.0 - Q_att) * -10000.0\n",
    "\n",
    "        A_att = A_att.unsqueeze(1).unsqueeze(2)\n",
    "        A_att = A_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        A_att = (1.0 - A_att) * -10000.0\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            embedding_output = self.embeddings(input_ids, token_ids)\n",
    "        else:\n",
    "            embedding_output = inputs_embeds\n",
    "#             print(f'shape inputs_embeds: {len(inputs_embeds)}, and shape embeddings: {len(self.embeddings(input_ids, token_ids))}')\n",
    "\n",
    "#         self.noise = self.generate_noise(embedding_output, attention_mask[0], epsilon=1e-5)\n",
    "        \n",
    "        print(f'embedding output: {embedding_output}')\n",
    "        return embedding_output, extended_attention_mask,output_all_encoded_layers\n",
    "        try:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers=output_all_encoded_layers)\n",
    "        except TypeError as e:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_hidden_states=output_all_encoded_layers)\n",
    "        \n",
    "#         self.adv_encoded_layers = self.encoder(embedding_output+self.noise,\n",
    "#                                       extended_attention_mask,\n",
    "#                                       output_all_encoded_layers=output_all_encoded_layers)\n",
    "        \n",
    "        \n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "\n",
    "        return extended_attention_mask,P_att,Q_att,A_att,sequence_output\n",
    "    \n",
    "class RaceDistilBertMultipleChoice(DistilBertForMultipleChoice):\n",
    "    def __init__(self, config):\n",
    "        super(RaceDistilBertMultipleChoice, self).__init__(config)\n",
    "        self.distilbert = dmodel\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = RaceDistilBert.from_pretrained('distilbert-base-uncased')\n",
    "dmodel_mc = RaceDistilBertMultipleChoice.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel_mc.distilbert.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMultipleChoice, BertModel\n",
    "from utils.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "distil_model = RaceDistilBertMultipleChoice.from_pretrained('distilbert-base-uncased',\n",
    "                                              cache_dir=os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtrain():\n",
    "    default_config = {\n",
    "             'multi_gpu_on':False,\n",
    "             'max_seq_length': 64,\n",
    "             'adam_eps': 6, \n",
    "             'adv_epsilon': 1e-6,\n",
    "             'adv_train': 1,\n",
    "             'adv_noise_var': 1e-5,\n",
    "             'adv_norm_level': 0,\n",
    "             'adv_step_size': 1e-3,\n",
    "             'bin_on': False,\n",
    "             'cuda': 1,\n",
    "             'encoder_type': None,\n",
    "             'fp16': True,\n",
    "             'fp16_opt_level': 'O1',\n",
    "             'global_grad_clipping': 1.0,\n",
    "             'grad_accumulation_step': 1,\n",
    "             'grad_clipping': 0,\n",
    "             'local_rank': -1,\n",
    "             'mkd_opt': 0,\n",
    "             'scheduler_type': 'ms',\n",
    "             'task_def_list': None,\n",
    "             'warmup': 0.1,\n",
    "             'warmup_schedule': 'warmup_linear',\n",
    "             'weight_decay': 0,\n",
    "             'weighted_on': False,\n",
    "             'state_dict': None,\n",
    "             'loss': 'LossCriterion.CeCriterion',\n",
    "             'epochs': 3,\n",
    "             'batch_size': 4,\n",
    "             'adv_k': 1,\n",
    "             'learning_rate':5e-5,\n",
    "             'adv_p_norm': 'inf',\n",
    "             'adv_alpha': 1,\n",
    "             'optimizer': 'radam',\n",
    "             'adv_loss': 'LossCriterion.SymKlCriterion',\n",
    "             'weight': 0\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"master-thesis\", config=default_config)\n",
    "    \n",
    "    device = default_config['cuda']\n",
    "    \n",
    "    config = wandb.config\n",
    "    \n",
    "    # build_dataset\n",
    "    train_dataloader, eval_dataloader = build_dataset(config)\n",
    "    \n",
    "    # create model\n",
    "    model = create_distil_model()\n",
    "    \n",
    "    # initialize model and losses\n",
    "    [mnetwork,\n",
    "     task_loss_criterion,\n",
    "     adv_task_loss_criterion,\n",
    "     adv_teacher,\n",
    "     optimizer_parameters,\n",
    "     optimizer,\n",
    "     scheduler] = _model_init(config=config, model=model, num_train_step=len(train_dataloader))\n",
    "    \n",
    "    return [config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler]\n",
    "\n",
    "    # training mode ON\n",
    "    model.train()\n",
    "    \n",
    "    # wandb watch\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # train/eval\n",
    "    training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dtrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=training_loop(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = params[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.distilbert.embeddings(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.distilbert.embeddings(input_ids[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_untrained = DistilBertForMultipleChoice(net.config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_untrained.distilbert.embeddings(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(net.distilbert.embeddings.forward), help(network.distilbert.embeddings.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(distil_model.to(device).distilbert.embeddings(input_ids[0]).cpu().detach().numpy() - net_untrained.distilbert.embeddings(input_ids[0]).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.distilbert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'large_models'\n",
    "\n",
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_train_loss, test_train_accuracy = 0, 0\n",
    "nb_test_train_steps, nb_test_train_examples = 0, 0\n",
    "test_train_total_logits = []\n",
    "test_train_total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(train_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            test_train_total_logits.append(logits)\n",
    "            test_train_total_labels.append(label_ids)\n",
    "\t\n",
    "test_train_total_logits = np.concatenate(total_logits)\n",
    "test_train_total_labels = np.concatenate(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "# np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"train_acc_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_examples = read_race(\"./RACE/dev\")\n",
    "# eval_features = convert_examples_to_features(\n",
    "#             eval_examples, tokenizer, args.max_seq_length, True)\n",
    "#         logger.info(\"***** Running evaluation *****\")\n",
    "#         logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "#         logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "#         all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n",
    "#         all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n",
    "#         all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n",
    "\n",
    "#         all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
    "#         eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
    "        # Run prediction for full data\n",
    "\n",
    "if True:\n",
    "    dev_examples = read_data(data_location,'dev')\n",
    "    test_examples = read_data(data_location,'test')\n",
    "\n",
    "if True:\n",
    "    test_features = convert_examples_to_features(\n",
    "                test_examples, tokenizer, max_seq_length, True)\n",
    "    dev_features = convert_examples_to_features(\n",
    "            dev_examples, tokenizer, max_seq_length, True)\n",
    "                \n",
    "if True:\n",
    "    test_data = build_tensor(test_features)\n",
    "    dev_data = build_tensor(dev_features)\n",
    "eval_sampler = SequentialSampler(dev_data)\n",
    "eval_dataloader = DataLoader(dev_data, sampler=eval_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "total_logits = np.concatenate(total_logits)\n",
    "total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/logits.npy\",total_logits)\n",
    "np.save(output_dir+\"/labels.npy\",total_labels)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy}\n",
    "\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_total_logits = [items for labels in total_logits for items in labels]\n",
    "assembled_total_logits = np.array(assembled_total_logits)\n",
    "\n",
    "assembled_total_labels = [items for labels in total_labels for items in labels]\n",
    "assembled_total_labels = np.array(assembled_total_labels)\n",
    "assembled_total_labels, assembled_total_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report as cls_report\n",
    "print(cls_report(test_train_total_labels, np.argmax(test_train_total_logits, axis=1)))\n",
    "print(cls_report(assembled_total_labels, np.argmax(assembled_total_logits, axis=1)))\n",
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('identification fails'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "chosen_features = convert_examples_to_features(chosen_examples, tokenizer, max_seq_length, is_training=False, debug=False)\n",
    "\n",
    "chosen_data = build_tensor(chosen_features)\n",
    "\n",
    "chosen_sampler = SequentialSampler(chosen_data)\n",
    "chosen_dataloader = DataLoader(chosen_data, sampler=chosen_sampler, batch_size=32)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(chosen_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "# output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "# with open(output_test_file, \"w\") as writer:\n",
    "# #             logger.info(\"***** Test results *****\")\n",
    "#             for key in sorted(result.keys()):\n",
    "# #                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                 writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_total_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(train_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('i cannot find my ope'):\n",
    "        chosen_examples.append(example)\n",
    "        print(example.endings[example.label])\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('i cannot find my ope'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = convert_examples_to_features(chosen_examples, tokenizer, max_seq_length, is_training=False, debug=False)\n",
    "\n",
    "chosen_data = build_tensor(chosen_features)\n",
    "\n",
    "chosen_sampler = SequentialSampler(chosen_data)\n",
    "chosen_dataloader = DataLoader(chosen_data, sampler=chosen_sampler, batch_size=32)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(chosen_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "# output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "# with open(output_test_file, \"w\") as writer:\n",
    "# #             logger.info(\"***** Test results *****\")\n",
    "#             for key in sorted(result.keys()):\n",
    "# #                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                 writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_total_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('The {FASTA}'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [example.context_sentence for example in chosen_examples]\n",
    "distribution = [example.label for example in chosen_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(contexts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
