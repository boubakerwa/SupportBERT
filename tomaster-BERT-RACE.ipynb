{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Training Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# %pip install bert_race/requirements.txt -qqq\n",
    "%pip install transformers==4.11.3 -qqq\n",
    "!pip install pytorch_pretrained_bert==0.4.0 -qqq\n",
    "# %conda install -c conda-forge ipywidgets\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %pip install transformers --upgrade\n",
    "\n",
    "!pip install wandb -qqq\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from loss import LossCriterion, LOSS_REGISTRY\n",
    "\n",
    "from module.bert_optim import RAdam\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# del variables\n",
    "gc.collect()\n",
    "\n",
    "device_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from utils.tokenization_utils import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_race.race import Race, read_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'support-bert-data'\n",
    "folder = 'race-dataset'\n",
    "mode = 'train'\n",
    "data_location = f's3://{bucket}/{folder}/{mode}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_race_samples = read_race(data_location, 'middle/middle')\n",
    "#train_features = convert_examples_to_features(\n",
    "                #train_race_samples, tokenizer, 128, True)\n",
    "#train_data = build_tensor(train_features)\n",
    "#train_sampler = SequentialSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                             sampler=train_sampler,\n",
    "                             batch_size=2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'support-bert-data'\n",
    "folder = 'race-dataset'\n",
    "mode = 'dev'\n",
    "data_location = f's3://{bucket}/{folder}/{mode}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sorted: support-bert-data/race-dataset/dev/middle/middle/13.txt\n",
      "1436 race_id: support-bert-data/race-dataset/dev/middle/middle/8280:0, context_sentence: What is color? Why do some of the things around us look red, some green, others blue?\n",
      "Colors are really made by deflected   light. We see color because most of the things reflect light. In the same way, if something is green, it reflects most of the green light. If something reflects all light, it is white. If it doesn't reflect any light, it is black.\n",
      "Some of the light is reflected and some is taken in   and turned into   heat  .The darker the color is, the less light is reflected, the more light is taken in. So dark-colored clothes are warmer in the sun than light-colored clothes., start_ending: When something reflects light, we can   _  ., ending_0: see its color, ending_1: see its heat, ending_2: not see its color, ending_3: see nothing, label: 0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "val_race_samples = read_race(data_location, 'middle/middle')\n",
    "dev_features = convert_examples_to_features(\n",
    "                val_race_samples, tokenizer, 128, True)\n",
    "dev_data = build_tensor(dev_features)\n",
    "eval_sampler = SequentialSampler(dev_data)\n",
    "eval_dataloader = DataLoader(dev_data,\n",
    "                             sampler=eval_sampler,\n",
    "                             batch_size=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r bert_race/requirements.txt\n",
    "# %cp bert_race/pytorch_pretrained_bert/tokenization.py utils/tokenization.py\n",
    "# %cp bert_race/pytorch_pretrained_bert/file_utils.py utils/file_utils.py\n",
    "\n",
    "from utils.tokenization import BertTokenizer \n",
    "from utils.tokenization_utils import convert_examples_to_features\n",
    "from data_utils.utils import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/24/2021 13:55:28 - INFO - utils.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "do_lower_case = True\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# %cp bert_race/pytorch_pretrained_bert/modeling.py utils/modeling.py\n",
    "# %cp bert_race/pytorch_pretrained_bert/optimization.py utils/optimization.py\n",
    "\n",
    "import torch\n",
    "from utils.modeling import BertForMultipleChoice, BertConfig, BertEmbeddings\n",
    "from utils.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from utils.optimization import BertAdam, WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset, IterableDataset)\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tokenization_utils import build_tensor\n",
    "def select_field(features, field):\n",
    "    return [\n",
    "        [\n",
    "            choice[field]\n",
    "            for choice in feature.choices_features\n",
    "        ]\n",
    "        for feature in features\n",
    "    ]\n",
    "\n",
    "def build_tensor(features):\n",
    "    all_input_ids = torch.tensor(select_field(features, 'input_ids'),\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(select_field(features, 'input_mask'),\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(select_field(features, 'segment_ids'),\n",
    "                                   dtype=torch.long)\n",
    "    all_label = torch.tensor([f.label for f in features],\n",
    "                             dtype=torch.long)\n",
    "    return TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "seed = 30\n",
    "do_lower_case = True\n",
    "bert_model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "print(n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare optimizer\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "\n",
    "# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#         ]\n",
    "    \n",
    "\n",
    "# global_step = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_examples = read_data(data_location,'train')\n",
    "# dev_examples = read_data(data_location,'dev')\n",
    "# test_examples = read_data(data_location,'test')\n",
    "\n",
    "# dev_train_examples = train_examples[:16]\n",
    "\n",
    "def build_dataset(config):\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, tokenizer, config['max_seq_length'], True)\n",
    "    dev_features = convert_examples_to_features(\n",
    "                dev_examples, tokenizer, config['max_seq_length'], True)\n",
    "    test_features = convert_examples_to_features(\n",
    "                test_examples, tokenizer, config['max_seq_length'], True)\n",
    "    dev_train_features = convert_examples_to_features(\n",
    "        train_examples, tokenizer, config['max_seq_length'], True)\n",
    "\n",
    "    train_data = build_tensor(train_features)\n",
    "    dev_data = build_tensor(dev_features)\n",
    "    test_data = build_tensor(test_features)\n",
    "    dev_train_data = build_tensor(dev_train_features)\n",
    "    \n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=config['batch_size'])\n",
    "    \n",
    "    eval_sampler = SequentialSampler(dev_data)\n",
    "    eval_dataloader = DataLoader(dev_data,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=2)    \n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data,\n",
    "                                 sampler=test_sampler,\n",
    "                                 batch_size=32)\n",
    "    \n",
    "    dev_train_sampler = SequentialSampler(dev_train_data)\n",
    "    dev_train_dataloader = DataLoader(dev_data,\n",
    "                                 sampler=dev_train_sampler,\n",
    "                                 batch_size=2) \n",
    "    \n",
    "    return train_dataloader, eval_dataloader, test_dataloader, dev_train_dataloader\n",
    "# [train_dataloader,\n",
    "#  eval_dataloader,\n",
    "#  test_dataloader,\n",
    "#  dev_train_dataloader] = build_dataset({'max_seq_length': 128,\n",
    "#                                         'batch_size':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/24/2021 13:55:42 - INFO - utils.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "10/24/2021 13:55:42 - INFO - utils.modeling -   extracting archive file /root/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpfj_p8o1p\n",
      "10/24/2021 13:55:45 - INFO - utils.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "10/24/2021 13:55:49 - INFO - utils.modeling -   Weights of BertForMultipleChoice not initialized from pretrained model: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'decoder.layer.0.attention.self.query.weight', 'decoder.layer.0.attention.self.query.bias', 'decoder.layer.0.attention.self.key.weight', 'decoder.layer.0.attention.self.key.bias', 'decoder.layer.0.attention.self.value.weight', 'decoder.layer.0.attention.self.value.bias', 'decoder.layer.0.attention.self.qquery.weight', 'decoder.layer.0.attention.self.qquery.bias', 'decoder.layer.0.attention.self.qkey.weight', 'decoder.layer.0.attention.self.qkey.bias', 'decoder.layer.0.attention.self.qvalue.weight', 'decoder.layer.0.attention.self.qvalue.bias', 'decoder.layer.0.attention.self.query_n.weight', 'decoder.layer.0.attention.self.query_n.bias', 'decoder.layer.0.attention.self.key_n.weight', 'decoder.layer.0.attention.self.key_n.bias', 'decoder.layer.0.attention.self.value_n.weight', 'decoder.layer.0.attention.self.value_n.bias', 'decoder.layer.0.attention.self.aquery.weight', 'decoder.layer.0.attention.self.aquery.bias', 'decoder.layer.0.attention.self.akey.weight', 'decoder.layer.0.attention.self.akey.bias', 'decoder.layer.0.attention.self.avalue.weight', 'decoder.layer.0.attention.self.avalue.bias', 'decoder.layer.0.attention.output.dense.weight', 'decoder.layer.0.attention.output.dense.bias', 'decoder.layer.0.attention.output.LayerNorm.weight', 'decoder.layer.0.attention.output.LayerNorm.bias', 'decoder.layer.0.attention.output.qdense.weight', 'decoder.layer.0.attention.output.qdense.bias', 'decoder.layer.0.attention.output.qLayerNorm.weight', 'decoder.layer.0.attention.output.qLayerNorm.bias', 'decoder.layer.0.intermediate.dense.weight', 'decoder.layer.0.intermediate.dense.bias', 'decoder.layer.0.intermediate.qdense.weight', 'decoder.layer.0.intermediate.qdense.bias', 'decoder.layer.0.output.dense.weight', 'decoder.layer.0.output.dense.bias', 'decoder.layer.0.output.LayerNorm.weight', 'decoder.layer.0.output.LayerNorm.bias', 'decoder.layer.0.output.qdense.weight', 'decoder.layer.0.output.qdense.bias', 'decoder.layer.0.output.qLayerNorm.weight', 'decoder.layer.0.output.qLayerNorm.bias', 'decoder.layer.0.output.adense.weight', 'decoder.layer.0.output.adense.bias', 'decoder.layer.0.output.aLayerNorm.weight', 'decoder.layer.0.output.aLayerNorm.bias', 'decoder.layer.1.attention.self.query.weight', 'decoder.layer.1.attention.self.query.bias', 'decoder.layer.1.attention.self.key.weight', 'decoder.layer.1.attention.self.key.bias', 'decoder.layer.1.attention.self.value.weight', 'decoder.layer.1.attention.self.value.bias', 'decoder.layer.1.attention.self.qquery.weight', 'decoder.layer.1.attention.self.qquery.bias', 'decoder.layer.1.attention.self.qkey.weight', 'decoder.layer.1.attention.self.qkey.bias', 'decoder.layer.1.attention.self.qvalue.weight', 'decoder.layer.1.attention.self.qvalue.bias', 'decoder.layer.1.attention.self.query_n.weight', 'decoder.layer.1.attention.self.query_n.bias', 'decoder.layer.1.attention.self.key_n.weight', 'decoder.layer.1.attention.self.key_n.bias', 'decoder.layer.1.attention.self.value_n.weight', 'decoder.layer.1.attention.self.value_n.bias', 'decoder.layer.1.attention.self.aquery.weight', 'decoder.layer.1.attention.self.aquery.bias', 'decoder.layer.1.attention.self.akey.weight', 'decoder.layer.1.attention.self.akey.bias', 'decoder.layer.1.attention.self.avalue.weight', 'decoder.layer.1.attention.self.avalue.bias', 'decoder.layer.1.attention.output.dense.weight', 'decoder.layer.1.attention.output.dense.bias', 'decoder.layer.1.attention.output.LayerNorm.weight', 'decoder.layer.1.attention.output.LayerNorm.bias', 'decoder.layer.1.attention.output.qdense.weight', 'decoder.layer.1.attention.output.qdense.bias', 'decoder.layer.1.attention.output.qLayerNorm.weight', 'decoder.layer.1.attention.output.qLayerNorm.bias', 'decoder.layer.1.intermediate.dense.weight', 'decoder.layer.1.intermediate.dense.bias', 'decoder.layer.1.intermediate.qdense.weight', 'decoder.layer.1.intermediate.qdense.bias', 'decoder.layer.1.output.dense.weight', 'decoder.layer.1.output.dense.bias', 'decoder.layer.1.output.LayerNorm.weight', 'decoder.layer.1.output.LayerNorm.bias', 'decoder.layer.1.output.qdense.weight', 'decoder.layer.1.output.qdense.bias', 'decoder.layer.1.output.qLayerNorm.weight', 'decoder.layer.1.output.qLayerNorm.bias', 'decoder.layer.1.output.adense.weight', 'decoder.layer.1.output.adense.bias', 'decoder.layer.1.output.aLayerNorm.weight', 'decoder.layer.1.output.aLayerNorm.bias', 'decoder.layer.2.attention.self.query.weight', 'decoder.layer.2.attention.self.query.bias', 'decoder.layer.2.attention.self.key.weight', 'decoder.layer.2.attention.self.key.bias', 'decoder.layer.2.attention.self.value.weight', 'decoder.layer.2.attention.self.value.bias', 'decoder.layer.2.attention.self.qquery.weight', 'decoder.layer.2.attention.self.qquery.bias', 'decoder.layer.2.attention.self.qkey.weight', 'decoder.layer.2.attention.self.qkey.bias', 'decoder.layer.2.attention.self.qvalue.weight', 'decoder.layer.2.attention.self.qvalue.bias', 'decoder.layer.2.attention.self.query_n.weight', 'decoder.layer.2.attention.self.query_n.bias', 'decoder.layer.2.attention.self.key_n.weight', 'decoder.layer.2.attention.self.key_n.bias', 'decoder.layer.2.attention.self.value_n.weight', 'decoder.layer.2.attention.self.value_n.bias', 'decoder.layer.2.attention.self.aquery.weight', 'decoder.layer.2.attention.self.aquery.bias', 'decoder.layer.2.attention.self.akey.weight', 'decoder.layer.2.attention.self.akey.bias', 'decoder.layer.2.attention.self.avalue.weight', 'decoder.layer.2.attention.self.avalue.bias', 'decoder.layer.2.attention.output.dense.weight', 'decoder.layer.2.attention.output.dense.bias', 'decoder.layer.2.attention.output.LayerNorm.weight', 'decoder.layer.2.attention.output.LayerNorm.bias', 'decoder.layer.2.attention.output.qdense.weight', 'decoder.layer.2.attention.output.qdense.bias', 'decoder.layer.2.attention.output.qLayerNorm.weight', 'decoder.layer.2.attention.output.qLayerNorm.bias', 'decoder.layer.2.intermediate.dense.weight', 'decoder.layer.2.intermediate.dense.bias', 'decoder.layer.2.intermediate.qdense.weight', 'decoder.layer.2.intermediate.qdense.bias', 'decoder.layer.2.output.dense.weight', 'decoder.layer.2.output.dense.bias', 'decoder.layer.2.output.LayerNorm.weight', 'decoder.layer.2.output.LayerNorm.bias', 'decoder.layer.2.output.qdense.weight', 'decoder.layer.2.output.qdense.bias', 'decoder.layer.2.output.qLayerNorm.weight', 'decoder.layer.2.output.qLayerNorm.bias', 'decoder.layer.2.output.adense.weight', 'decoder.layer.2.output.adense.bias', 'decoder.layer.2.output.aLayerNorm.weight', 'decoder.layer.2.output.aLayerNorm.bias', 'decoder.layer.3.attention.self.query.weight', 'decoder.layer.3.attention.self.query.bias', 'decoder.layer.3.attention.self.key.weight', 'decoder.layer.3.attention.self.key.bias', 'decoder.layer.3.attention.self.value.weight', 'decoder.layer.3.attention.self.value.bias', 'decoder.layer.3.attention.self.qquery.weight', 'decoder.layer.3.attention.self.qquery.bias', 'decoder.layer.3.attention.self.qkey.weight', 'decoder.layer.3.attention.self.qkey.bias', 'decoder.layer.3.attention.self.qvalue.weight', 'decoder.layer.3.attention.self.qvalue.bias', 'decoder.layer.3.attention.self.query_n.weight', 'decoder.layer.3.attention.self.query_n.bias', 'decoder.layer.3.attention.self.key_n.weight', 'decoder.layer.3.attention.self.key_n.bias', 'decoder.layer.3.attention.self.value_n.weight', 'decoder.layer.3.attention.self.value_n.bias', 'decoder.layer.3.attention.self.aquery.weight', 'decoder.layer.3.attention.self.aquery.bias', 'decoder.layer.3.attention.self.akey.weight', 'decoder.layer.3.attention.self.akey.bias', 'decoder.layer.3.attention.self.avalue.weight', 'decoder.layer.3.attention.self.avalue.bias', 'decoder.layer.3.attention.output.dense.weight', 'decoder.layer.3.attention.output.dense.bias', 'decoder.layer.3.attention.output.LayerNorm.weight', 'decoder.layer.3.attention.output.LayerNorm.bias', 'decoder.layer.3.attention.output.qdense.weight', 'decoder.layer.3.attention.output.qdense.bias', 'decoder.layer.3.attention.output.qLayerNorm.weight', 'decoder.layer.3.attention.output.qLayerNorm.bias', 'decoder.layer.3.intermediate.dense.weight', 'decoder.layer.3.intermediate.dense.bias', 'decoder.layer.3.intermediate.qdense.weight', 'decoder.layer.3.intermediate.qdense.bias', 'decoder.layer.3.output.dense.weight', 'decoder.layer.3.output.dense.bias', 'decoder.layer.3.output.LayerNorm.weight', 'decoder.layer.3.output.LayerNorm.bias', 'decoder.layer.3.output.qdense.weight', 'decoder.layer.3.output.qdense.bias', 'decoder.layer.3.output.qLayerNorm.weight', 'decoder.layer.3.output.qLayerNorm.bias', 'decoder.layer.3.output.adense.weight', 'decoder.layer.3.output.adense.bias', 'decoder.layer.3.output.aLayerNorm.weight', 'decoder.layer.3.output.aLayerNorm.bias', 'decoder.layer.4.attention.self.query.weight', 'decoder.layer.4.attention.self.query.bias', 'decoder.layer.4.attention.self.key.weight', 'decoder.layer.4.attention.self.key.bias', 'decoder.layer.4.attention.self.value.weight', 'decoder.layer.4.attention.self.value.bias', 'decoder.layer.4.attention.self.qquery.weight', 'decoder.layer.4.attention.self.qquery.bias', 'decoder.layer.4.attention.self.qkey.weight', 'decoder.layer.4.attention.self.qkey.bias', 'decoder.layer.4.attention.self.qvalue.weight', 'decoder.layer.4.attention.self.qvalue.bias', 'decoder.layer.4.attention.self.query_n.weight', 'decoder.layer.4.attention.self.query_n.bias', 'decoder.layer.4.attention.self.key_n.weight', 'decoder.layer.4.attention.self.key_n.bias', 'decoder.layer.4.attention.self.value_n.weight', 'decoder.layer.4.attention.self.value_n.bias', 'decoder.layer.4.attention.self.aquery.weight', 'decoder.layer.4.attention.self.aquery.bias', 'decoder.layer.4.attention.self.akey.weight', 'decoder.layer.4.attention.self.akey.bias', 'decoder.layer.4.attention.self.avalue.weight', 'decoder.layer.4.attention.self.avalue.bias', 'decoder.layer.4.attention.output.dense.weight', 'decoder.layer.4.attention.output.dense.bias', 'decoder.layer.4.attention.output.LayerNorm.weight', 'decoder.layer.4.attention.output.LayerNorm.bias', 'decoder.layer.4.attention.output.qdense.weight', 'decoder.layer.4.attention.output.qdense.bias', 'decoder.layer.4.attention.output.qLayerNorm.weight', 'decoder.layer.4.attention.output.qLayerNorm.bias', 'decoder.layer.4.intermediate.dense.weight', 'decoder.layer.4.intermediate.dense.bias', 'decoder.layer.4.intermediate.qdense.weight', 'decoder.layer.4.intermediate.qdense.bias', 'decoder.layer.4.output.dense.weight', 'decoder.layer.4.output.dense.bias', 'decoder.layer.4.output.LayerNorm.weight', 'decoder.layer.4.output.LayerNorm.bias', 'decoder.layer.4.output.qdense.weight', 'decoder.layer.4.output.qdense.bias', 'decoder.layer.4.output.qLayerNorm.weight', 'decoder.layer.4.output.qLayerNorm.bias', 'decoder.layer.4.output.adense.weight', 'decoder.layer.4.output.adense.bias', 'decoder.layer.4.output.aLayerNorm.weight', 'decoder.layer.4.output.aLayerNorm.bias', 'LayerNorm.weight', 'LayerNorm.bias', 'spa.weight', 'spa.bias', 'sa.weight', 'sa.bias', 'spq.weight', 'spq.bias', 'sq.weight', 'sq.bias', 'dense.weight', 'dense.bias']\n",
      "10/24/2021 13:55:49 - INFO - utils.modeling -   Weights from pretrained model not used in BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertForMultipleChoice.from_pretrained(bert_model,\n",
    "                                              cache_dir=os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(-1)),\n",
    "                                              num_choices=4)\n",
    "    \n",
    "\n",
    "def create_model(device):\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        ln = 24\n",
    "        if name.startswith('bert.encoder'):\n",
    "        \tl = name.split('.')\n",
    "        \tln = int(l[3])\n",
    "      \n",
    "        if name.startswith('bert.embeddings') or ln < 6:\n",
    "#         \tprint(name)  \n",
    "        \tparam.requires_grad = False\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def _model_init(config, model, device, state_dict=None, num_train_step=-1):\n",
    "        total_param = sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
    "        if config['cuda']:\n",
    "            if config['local_rank'] != -1:\n",
    "                model = model.to(device)\n",
    "            else:\n",
    "                model = model.to(device)\n",
    "        network = model\n",
    "        if state_dict:\n",
    "            missing_keys, unexpected_keys = network.load_state_dict(state_dict['state'], strict=False)\n",
    "\n",
    "        optimizer_parameters = _get_param_groups(network)\n",
    "#         try:\n",
    "        optimizer, scheduler = _setup_optim(config,optimizer_parameters, state_dict, num_train_step, network)\n",
    "#         except Exceptio#work: {network}')\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        #if self.config[\"local_rank\"] not in [-1, 0]:\n",
    "        #    torch.distributed.barrier()\n",
    "\n",
    "        if config['local_rank'] != -1:\n",
    "            mnetwork = torch.nn.parallel.DistributedDataParallel(network, device_ids=[self.config[\"local_rank\"]], output_device=self.config[\"local_rank\"], find_unused_parameters=True)\n",
    "        elif config['multi_gpu_on']:\n",
    "            mnetwork = torch.nn.DataParallel(network, device_ids=[0, 1, 2])\n",
    "        else:\n",
    "            mnetwork = network\n",
    "        task_loss_criterion = _setup_lossmap(config)\n",
    "        adv_task_loss_criterion = _setup_adv_lossmap(config)\n",
    "        adv_teacher = _setup_adv_training(config, adv_task_loss_criterion)\n",
    "        \n",
    "        return [mnetwork,\n",
    "                task_loss_criterion,\n",
    "                adv_task_loss_criterion,\n",
    "                adv_teacher,\n",
    "                optimizer_parameters,\n",
    "                optimizer,\n",
    "                scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perturbation import SmartPerturbation\n",
    "\n",
    "def _setup_adv_training(config, adv_task_loss_criterion):\n",
    "        adv_teacher = None\n",
    "        if config.get('adv_train', False):\n",
    "            adv_teacher = SmartPerturbation(config['adv_epsilon'],\n",
    "                    config['multi_gpu_on'],\n",
    "                    config['adv_step_size'],\n",
    "                    config['adv_noise_var'],\n",
    "                    config['adv_p_norm'],\n",
    "                    config['adv_k'],\n",
    "                    config['fp16'],\n",
    "                    config['encoder_type'],\n",
    "                    loss_map=adv_task_loss_criterion,\n",
    "                    norm_level=config['adv_norm_level'])\n",
    "        return adv_teacher\n",
    "            \n",
    "def _setup_adv_lossmap(config):\n",
    "        task_def_list: List[TaskDef] = config['task_def_list']\n",
    "        adv_task_loss_criterion = []\n",
    "        if config.get('adv_train', False):\n",
    "            cs = config['adv_loss']\n",
    "            assert cs in ['LossCriterion.SymKlCriterion', 'LossCriterion.KlCriterion']\n",
    "            if cs == 'LossCriterion.SymKlCriterion':\n",
    "                lc = LOSS_REGISTRY[LossCriterion.SymKlCriterion](name='Adv Loss func of task {}: {}'.format(0, cs))\n",
    "                adv_task_loss_criterion.append(lc)\n",
    "            else:\n",
    "                lc = LOSS_REGISTRY[LossCriterion.KlCriterion](name='Adv Loss func of task {}: {}'.format(0, cs))\n",
    "                adv_task_loss_criterion.append(lc)\n",
    "            return adv_task_loss_criterion\n",
    "            \n",
    "def _setup_lossmap(config):\n",
    "        task_def_list: List[TaskDef] = config['task_def_list']\n",
    "        task_loss_criterion = []\n",
    "        cs = config['loss'] # this loss has later to be passed through config file\n",
    "        if cs=='LossCriterion.CeCriterion':\n",
    "            lc = LOSS_REGISTRY[LossCriterion.CeCriterion](name='Loss func of task {}: {}'.format(0, cs)) \n",
    "            task_loss_criterion.append(lc)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return task_loss_criterion\n",
    "            \n",
    "def _get_param_groups(network):\n",
    "        param_optimizer = [n for n in list(network.named_parameters()) if 'pooler' not in n[0]]\n",
    "        no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    \n",
    "from pytorch_pretrained_bert import BertAdam as Adam\n",
    "\n",
    "def _setup_optim(config, optimizer_parameters, state_dict=None, num_train_step=-1, network=None):\n",
    "    if config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(optimizer_parameters, config['learning_rate'],\n",
    "                                   weight_decay=config['weight_decay'])\n",
    "\n",
    "    elif config['optimizer'] == 'adamax':\n",
    "        optimizer = Adamax(optimizer_parameters,\n",
    "                                config['learning_rate'],\n",
    "                                warmup=config['warmup'],\n",
    "                                t_total=num_train_step,\n",
    "                                max_grad_norm=config['grad_clipping'],\n",
    "                                schedule=config['warmup_schedule'],\n",
    "                                weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "    elif config['optimizer'] == 'radam':\n",
    "        optimizer = RAdam(optimizer_parameters,\n",
    "                                config['learning_rate'],\n",
    "                                warmup=config['warmup'],\n",
    "                                t_total=num_train_step,\n",
    "                                max_grad_norm=config['grad_clipping'],\n",
    "                                schedule=config['warmup_schedule'],\n",
    "                                eps=config['adam_eps'],\n",
    "                                weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "        # The current radam does not support FP16.\n",
    "        config['fp16'] = False\n",
    "    elif config['optimizer'] == 'adam':\n",
    "        optimizer = Adam(optimizer_parameters,\n",
    "                              lr=config['learning_rate'],\n",
    "                              warmup=config['warmup'],\n",
    "                              t_total=num_train_step,\n",
    "                              max_grad_norm=config['grad_clipping'],\n",
    "                              schedule=config['warmup_schedule'],\n",
    "                              weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "    else:\n",
    "        raise RuntimeError('Unsupported optimizer: %s' % opt['optimizer'])\n",
    "\n",
    "    if state_dict and 'optimizer' in state_dict:\n",
    "        optimizer.load_state_dict(state_dict['optimizer'])\n",
    "\n",
    "    if config['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "            global amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(network, optimizer, opt_level=config['fp16_opt_level'])\n",
    "        network = model\n",
    "        optimizer = optimizer\n",
    "\n",
    "    if config.get('have_lr_scheduler', False):\n",
    "        if config.get('scheduler_type', 'rop') == 'rop':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=config['lr_gamma'], patience=3)\n",
    "        elif config.get('scheduler_type', 'rop') == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=config.get('lr_gamma', 0.95))\n",
    "        else:\n",
    "            milestones = [int(step) for step in config.get('multi_step_lr', '10,20,30').split(',')]\n",
    "            scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=config.get('lr_gamma'))\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return optimizer, scheduler\n",
    "\n",
    "def perturbated_loss():\n",
    "    smartPerturbation = SmartPerturbation(epsilon=1e-6,\n",
    "                    multi_gpu_on=False,\n",
    "                    step_size=1e-3,\n",
    "                    noise_var=1e-5,\n",
    "                    norm_p='inf',\n",
    "                    k=1,\n",
    "                    fp16=False,\n",
    "                    encoder_type=EncoderModelType.BERT,\n",
    "                    loss_map=[SymKlCriterion],\n",
    "                    norm_level=0)\n",
    "    \n",
    "    return smartPerturbation\n",
    "\n",
    "def _norm_grad(grad, norm_p, epsilon, eff_grad=None, sentence_level=False):\n",
    "        eff_direction = 0\n",
    "        if norm_p == 'l2':\n",
    "            if sentence_level:\n",
    "                direction = grad / (torch.norm(grad, dim=(-2, -1), keepdim=True) + epsilon)\n",
    "            else:\n",
    "                direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + epsilon)\n",
    "        elif norm_p == 'l1':\n",
    "            direction = grad.sign()\n",
    "        else:\n",
    "            if sentence_level:\n",
    "                direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + epsilon)\n",
    "            else:\n",
    "                direction = grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
    "                eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
    "        return direction, eff_direction\n",
    "    \n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    #print(outputs,outputs == labels)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "from loss import stable_kl\n",
    "\n",
    "def generate_noise(embed, mask, epsilon=1e-5):\n",
    "    noise = embed.data.new(embed.size()).normal_(0, 1) *  epsilon\n",
    "    noise.detach()\n",
    "    noise.requires_grad_()\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_save_ckp(metric, best_metric, model, optimizer, save_path, epoch, metric_name='train_loss'):\n",
    "    if metric < best_metric:\n",
    "        wandb.summary[f\"best_{metric_name}\"] = metric\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, save_path)\n",
    "        return metric\n",
    "    return best_metric\n",
    "    \n",
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  model,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  run_id,\n",
    "                  device,\n",
    "                  save_path=''):\n",
    "    \n",
    "    # preliminaries for training loop\n",
    "    loss_scale=0\n",
    "    output_dir=\"large_models\"\n",
    "    %mkdir large_models\n",
    "    output_train_file = os.path.join(output_dir, \"train_results.txt\")\n",
    "    output_smart_train_file = os.path.join(output_dir, \"smart_train_results.txt\")\n",
    "    loss_writer = open(output_train_file, \"w\",1)\n",
    "    smart_loss_writer = open(output_smart_train_file, \"w\",1)\n",
    "\n",
    "    train_loss_meter = AverageMeter()\n",
    "    adv_loss_meter = AverageMeter()\n",
    "    emb_val_meter = AverageMeter()\n",
    "    eff_perturb_meter = AverageMeter()\n",
    "    val_loss_meter = AverageMeter()\n",
    "    \n",
    "    local_updates = 0\n",
    "    updates =0\n",
    "    best_loss = np.inf\n",
    "    last_es_criterion = np.inf\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_std_tr_loss = 0\n",
    "    overall_wrong_confidence_scores = []\n",
    "    overall_correct_confidence_scores = []\n",
    "    trigger_times = 0\n",
    "    stopping = False\n",
    "    best_metrics = {}\n",
    "\n",
    "    for epoch in trange(int(config['epochs']), desc=\"Epoch\"):\n",
    "                tr_loss = 0\n",
    "                last_tr_loss = 0\n",
    "                std_loss = 0\n",
    "\n",
    "                nb_tr_examples, nb_tr_steps = 0, 0\n",
    "                for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                    batch = tuple(t.to(device) for t in batch)\n",
    "                    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                    logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = task_loss_criterion[0](logits, label_ids, config['weight'], ignore_index=-1)\n",
    "                    \n",
    "                    if config['adv_train']:\n",
    "                        if config['multi_gpu_on']:\n",
    "                            embed = torch.stack([model.module.bert.embeddings(ids, mask) for ids, mask in zip(input_ids, input_mask)]) # recover batch-embeddings\n",
    "                        else:\n",
    "                            embed = torch.stack([model.bert.embeddings(ids, mask) for ids, mask in zip(input_ids, input_mask)]) # recover batch-embeddings\n",
    "                        noise = generate_noise(embed, input_mask, config['adv_noise_var'])\n",
    "                        for step in range(config['adv_k']):\n",
    "                            adv_logits = torch.stack([model(input_ids=ids,\n",
    "                                                               token_type_ids=mask,\n",
    "                                                               attention_mask=segment,\n",
    "                                                               inputs_embeds=in_embed)[-1] for ids, mask, segment, in_embed in zip(input_ids,\n",
    "                                                                                                                                   input_mask,\n",
    "                                                                                                                                   segment_ids,\n",
    "                                                                                                                                   noise+embed)\n",
    "                                                     ]\n",
    "                                                    )\n",
    "                            adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
    "                            delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
    "                            norm = delta_grad.norm()\n",
    "                            if (torch.isnan(norm) or torch.isinf(norm)):\n",
    "                                adv_loss = emb_val = eff_perturbation = 0\n",
    "                                break\n",
    "                            eff_delta_grad = delta_grad * config['adv_step_size']\n",
    "                            delta_grad = noise + delta_grad * config['adv_step_size']\n",
    "                            noise, eff_noise = _norm_grad(delta_grad,\n",
    "                                                          norm_p=config['adv_p_norm'],\n",
    "                                                          epsilon=config['adv_epsilon'],\n",
    "                                                          eff_grad=eff_delta_grad,\n",
    "                                                          sentence_level=config['adv_norm_level'])\n",
    "                            noise = noise.detach()\n",
    "                            noise.requires_grad_()\n",
    "                        adv_logits = torch.stack([model(input_ids=ids,\n",
    "                                               token_type_ids=mask,\n",
    "                                               attention_mask=segment,\n",
    "                                               inputs_embeds=in_embed)[-1] for ids, mask, segment, in_embed in zip(input_ids,\n",
    "                                                                                                                   input_mask,\n",
    "                                                                                                                   segment_ids,\n",
    "                                                                                                                   noise+embed)\n",
    "                                                 ]\n",
    "                                                )\n",
    "                        adv_loss = adv_task_loss_criterion[-1](logits, adv_logits, ignore_index=-1)\n",
    "                        emb_val = embed.detach().abs().mean()\n",
    "                        try:\n",
    "                            eff_perturb = eff_noise.detach().abs().mean()\n",
    "                            eff_perturb_meter.update(eff_perturb.item(), config['batch_size'])\n",
    "                        except AttributeError:\n",
    "                            eff_perturb = 0\n",
    "                            eff_perturb_meter.update(0, config['batch_size'])\n",
    "\n",
    "                        loss += config['adv_alpha']*adv_loss\n",
    "                        adv_loss_meter.update(adv_loss.item(), config['batch_size'])\n",
    "                        emb_val_meter.update(emb_val.item(), config['batch_size'])\n",
    "                    \n",
    "                    else:\n",
    "                        adv_loss = 0\n",
    "                        emb_val = 0\n",
    "                        adv_loss_meter.update(adv_loss, config['batch_size'])\n",
    "                        emb_val_meter.update(emb_val, config['batch_size'])\n",
    "                    train_loss_meter.update(loss.item(), config['batch_size'])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    loss = loss /config.get('grad_accumulation_step', 1)\n",
    "                    if config['fp16']:\n",
    "                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                            scaled_loss.backward()\n",
    "\n",
    "                    local_updates += 1\n",
    "                    if local_updates % config.get('grad_accumulation_step', 1) == 0:\n",
    "                        if config['global_grad_clipping'] > 0:\n",
    "                            if config['fp16']:\n",
    "                                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                                               config['global_grad_clipping'])\n",
    "                            else:\n",
    "                                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                                               config['global_grad_clipping'])\n",
    "                        updates += 1\n",
    "                        # reset number of the grad accumulation\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad() \n",
    "                    \n",
    "    \n",
    "#                     accumulated_tr_loss+=train_loss_meter.val\n",
    "#                     accumulated_std_tr_loss+=train_loss_meter.val-adv_loss_meter.val\n",
    "                    if (updates) % (config['log_per_updates']) == 0 or updates == 1:\n",
    "        # RECHNAGE TO OOOOOOOOOOOOOR\n",
    "                        print('evaluating model......')\n",
    "                        wandb.log({\"avg standard loss\": train_loss_meter.avg - adv_loss_meter.avg})\n",
    "                        wandb.log({\"avg regularized loss\": train_loss_meter.avg})\n",
    "                                                \n",
    "#                         wandb.log({\"moving avg standard batch loss\": accumulated_std_tr_loss/config['log_per_updates']})\n",
    "#                         wandb.log({\"moving avg regularized batch loss\": accumulated_tr_loss/config['log_per_updates']})\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        if config['early_stopping_loss']=='train_loss':\n",
    "                            es_criterion = train_loss_meter.avg\n",
    "                        \n",
    "#                         accumulated_tr_loss = 0\n",
    "#                         accumulated_std_tr_loss = 0\n",
    "                        \n",
    "                        best_loss=check_save_ckp(train_loss_meter.val,\n",
    "                                                 best_loss,\n",
    "                                                 model,\n",
    "                                                 optimizer,\n",
    "                                                 f'model_ckp{run_id}.pt',\n",
    "                                                 epoch,\n",
    "                                                 'train_loss')\n",
    "                        \n",
    "                        _, metrics, _ = evaluation(model,\n",
    "                                             train_dataloader,\n",
    "                                             device,\n",
    "                                             mode='train')\n",
    "                \n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'train {key}': val})\n",
    "                            if val < best_metrics.get(f'train {key}', np.inf):\n",
    "                                wandb.summary[f'best train {key}'] = val\n",
    "                                best_metrics[f'best train {key}'] = val\n",
    "                            \n",
    "                        val_loss_meter, metrics, scores = evaluation(model,\n",
    "                                                                     eval_dataloader,\n",
    "                                                                     device,\n",
    "                                                                     val_loss_meter,\n",
    "                                                                     mode='val')\n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'val {key}': val})\n",
    "                            if val < best_metrics.get(f'val {key}', np.inf):\n",
    "                                wandb.summary[f'best val {key}'] = val\n",
    "                                best_metrics[f'best val {key}'] = val\n",
    "                        \n",
    "                        wandb.log({\"avg eval loss\": val_loss_meter.avg})\n",
    "                        if val_loss_meter.avg < best_metrics.get('best_avg_val_loss', np.inf):\n",
    "                            wandb.summary[f\"best eval loss\"] = val_loss_meter.avg\n",
    "                            best_metrics['best_avg_val_loss'] = val_loss_meter.avg\n",
    "                        #_, metrics, scores = evaluation(model,\n",
    "                                                        #test_dataloader,\n",
    "                                                       # device,\n",
    "                                                        #mode='test')\n",
    "                        #for key, val in metrics.items():\n",
    "                            #wandb.log({f'test {key}': val})\n",
    "                           # if val < best_metrics.get(f'test {key}', np.inf):\n",
    "                               # wandb.summary[f'best test {key}'] = val\n",
    "                             #   best_metrics[f'best test {key}'] = val\n",
    "                            \n",
    "                        if config['early_stopping_loss']!='train_loss':\n",
    "                            es_criterion = val_loss_meter.avg\n",
    "                        \n",
    "                        \n",
    "                        if es_criterion > last_es_criterion:\n",
    "                            trigger_times += 1\n",
    "\n",
    "                            if trigger_times >= config['patience']:\n",
    "                                stopping=True\n",
    "                                break\n",
    "\n",
    "                        else:\n",
    "                            trigger_times = 0\n",
    "\n",
    "                        last_es_criterion = es_criterion\n",
    "                    \n",
    "                        overall_correct_confidence_scores.extend(scores['correct_confidence'])\n",
    "                        overall_wrong_confidence_scores.extend(scores['wrong_confidence'])\n",
    "                        \n",
    "                    \n",
    "                    overall_correct_confidence_scores = [[sigmoid(s)] for s in overall_correct_confidence_scores]\n",
    "                    overall_wrong_confidence_scores = [[sigmoid(s)] for s in overall_wrong_confidence_scores]\n",
    "                    correct_table = wandb.Table(data=overall_correct_confidence_scores, columns=[\"confidence scores\"])\n",
    "                    wrong_table = wandb.Table(data=overall_wrong_confidence_scores, columns=[\"confidence scores\"])\n",
    "                    \n",
    "                    wandb.log({f'correct_pred_scores epoch {epoch}': wandb.plot.histogram(correct_table,\n",
    "                                                                    \"scores\",\n",
    "                                                                    title=\"(Correct) Prediction Score Distribution\")})\n",
    "                    wandb.log({f'wrong_pred_scores epoch {epoch}': wandb.plot.histogram(wrong_table,\n",
    "                                                                    \"scores\",\n",
    "                                                                    title=\"(Wrong) Prediction Score Distribution\")})\n",
    "                    \n",
    "                    overall_wrong_confidence_scores = []\n",
    "                    overall_correct_confidence_scores = []\n",
    "                    \n",
    "                    wandb.log({'batch adv_loss': train_loss_meter.avg})\n",
    "                    wandb.log({'batch std_loss': train_loss_meter.avg - adv_loss_meter.avg})\n",
    "    return model\n",
    "#                     if nb_tr_examples % 512 == 0 or False:\n",
    "            \n",
    "            \n",
    "\n",
    "                        \n",
    "                        \n",
    "#                         loss_log = (train_loss_meter.val - last_tr_loss)*1.0/512\n",
    "#                         print(nb_tr_examples,loss_log)\n",
    "#                         loss_writer.write(\"%d %f \\n\" % (nb_tr_examples,loss_log))\n",
    "#                         smart_loss_writer.write((\"%d %f \\n\" % (nb_tr_examples,train_loss_meter.val)))\n",
    "#                         last_tr_loss = tr_loss\n",
    "\n",
    "#                         # evaluate model\n",
    "#                         model.eval()\n",
    "#                         tr_val_loss = 0\n",
    "#                         eval_loss, eval_accuracy = 0, 0\n",
    "#                         nb_eval_steps, nb_eval_examples = 0, 0\n",
    "#                         total_logits = []\n",
    "#                         total_labels = []\n",
    "#                         for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "#                             input_ids = input_ids.to(device)\n",
    "#                             input_mask = input_mask.to(device)\n",
    "#                             segment_ids = segment_ids.to(device)\n",
    "#                             label_ids = label_ids.to(device)\n",
    "\n",
    "#                             with torch.no_grad():\n",
    "#                                 tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "#                                 logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "#                             logits = logits.detach().cpu().numpy()\n",
    "#                             label_ids = label_ids.to('cpu').numpy()\n",
    "#                             tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "#                             eval_loss += tmp_eval_loss.mean().item()\n",
    "#                             eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#                             nb_eval_examples += input_ids.size(0)\n",
    "#                             nb_eval_steps += 1\n",
    "\n",
    "#                             total_logits.append(logits)\n",
    "#                             total_labels.append(label_ids)\n",
    "\n",
    "#                         total_logits = np.concatenate(total_logits)\n",
    "#                         total_labels = np.concatenate(total_labels)\n",
    "\n",
    "#                         eval_loss = eval_loss / nb_eval_steps\n",
    "#                         eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "#                         wandb.log({'eval_loss': eval_loss,\n",
    "#                                   'eval_accuracy': eval_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "def calc_metrics(predictions, labels):\n",
    "    return {'acc': np.round(100*accuracy_score(labels, predictions)),\n",
    "            'f1': np.round(100*f1_score(labels, predictions, average=\"weighted\"), 2)}\n",
    "     \n",
    "\n",
    "def evaluation(model, dataloader, device, val_loss_meter=None, mode='val'):\n",
    "    assert mode in ['test', 'val', 'dev', 'train']\n",
    "    with torch.no_grad():\n",
    "        [metrics,\n",
    "         predictions,\n",
    "         loss,\n",
    "         batch_size,\n",
    "         scores] = eval_model(model, dataloader, device)\n",
    "    if mode in ['val', 'dev']:\n",
    "        assert val_loss_meter is not None\n",
    "        val_loss_meter.update(loss.item(), batch_size)\n",
    "    return val_loss_meter, metrics, scores\n",
    "                         \n",
    "def eval_model(model, dataloader, device):\n",
    "    overall_predictions = []\n",
    "    overall_labels = []\n",
    "    golds = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    overall_metrics = {}\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss, logits = model(input_ids,\n",
    "                             segment_ids,\n",
    "                             input_mask,\n",
    "                             label_ids,\n",
    "                             return_logits=True)\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        \n",
    "        normalized_confidence = softmax(logits)\n",
    "        prediction_confidence = np.max(normalized_confidence, axis=1)\n",
    "        \n",
    "        correct_label_prediction = prediction_confidence[predictions==label_ids]\n",
    "        wrong_label_prediction = prediction_confidence[predictions!=label_ids]\n",
    "        \n",
    "        prediction_confidence = {'correct_confidence': correct_label_prediction,\n",
    "                                'wrong_confidence': wrong_label_prediction}\n",
    "        overall_predictions.extend(predictions)\n",
    "        overall_labels.extend(label_ids)\n",
    "    \n",
    "    metrics = calc_metrics(overall_predictions, list(overall_labels))\n",
    "        \n",
    "    return metrics, predictions, loss, len(predictions), prediction_confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset\n",
    "# train_dataloader, eval_dataloader = build_dataset(wandb.config)\n",
    "\n",
    "def train():\n",
    "    default_config = {\n",
    "             'multi_gpu_on':False,\n",
    "             'max_seq_length': 128,\n",
    "             'adam_eps': 6, \n",
    "             'adv_epsilon': 1e-6,\n",
    "             'adv_train': 1,\n",
    "             'adv_noise_var': 1e-5,\n",
    "             'adv_norm_level': 0,\n",
    "             'adv_step_size': 1e-3,\n",
    "             'bin_on': False,\n",
    "             'cuda': 1,\n",
    "             'encoder_type': None,\n",
    "             'fp16': True,\n",
    "             'fp16_opt_level': 'O1',\n",
    "             'global_grad_clipping': 1.0,\n",
    "             'grad_accumulation_step': 1,\n",
    "             'grad_clipping': 1,\n",
    "             'local_rank': -1,\n",
    "             'mkd_opt': 0,\n",
    "             'scheduler_type': 'ms',\n",
    "             'task_def_list': None,\n",
    "             'warmup': 0.1,\n",
    "             'warmup_schedule': 'warmup_linear',\n",
    "             'weight_decay': 0,\n",
    "             'weighted_on': False,\n",
    "             'state_dict': None,\n",
    "             'loss': 'LossCriterion.CeCriterion',\n",
    "             'epochs': 5,\n",
    "             'batch_size': 2,\n",
    "             'adv_k': 1,\n",
    "             'learning_rate':5e-5,\n",
    "             'adv_p_norm': 'inf',\n",
    "             'adv_alpha': 1,\n",
    "             'optimizer': 'adam',\n",
    "             'adv_loss': 'LossCriterion.SymKlCriterion',\n",
    "             'weight': 0,\n",
    "             'log_per_updates': 1024,\n",
    "             'have_lr_scheduler': True,\n",
    "             'patience': 4,\n",
    "             'early_stopping_loss': 'train_loss'\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"master-thesis-RACE\", config=default_config)\n",
    "    \n",
    "    cuda = default_config['cuda']\n",
    "    if cuda:\n",
    "        device='cuda'\n",
    "    \n",
    "    config = wandb.config\n",
    "    run_id = wandb.run.id\n",
    "    \n",
    "#     # build_dataset\n",
    "#     train_dataloader, eval_dataloader = build_dataset(config)\n",
    "    \n",
    "    # create model\n",
    "    model = create_model(device)\n",
    "    \n",
    "    # initialize model and losses\n",
    "    [mnetwork,\n",
    "     task_loss_criterion,\n",
    "     adv_task_loss_criterion,\n",
    "     adv_teacher,\n",
    "     optimizer_parameters,\n",
    "     optimizer,\n",
    "     scheduler] = _model_init(config=config, model=model, num_train_step=len(train_dataloader), device=device)\n",
    "    \n",
    "    # training mode ON\n",
    "    mnetwork.train()\n",
    "    \n",
    "    # wandb watch\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # train/eval\n",
    "    training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  run_id,\n",
    "                  device=device)\n",
    "    \n",
    "    # stop wandb\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/wassimboubaker/master-thesis-RACE/runs/1wc3w50h\" target=\"_blank\">eager-totem-3</a></strong> to <a href=\"https://wandb.ai/wassimboubaker/master-thesis-RACE\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "mkdir: cannot create directory ‘large_models’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/6186 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 1/6186 [11:05<1142:57:49, 665.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 2/6186 [11:06<800:48:51, 466.19s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 3/6186 [11:08<561:24:46, 326.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 4/6186 [11:10<393:48:26, 229.33s/it]\u001b[A/opt/conda/lib/python3.6/site-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /codebuild/output/src811146734/src/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  return orig_fn(arg0, *args, **kwargs)\n",
      "\n",
      "Iteration:   0%|          | 5/6186 [11:12<276:37:40, 161.12s/it]\u001b[A\n",
      "Iteration:   0%|          | 6/6186 [11:14<194:34:43, 113.35s/it]\u001b[A\n",
      "Iteration:   0%|          | 7/6186 [11:16<137:09:31, 79.91s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 8/6186 [11:18<96:59:17, 56.52s/it] \u001b[A\n",
      "Iteration:   0%|          | 9/6186 [11:20<68:50:47, 40.12s/it]\u001b[A\n",
      "Iteration:   0%|          | 10/6186 [11:21<49:08:13, 28.64s/it]\u001b[A\n",
      "Iteration:   0%|          | 11/6186 [11:23<35:22:26, 20.62s/it]\u001b[A\n",
      "Iteration:   0%|          | 12/6186 [11:25<25:41:56, 14.98s/it]\u001b[A\n",
      "Iteration:   0%|          | 13/6186 [11:27<18:58:22, 11.06s/it]\u001b[A\n",
      "Iteration:   0%|          | 14/6186 [11:29<14:13:22,  8.30s/it]\u001b[A\n",
      "Iteration:   0%|          | 15/6186 [11:31<10:54:47,  6.37s/it]\u001b[A\n",
      "Iteration:   0%|          | 16/6186 [11:33<8:38:58,  5.05s/it] \u001b[A\n",
      "Iteration:   0%|          | 17/6186 [11:35<7:04:45,  4.13s/it]\u001b[A\n",
      "Iteration:   0%|          | 18/6186 [11:37<5:58:07,  3.48s/it]\u001b[A\n",
      "Iteration:   0%|          | 19/6186 [11:39<5:13:13,  3.05s/it]\u001b[A\n",
      "Iteration:   0%|          | 20/6186 [11:41<4:36:15,  2.69s/it]\u001b[A\n",
      "Iteration:   0%|          | 21/6186 [11:43<4:14:11,  2.47s/it]\u001b[A\n",
      "Iteration:   0%|          | 22/6186 [11:44<3:54:31,  2.28s/it]\u001b[A\n",
      "Iteration:   0%|          | 23/6186 [11:46<3:40:27,  2.15s/it]\u001b[A\n",
      "Iteration:   0%|          | 24/6186 [11:48<3:35:08,  2.09s/it]\u001b[A\n",
      "Iteration:   0%|          | 25/6186 [11:50<3:28:37,  2.03s/it]\u001b[A\n",
      "Iteration:   0%|          | 26/6186 [11:52<3:26:44,  2.01s/it]\u001b[A\n",
      "Iteration:   0%|          | 27/6186 [11:54<3:22:04,  1.97s/it]\u001b[A\n",
      "Iteration:   0%|          | 28/6186 [11:56<3:20:13,  1.95s/it]\u001b[A\n",
      "Iteration:   0%|          | 29/6186 [11:58<3:17:33,  1.93s/it]\u001b[A\n",
      "Iteration:   0%|          | 30/6186 [11:59<3:14:34,  1.90s/it]\u001b[A\n",
      "Iteration:   1%|          | 31/6186 [12:02<3:20:10,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 32/6186 [12:04<3:41:42,  2.16s/it]\u001b[A\n",
      "Iteration:   1%|          | 33/6186 [12:06<3:33:08,  2.08s/it]\u001b[A\n",
      "Iteration:   1%|          | 34/6186 [12:08<3:28:18,  2.03s/it]\u001b[A\n",
      "Iteration:   1%|          | 35/6186 [12:10<3:23:46,  1.99s/it]\u001b[A\n",
      "Iteration:   1%|          | 36/6186 [12:12<3:20:18,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 37/6186 [12:14<3:18:15,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 38/6186 [12:16<3:16:38,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 39/6186 [12:18<3:19:20,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 40/6186 [12:19<3:17:15,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 41/6186 [12:21<3:15:23,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|          | 42/6186 [12:23<3:16:38,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 43/6186 [12:25<3:17:48,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 44/6186 [12:27<3:17:34,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 45/6186 [12:29<3:18:03,  1.94s/it]\u001b[A\n",
      "Iteration:   1%|          | 46/6186 [12:31<3:16:34,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 47/6186 [12:33<3:15:06,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|          | 48/6186 [12:35<3:14:14,  1.90s/it]\u001b[A\n",
      "Iteration:   1%|          | 49/6186 [12:37<3:17:32,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 50/6186 [12:39<3:16:51,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 51/6186 [12:41<3:15:31,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|          | 52/6186 [12:42<3:16:45,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 53/6186 [12:44<3:18:08,  1.94s/it]\u001b[A\n",
      "Iteration:   1%|          | 54/6186 [12:46<3:20:16,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|          | 55/6186 [12:48<3:17:16,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 56/6186 [12:50<3:19:57,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|          | 57/6186 [12:52<3:17:40,  1.94s/it]\u001b[A\n",
      "Iteration:   1%|          | 58/6186 [12:54<3:15:54,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 59/6186 [12:56<3:15:33,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 60/6186 [12:58<3:22:28,  1.98s/it]\u001b[A\n",
      "Iteration:   1%|          | 61/6186 [13:00<3:23:09,  1.99s/it]\u001b[A\n",
      "Iteration:   1%|          | 62/6186 [13:02<3:21:56,  1.98s/it]\u001b[A\n",
      "Iteration:   1%|          | 63/6186 [13:04<3:18:31,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 64/6186 [13:06<3:20:57,  1.97s/it]\u001b[A\n",
      "Iteration:   1%|          | 65/6186 [13:08<3:17:41,  1.94s/it]\u001b[A\n",
      "Iteration:   1%|          | 66/6186 [13:10<3:20:10,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|          | 67/6186 [13:12<3:19:26,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|          | 68/6186 [13:14<3:16:38,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 69/6186 [13:16<3:15:58,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 70/6186 [13:17<3:14:28,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|          | 71/6186 [13:19<3:16:07,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 72/6186 [13:21<3:19:17,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|          | 73/6186 [13:23<3:19:41,  1.96s/it]\u001b[A\n",
      "Iteration:   1%|          | 74/6186 [13:25<3:16:52,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 75/6186 [13:27<3:15:48,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|          | 76/6186 [13:29<3:13:41,  1.90s/it]\u001b[A\n",
      "Iteration:   1%|          | 77/6186 [13:31<3:13:11,  1.90s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 78/6186 [13:33<3:12:43,  1.89s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 79/6186 [13:35<3:11:47,  1.88s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 80/6186 [13:37<3:13:56,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 81/6186 [13:39<3:17:07,  1.94s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 82/6186 [13:41<3:21:38,  1.98s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 83/6186 [13:43<3:18:08,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 84/6186 [13:44<3:16:06,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 85/6186 [13:46<3:13:51,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 86/6186 [13:48<3:15:48,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 87/6186 [13:50<3:18:21,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 88/6186 [13:52<3:14:58,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 89/6186 [13:54<3:17:05,  1.94s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 90/6186 [13:56<3:15:12,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 91/6186 [13:58<3:16:52,  1.94s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 92/6186 [14:00<3:15:45,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 93/6186 [14:02<3:14:08,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 94/6186 [14:04<3:13:27,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 95/6186 [14:06<3:12:46,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 96/6186 [14:07<3:13:20,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 97/6186 [14:10<3:16:45,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 98/6186 [14:11<3:13:31,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 99/6186 [14:13<3:12:23,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 100/6186 [14:15<3:11:59,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 101/6186 [14:17<3:15:27,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 102/6186 [14:19<3:13:16,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 103/6186 [14:21<3:12:22,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 104/6186 [14:23<3:11:36,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 105/6186 [14:25<3:11:09,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 106/6186 [14:27<3:16:07,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 107/6186 [14:29<3:15:41,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 108/6186 [14:30<3:14:02,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 109/6186 [14:32<3:12:14,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 110/6186 [14:34<3:11:38,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 111/6186 [14:36<3:15:14,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 112/6186 [14:38<3:16:28,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 113/6186 [14:40<3:17:34,  1.95s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 114/6186 [14:42<3:18:04,  1.96s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 115/6186 [14:44<3:18:39,  1.96s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 116/6186 [14:46<3:19:11,  1.97s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 117/6186 [14:48<3:16:00,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 118/6186 [14:50<3:13:55,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 119/6186 [14:52<3:16:53,  1.95s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 120/6186 [14:54<3:14:43,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 121/6186 [14:56<3:13:27,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 122/6186 [14:58<3:16:15,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 123/6186 [15:00<3:15:34,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 124/6186 [15:02<3:17:44,  1.96s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 125/6186 [15:03<3:17:57,  1.96s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 126/6186 [15:05<3:14:31,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 127/6186 [15:07<3:13:47,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 128/6186 [15:09<3:12:11,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 129/6186 [15:11<3:11:36,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 130/6186 [15:13<3:11:09,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 131/6186 [15:15<3:10:13,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 132/6186 [15:17<3:09:25,  1.88s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 133/6186 [15:19<3:10:52,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 134/6186 [15:20<3:11:18,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 135/6186 [15:22<3:11:05,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 136/6186 [15:24<3:10:23,  1.89s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 137/6186 [15:26<3:14:47,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 138/6186 [15:28<3:12:46,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 139/6186 [15:30<3:11:50,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 140/6186 [15:32<3:15:00,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 141/6186 [15:34<3:15:39,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 142/6186 [15:36<3:14:14,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 143/6186 [15:38<3:13:30,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 144/6186 [15:40<3:15:14,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 145/6186 [15:42<3:13:17,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 146/6186 [15:43<3:12:03,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 147/6186 [15:45<3:15:08,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 148/6186 [15:47<3:16:09,  1.95s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 149/6186 [15:49<3:13:37,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 150/6186 [15:51<3:15:44,  1.95s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 151/6186 [15:53<3:14:04,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 152/6186 [15:55<3:12:12,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 153/6186 [15:57<3:10:33,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 154/6186 [15:59<3:10:11,  1.89s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 155/6186 [16:01<3:11:01,  1.90s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 156/6186 [16:03<3:13:55,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 157/6186 [16:05<3:12:18,  1.91s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 158/6186 [16:07<3:14:00,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 159/6186 [16:09<3:15:40,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 160/6186 [16:11<3:16:49,  1.96s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 161/6186 [16:12<3:13:36,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 162/6186 [16:14<3:15:59,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 163/6186 [16:16<3:13:02,  1.92s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 164/6186 [16:18<3:14:20,  1.94s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 165/6186 [16:20<3:13:28,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 166/6186 [16:22<3:11:00,  1.90s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 167/6186 [16:24<3:12:50,  1.92s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 168/6186 [16:26<3:13:35,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 169/6186 [16:28<3:14:49,  1.94s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 170/6186 [16:30<3:13:18,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 171/6186 [16:32<3:15:09,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 172/6186 [16:34<3:12:07,  1.92s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 173/6186 [16:35<3:10:39,  1.90s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 174/6186 [16:37<3:09:23,  1.89s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 175/6186 [16:39<3:09:18,  1.89s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 176/6186 [16:41<3:09:06,  1.89s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 177/6186 [16:43<3:11:37,  1.91s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 178/6186 [16:45<3:13:03,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 179/6186 [16:47<3:14:10,  1.94s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 180/6186 [16:49<3:14:48,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 181/6186 [16:51<3:15:16,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 182/6186 [16:53<3:11:36,  1.91s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 183/6186 [16:55<3:13:39,  1.94s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 184/6186 [16:57<3:20:44,  2.01s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 185/6186 [16:59<3:18:57,  1.99s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 186/6186 [17:01<3:14:29,  1.94s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 187/6186 [17:03<3:14:38,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 188/6186 [17:05<3:11:55,  1.92s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 189/6186 [17:07<3:13:35,  1.94s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 190/6186 [17:08<3:14:45,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 191/6186 [17:10<3:15:44,  1.96s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 192/6186 [17:12<3:16:09,  1.96s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 193/6186 [17:14<3:12:12,  1.92s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 194/6186 [17:16<3:09:24,  1.90s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 195/6186 [17:18<3:11:18,  1.92s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 196/6186 [17:20<3:10:32,  1.91s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 197/6186 [17:22<3:17:32,  1.98s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 198/6186 [17:24<3:14:32,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 199/6186 [17:26<3:16:44,  1.97s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 200/6186 [17:28<3:17:22,  1.98s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 201/6186 [17:30<3:15:22,  1.96s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 202/6186 [17:32<3:17:00,  1.98s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 203/6186 [17:34<3:17:50,  1.98s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 204/6186 [17:36<3:17:53,  1.98s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 205/6186 [17:38<3:14:13,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 206/6186 [17:40<3:16:09,  1.97s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 207/6186 [17:42<3:18:26,  1.99s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 208/6186 [17:44<3:18:42,  1.99s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 209/6186 [17:46<3:16:39,  1.97s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 210/6186 [17:48<3:15:09,  1.96s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 211/6186 [17:50<3:12:15,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 212/6186 [17:52<3:14:07,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 213/6186 [17:53<3:10:50,  1.92s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 214/6186 [17:55<3:08:09,  1.89s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 215/6186 [17:57<3:12:21,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 216/6186 [17:59<3:09:02,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 217/6186 [18:01<3:06:59,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 218/6186 [18:03<3:08:43,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 219/6186 [18:05<3:11:18,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 220/6186 [18:07<3:12:08,  1.93s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 221/6186 [18:09<3:10:29,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 222/6186 [18:11<3:09:34,  1.91s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 223/6186 [18:13<3:15:32,  1.97s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 224/6186 [18:15<3:22:00,  2.03s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 225/6186 [18:17<3:20:09,  2.01s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 226/6186 [18:19<3:16:03,  1.97s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 227/6186 [18:21<3:18:32,  2.00s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 228/6186 [18:23<3:14:51,  1.96s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 229/6186 [18:25<3:12:48,  1.94s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 230/6186 [18:26<3:11:26,  1.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   4%|▎         | 231/6186 [18:28<3:07:30,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 232/6186 [18:30<3:09:09,  1.91s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 233/6186 [18:32<3:09:33,  1.91s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 234/6186 [18:34<3:08:17,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 235/6186 [18:36<3:07:59,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 236/6186 [18:38<3:10:43,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 237/6186 [18:40<3:11:40,  1.93s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 238/6186 [18:42<3:10:05,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 239/6186 [18:44<3:11:18,  1.93s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 240/6186 [18:46<3:10:45,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 241/6186 [18:48<3:20:01,  2.02s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 242/6186 [18:50<3:14:09,  1.96s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 243/6186 [18:52<3:14:06,  1.96s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 244/6186 [18:54<3:14:33,  1.96s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 245/6186 [18:55<3:10:36,  1.93s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 246/6186 [18:57<3:07:34,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 247/6186 [18:59<3:06:55,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 248/6186 [19:01<3:06:40,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 249/6186 [19:03<3:06:10,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 250/6186 [19:05<3:05:40,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 251/6186 [19:07<3:05:53,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 252/6186 [19:08<3:06:01,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 253/6186 [19:10<3:06:24,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 254/6186 [19:12<3:06:39,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 255/6186 [19:14<3:08:40,  1.91s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 256/6186 [19:16<3:09:23,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 257/6186 [19:18<3:07:42,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 258/6186 [19:20<3:06:44,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 259/6186 [19:22<3:06:15,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 260/6186 [19:24<3:06:21,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 261/6186 [19:26<3:06:13,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 262/6186 [19:27<3:05:40,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 263/6186 [19:29<3:05:32,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 264/6186 [19:31<3:06:20,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 265/6186 [19:33<3:09:38,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 266/6186 [19:35<3:07:26,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 267/6186 [19:37<3:06:45,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 268/6186 [19:39<3:06:21,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 269/6186 [19:41<3:06:40,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 270/6186 [19:43<3:06:38,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 271/6186 [19:45<3:10:00,  1.93s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 272/6186 [19:47<3:12:23,  1.95s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 273/6186 [19:49<3:14:05,  1.97s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 274/6186 [19:51<3:14:19,  1.97s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 275/6186 [19:53<3:18:16,  2.01s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 276/6186 [19:55<3:13:53,  1.97s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 277/6186 [19:56<3:10:29,  1.93s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 278/6186 [19:58<3:08:39,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 279/6186 [20:00<3:09:24,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 280/6186 [20:02<3:10:20,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 281/6186 [20:04<3:08:53,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 282/6186 [20:06<3:07:33,  1.91s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 283/6186 [20:08<3:06:44,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 284/6186 [20:10<3:06:44,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 285/6186 [20:12<3:06:43,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 286/6186 [20:14<3:06:34,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 287/6186 [20:15<3:05:46,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 288/6186 [20:17<3:05:20,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 289/6186 [20:19<3:08:43,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 290/6186 [20:21<3:07:49,  1.91s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 291/6186 [20:23<3:06:41,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 292/6186 [20:25<3:09:27,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 293/6186 [20:27<3:12:54,  1.96s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 294/6186 [20:29<3:14:09,  1.98s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 295/6186 [20:31<3:14:39,  1.98s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 296/6186 [20:33<3:17:13,  2.01s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 297/6186 [20:35<3:16:50,  2.01s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 298/6186 [20:37<3:16:27,  2.00s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 299/6186 [20:39<3:16:31,  2.00s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 300/6186 [20:41<3:13:00,  1.97s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 301/6186 [20:43<3:10:11,  1.94s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 302/6186 [20:45<3:08:38,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 303/6186 [20:47<3:08:55,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 304/6186 [20:49<3:06:57,  1.91s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 305/6186 [20:50<3:06:39,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 306/6186 [20:52<3:05:17,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 307/6186 [20:54<3:04:41,  1.88s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 308/6186 [20:56<3:08:26,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 309/6186 [20:58<3:10:57,  1.95s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 310/6186 [21:00<3:08:30,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 311/6186 [21:02<3:11:21,  1.95s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 312/6186 [21:04<3:10:00,  1.94s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 313/6186 [21:06<3:11:45,  1.96s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 314/6186 [21:08<3:13:06,  1.97s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 315/6186 [21:10<3:11:12,  1.95s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 316/6186 [21:12<3:08:57,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 317/6186 [21:14<3:07:16,  1.91s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 318/6186 [21:16<3:10:11,  1.94s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 319/6186 [21:18<3:07:56,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 320/6186 [21:20<3:08:15,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 321/6186 [21:21<3:09:26,  1.94s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 322/6186 [21:23<3:09:53,  1.94s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 323/6186 [21:25<3:08:14,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 324/6186 [21:27<3:10:28,  1.95s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 325/6186 [21:29<3:07:58,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 326/6186 [21:31<3:08:49,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 327/6186 [21:33<3:08:58,  1.94s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 328/6186 [21:35<3:06:59,  1.92s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 329/6186 [21:37<3:09:57,  1.95s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 330/6186 [21:39<3:08:34,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 331/6186 [21:41<3:11:02,  1.96s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 332/6186 [21:43<3:17:06,  2.02s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 333/6186 [21:45<3:16:06,  2.01s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 334/6186 [21:47<3:12:20,  1.97s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 335/6186 [21:49<3:13:09,  1.98s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 336/6186 [21:51<3:10:38,  1.96s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 337/6186 [21:53<3:08:21,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 338/6186 [21:55<3:10:55,  1.96s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 339/6186 [21:57<3:08:32,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 340/6186 [21:59<3:09:29,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 341/6186 [22:00<3:07:51,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 342/6186 [22:02<3:09:02,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 343/6186 [22:04<3:07:15,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 344/6186 [22:06<3:05:42,  1.91s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 345/6186 [22:08<3:04:38,  1.90s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 346/6186 [22:10<3:08:22,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 347/6186 [22:12<3:10:34,  1.96s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 348/6186 [22:14<3:08:45,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 349/6186 [22:16<3:09:42,  1.95s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 350/6186 [22:18<3:10:12,  1.96s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 351/6186 [22:20<3:07:39,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 352/6186 [22:22<3:06:10,  1.91s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 353/6186 [22:24<3:08:27,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 354/6186 [22:26<3:09:24,  1.95s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 355/6186 [22:28<3:10:18,  1.96s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 356/6186 [22:30<3:11:01,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 357/6186 [22:32<3:11:01,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 358/6186 [22:33<3:08:25,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 359/6186 [22:35<3:11:06,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 360/6186 [22:37<3:10:49,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 361/6186 [22:39<3:08:05,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 362/6186 [22:41<3:06:32,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 363/6186 [22:43<3:06:10,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 364/6186 [22:45<3:08:17,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 365/6186 [22:47<3:10:05,  1.96s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 366/6186 [22:49<3:10:32,  1.96s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 367/6186 [22:51<3:06:54,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 368/6186 [22:53<3:08:01,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 369/6186 [22:55<3:09:15,  1.95s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 370/6186 [22:57<3:06:48,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 371/6186 [22:59<3:05:00,  1.91s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 372/6186 [23:01<3:31:40,  2.18s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 373/6186 [23:04<3:28:45,  2.15s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 374/6186 [23:06<3:23:34,  2.10s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 375/6186 [23:08<3:21:36,  2.08s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 376/6186 [23:10<3:18:10,  2.05s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 377/6186 [23:11<3:15:36,  2.02s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 378/6186 [23:13<3:11:09,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 379/6186 [23:15<3:08:14,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 380/6186 [23:17<3:06:29,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 381/6186 [23:19<3:05:01,  1.91s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 382/6186 [23:21<3:06:24,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 383/6186 [23:23<3:08:03,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 384/6186 [23:25<3:05:23,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 385/6186 [23:27<3:03:22,  1.90s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 386/6186 [23:29<3:04:28,  1.91s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 387/6186 [23:30<3:04:49,  1.91s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 388/6186 [23:32<3:06:49,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 389/6186 [23:34<3:05:22,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 390/6186 [23:36<3:03:59,  1.90s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 391/6186 [23:38<3:03:19,  1.90s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 392/6186 [23:40<3:03:36,  1.90s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 393/6186 [23:42<3:08:44,  1.95s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 394/6186 [23:44<3:09:42,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 395/6186 [23:46<3:09:42,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 396/6186 [23:48<3:10:57,  1.98s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 397/6186 [23:50<3:11:37,  1.99s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 398/6186 [23:52<3:12:22,  1.99s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 399/6186 [23:54<3:09:10,  1.96s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 400/6186 [23:56<3:07:12,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 401/6186 [23:58<3:06:31,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 402/6186 [24:00<3:04:31,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 403/6186 [24:02<3:05:44,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 404/6186 [24:04<3:06:28,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 405/6186 [24:05<3:04:02,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 406/6186 [24:07<3:02:58,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 407/6186 [24:09<3:03:09,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 408/6186 [24:11<3:02:37,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 409/6186 [24:13<3:11:36,  1.99s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 410/6186 [24:15<3:10:09,  1.98s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 411/6186 [24:17<3:06:22,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 412/6186 [24:19<3:05:00,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 413/6186 [24:21<3:03:37,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 414/6186 [24:23<3:06:08,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 415/6186 [24:25<3:04:59,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 416/6186 [24:27<3:04:52,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 417/6186 [24:29<3:06:27,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 418/6186 [24:31<3:07:31,  1.95s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 419/6186 [24:33<3:07:50,  1.95s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 420/6186 [24:35<3:08:56,  1.97s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 421/6186 [24:36<3:05:51,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 422/6186 [24:38<3:07:39,  1.95s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 423/6186 [24:40<3:05:40,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 424/6186 [24:42<3:05:38,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 425/6186 [24:44<3:03:54,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 426/6186 [24:46<3:05:35,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 427/6186 [24:48<3:06:36,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 428/6186 [24:50<3:05:30,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 429/6186 [24:52<3:03:46,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 430/6186 [24:54<3:03:28,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 431/6186 [24:56<3:02:35,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 432/6186 [24:58<3:02:16,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 433/6186 [24:59<3:04:22,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 434/6186 [25:01<3:05:24,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 435/6186 [25:03<3:03:38,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 436/6186 [25:05<3:02:42,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 437/6186 [25:07<3:05:28,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 438/6186 [25:09<3:04:15,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 439/6186 [25:11<3:03:15,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 440/6186 [25:13<3:01:48,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 441/6186 [25:15<3:01:31,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 442/6186 [25:17<3:00:52,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 443/6186 [25:19<3:07:10,  1.96s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 444/6186 [25:21<3:09:06,  1.98s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 445/6186 [25:23<3:06:05,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 446/6186 [25:25<3:04:32,  1.93s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 447/6186 [25:27<3:07:11,  1.96s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 448/6186 [25:28<3:05:36,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 449/6186 [25:30<3:03:46,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 450/6186 [25:32<3:02:02,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 451/6186 [25:34<3:02:03,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 452/6186 [25:36<3:01:09,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 453/6186 [25:38<3:00:42,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 454/6186 [25:40<3:00:55,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 455/6186 [25:42<3:00:18,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 456/6186 [25:43<3:00:01,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 457/6186 [25:46<3:03:44,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 458/6186 [25:47<3:05:01,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 459/6186 [25:49<3:01:38,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 460/6186 [25:51<3:00:37,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 461/6186 [25:53<3:03:34,  1.92s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 462/6186 [25:55<3:04:54,  1.94s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 463/6186 [25:57<3:07:00,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 464/6186 [25:59<3:15:53,  2.05s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 465/6186 [26:01<3:10:46,  2.00s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 466/6186 [26:03<3:07:11,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 467/6186 [26:05<3:08:16,  1.98s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 468/6186 [26:07<3:05:24,  1.95s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 469/6186 [26:09<3:04:47,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 470/6186 [26:11<3:06:45,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 471/6186 [26:13<3:07:32,  1.97s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 472/6186 [26:15<3:04:23,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 473/6186 [26:17<3:05:36,  1.95s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 474/6186 [26:19<3:06:22,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 475/6186 [26:21<3:04:58,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 476/6186 [26:23<3:02:17,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 477/6186 [26:24<3:02:11,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 478/6186 [26:26<3:03:06,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 479/6186 [26:28<3:01:39,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 480/6186 [26:30<3:04:31,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 481/6186 [26:32<3:06:15,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 482/6186 [26:34<3:05:37,  1.95s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 483/6186 [26:36<3:04:14,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 484/6186 [26:38<3:06:27,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 485/6186 [26:40<3:08:21,  1.98s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 486/6186 [26:42<3:05:33,  1.95s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 487/6186 [26:44<3:04:11,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 488/6186 [26:46<3:05:38,  1.95s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 489/6186 [26:48<3:03:36,  1.93s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 490/6186 [26:50<3:04:30,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 491/6186 [26:52<3:01:51,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 492/6186 [26:54<3:03:33,  1.93s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 493/6186 [26:56<3:01:53,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 494/6186 [26:57<3:00:10,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 495/6186 [26:59<2:59:13,  1.89s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 496/6186 [27:01<2:59:50,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 497/6186 [27:03<3:06:06,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 498/6186 [27:05<3:03:34,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 499/6186 [27:07<3:01:36,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 500/6186 [27:09<3:00:17,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 501/6186 [27:11<3:02:03,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 502/6186 [27:13<3:04:30,  1.95s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 503/6186 [27:15<3:03:11,  1.93s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 504/6186 [27:17<3:07:24,  1.98s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 505/6186 [27:19<3:06:46,  1.97s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 506/6186 [27:21<3:06:45,  1.97s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 507/6186 [27:23<3:07:01,  1.98s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 508/6186 [27:25<3:03:06,  1.93s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 509/6186 [27:27<3:06:38,  1.97s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 510/6186 [27:29<3:06:47,  1.97s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 511/6186 [27:31<3:03:43,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 512/6186 [27:32<3:01:54,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 513/6186 [27:34<3:00:44,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 514/6186 [27:36<2:59:31,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 515/6186 [27:38<2:59:02,  1.89s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 516/6186 [27:40<3:00:16,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 517/6186 [27:42<3:00:47,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 518/6186 [27:44<2:59:20,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 519/6186 [27:46<3:01:24,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 520/6186 [27:48<3:00:03,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 521/6186 [27:49<2:59:16,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 522/6186 [27:51<3:01:41,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 523/6186 [27:53<3:02:46,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 524/6186 [27:55<3:03:32,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 525/6186 [27:57<3:03:55,  1.95s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 526/6186 [27:59<3:05:10,  1.96s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 527/6186 [28:01<3:02:35,  1.94s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 528/6186 [28:03<3:00:42,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 529/6186 [28:05<3:00:02,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 530/6186 [28:07<2:59:22,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 531/6186 [28:09<2:58:46,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 532/6186 [28:11<2:59:03,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 533/6186 [28:13<2:59:17,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 534/6186 [28:15<3:02:25,  1.94s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 535/6186 [28:16<3:00:35,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 536/6186 [28:18<2:58:19,  1.89s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 537/6186 [28:20<3:00:17,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 538/6186 [28:22<2:59:36,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 539/6186 [28:24<3:02:34,  1.94s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 540/6186 [28:26<3:06:17,  1.98s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 541/6186 [28:28<3:06:15,  1.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 542/6186 [28:30<3:06:12,  1.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 543/6186 [28:32<3:02:01,  1.94s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 544/6186 [28:34<3:03:07,  1.95s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 545/6186 [28:36<3:04:13,  1.96s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 546/6186 [28:38<3:03:49,  1.96s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 547/6186 [28:40<3:04:16,  1.96s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 548/6186 [28:42<3:04:19,  1.96s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 549/6186 [28:44<3:00:53,  1.93s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 550/6186 [28:46<3:00:35,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 551/6186 [28:48<3:09:14,  2.01s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 552/6186 [28:50<3:08:07,  2.00s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 553/6186 [28:52<3:03:27,  1.95s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 554/6186 [28:54<3:12:01,  2.05s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 555/6186 [28:56<3:08:11,  2.01s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 556/6186 [28:58<3:05:46,  1.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 557/6186 [29:00<3:05:44,  1.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 558/6186 [29:02<3:05:41,  1.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 559/6186 [29:04<3:05:56,  1.98s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 560/6186 [29:06<3:03:26,  1.96s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 561/6186 [29:08<3:01:09,  1.93s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 562/6186 [29:09<2:59:50,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 563/6186 [29:11<3:02:00,  1.94s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 564/6186 [29:13<3:03:02,  1.95s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 565/6186 [29:15<3:01:03,  1.93s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 566/6186 [29:17<2:59:55,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 567/6186 [29:19<3:01:42,  1.94s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 568/6186 [29:21<3:01:00,  1.93s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 569/6186 [29:23<3:00:06,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 570/6186 [29:25<2:59:43,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 571/6186 [29:27<3:02:44,  1.95s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 572/6186 [29:29<3:03:25,  1.96s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 573/6186 [29:31<3:00:01,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 574/6186 [29:33<3:00:43,  1.93s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 575/6186 [29:35<3:02:07,  1.95s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 576/6186 [29:37<3:02:37,  1.95s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 577/6186 [29:38<2:59:38,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 578/6186 [29:40<2:58:34,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 579/6186 [29:42<2:58:04,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 580/6186 [29:44<2:57:25,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 581/6186 [29:46<2:58:04,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 582/6186 [29:48<2:57:29,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 583/6186 [29:50<2:56:52,  1.89s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 584/6186 [29:52<2:57:16,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 585/6186 [29:54<2:59:32,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 586/6186 [29:56<3:00:49,  1.94s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 587/6186 [29:58<3:01:31,  1.95s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 588/6186 [29:59<2:58:21,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 589/6186 [30:01<2:57:12,  1.90s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 590/6186 [30:03<2:59:54,  1.93s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 591/6186 [30:05<2:58:22,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 592/6186 [30:07<3:00:43,  1.94s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 593/6186 [30:09<3:03:09,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 594/6186 [30:11<3:04:38,  1.98s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 595/6186 [30:13<3:02:01,  1.95s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 596/6186 [30:15<3:03:01,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 597/6186 [30:17<3:03:27,  1.97s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 598/6186 [30:19<3:03:00,  1.97s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 599/6186 [30:21<2:59:35,  1.93s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 600/6186 [30:23<2:58:42,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 601/6186 [30:25<2:57:56,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 602/6186 [30:27<3:00:19,  1.94s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 603/6186 [30:29<2:58:47,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 604/6186 [30:31<3:01:32,  1.95s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 605/6186 [30:32<2:59:35,  1.93s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 606/6186 [30:34<2:59:22,  1.93s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 607/6186 [30:36<2:57:43,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 608/6186 [30:38<2:56:34,  1.90s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 609/6186 [30:40<2:58:41,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 610/6186 [30:42<2:56:19,  1.90s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 611/6186 [30:44<2:54:51,  1.88s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 612/6186 [30:46<2:57:33,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 613/6186 [30:48<2:56:16,  1.90s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 614/6186 [30:50<2:55:42,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 615/6186 [30:51<2:55:45,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 616/6186 [30:53<2:57:38,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 617/6186 [30:55<2:58:43,  1.93s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 618/6186 [30:57<2:57:28,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|█         | 619/6186 [30:59<2:56:49,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|█         | 620/6186 [31:01<2:55:48,  1.90s/it]\u001b[A\n",
      "Iteration:  10%|█         | 621/6186 [31:03<2:55:18,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 622/6186 [31:05<2:58:01,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|█         | 623/6186 [31:07<2:58:44,  1.93s/it]\u001b[A\n",
      "Iteration:  10%|█         | 624/6186 [31:09<3:01:34,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|█         | 625/6186 [31:11<3:00:36,  1.95s/it]\u001b[A\n",
      "Iteration:  10%|█         | 626/6186 [31:13<3:02:11,  1.97s/it]\u001b[A\n",
      "Iteration:  10%|█         | 627/6186 [31:15<3:02:25,  1.97s/it]\u001b[A\n",
      "Iteration:  10%|█         | 628/6186 [31:17<2:59:44,  1.94s/it]\u001b[A\n",
      "Iteration:  10%|█         | 629/6186 [31:19<3:01:22,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|█         | 630/6186 [31:20<2:57:40,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|█         | 631/6186 [31:22<2:56:40,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|█         | 632/6186 [31:24<2:59:08,  1.94s/it]\u001b[A\n",
      "Iteration:  10%|█         | 633/6186 [31:26<3:01:07,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|█         | 634/6186 [31:28<3:01:46,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|█         | 635/6186 [31:30<3:01:55,  1.97s/it]\u001b[A\n",
      "Iteration:  10%|█         | 636/6186 [31:32<3:05:01,  2.00s/it]\u001b[A\n",
      "Iteration:  10%|█         | 637/6186 [31:34<3:00:49,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|█         | 638/6186 [31:36<2:57:52,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|█         | 639/6186 [31:38<2:55:38,  1.90s/it]\u001b[A\n",
      "Iteration:  10%|█         | 640/6186 [31:40<2:54:30,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 641/6186 [31:42<2:56:19,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|█         | 642/6186 [31:44<2:57:39,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|█         | 643/6186 [31:46<2:58:56,  1.94s/it]\u001b[A\n",
      "Iteration:  10%|█         | 644/6186 [31:48<2:56:38,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|█         | 645/6186 [31:49<2:56:10,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|█         | 646/6186 [31:51<2:58:18,  1.93s/it]\u001b[A\n",
      "Iteration:  10%|█         | 647/6186 [31:53<2:59:57,  1.95s/it]\u001b[A\n",
      "Iteration:  10%|█         | 648/6186 [31:55<3:00:53,  1.96s/it]\u001b[A\n",
      "Iteration:  10%|█         | 649/6186 [31:57<2:57:43,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█         | 650/6186 [31:59<2:59:24,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 651/6186 [32:01<2:56:35,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 652/6186 [32:03<2:57:48,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█         | 653/6186 [32:05<2:55:45,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 654/6186 [32:07<2:58:21,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█         | 655/6186 [32:09<2:57:03,  1.92s/it]\u001b[A\n",
      "Iteration:  11%|█         | 656/6186 [32:11<2:58:50,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 657/6186 [32:13<2:55:52,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 658/6186 [32:14<2:53:46,  1.89s/it]\u001b[A\n",
      "Iteration:  11%|█         | 659/6186 [32:16<2:56:09,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 660/6186 [32:18<2:58:29,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 661/6186 [32:20<2:57:35,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█         | 662/6186 [32:22<2:58:44,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 663/6186 [32:24<2:59:52,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 664/6186 [32:26<2:59:40,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 665/6186 [32:28<3:00:44,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 666/6186 [32:30<2:59:09,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 667/6186 [32:32<2:59:46,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 668/6186 [32:34<3:02:53,  1.99s/it]\u001b[A\n",
      "Iteration:  11%|█         | 669/6186 [32:36<2:59:49,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 670/6186 [32:38<3:02:35,  1.99s/it]\u001b[A\n",
      "Iteration:  11%|█         | 671/6186 [32:40<2:58:28,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 672/6186 [32:42<2:59:12,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 673/6186 [32:44<2:58:25,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 674/6186 [32:46<3:00:18,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 675/6186 [32:48<2:58:41,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 676/6186 [32:50<3:00:16,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 677/6186 [32:52<2:58:42,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 678/6186 [32:54<2:57:51,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 679/6186 [32:56<3:00:11,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 680/6186 [32:58<2:59:55,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 681/6186 [32:59<2:57:24,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█         | 682/6186 [33:01<2:55:39,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 683/6186 [33:03<2:55:06,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 684/6186 [33:05<2:58:03,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 685/6186 [33:07<2:58:55,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█         | 686/6186 [33:09<2:59:43,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 687/6186 [33:11<3:08:25,  2.06s/it]\u001b[A\n",
      "Iteration:  11%|█         | 688/6186 [33:13<3:05:25,  2.02s/it]\u001b[A\n",
      "Iteration:  11%|█         | 689/6186 [33:15<3:04:44,  2.02s/it]\u001b[A\n",
      "Iteration:  11%|█         | 690/6186 [33:17<2:59:43,  1.96s/it]\u001b[A\n",
      "Iteration:  11%|█         | 691/6186 [33:19<2:57:05,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█         | 692/6186 [33:21<2:57:51,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 693/6186 [33:23<2:56:38,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█         | 694/6186 [33:25<2:57:51,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█         | 695/6186 [33:27<2:59:56,  1.97s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 696/6186 [33:29<3:00:49,  1.98s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 697/6186 [33:31<3:00:55,  1.98s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 698/6186 [33:33<3:01:04,  1.98s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 699/6186 [33:35<3:00:21,  1.97s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 700/6186 [33:37<3:00:00,  1.97s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 701/6186 [33:39<3:00:01,  1.97s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 702/6186 [33:41<3:00:17,  1.97s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 703/6186 [33:43<3:00:47,  1.98s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 704/6186 [33:45<2:57:29,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 705/6186 [33:46<2:54:52,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 706/6186 [33:48<2:56:20,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 707/6186 [33:50<2:57:52,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 708/6186 [33:52<2:58:28,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 709/6186 [33:54<2:55:19,  1.92s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 710/6186 [33:56<2:57:03,  1.94s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 711/6186 [33:58<2:55:55,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 712/6186 [34:00<2:55:19,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 713/6186 [34:02<2:57:47,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 714/6186 [34:04<2:58:53,  1.96s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 715/6186 [34:06<2:59:10,  1.96s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 716/6186 [34:08<2:58:09,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 717/6186 [34:10<2:58:05,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 718/6186 [34:12<2:55:22,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 719/6186 [34:14<2:56:14,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 720/6186 [34:16<2:54:11,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 721/6186 [34:18<2:55:55,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 722/6186 [34:19<2:54:43,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 723/6186 [34:21<2:54:01,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 724/6186 [34:23<2:53:04,  1.90s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 725/6186 [34:25<2:55:56,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 726/6186 [34:27<2:57:49,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 727/6186 [34:29<2:55:47,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 728/6186 [34:31<2:56:45,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 729/6186 [34:33<2:53:51,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 730/6186 [34:35<2:55:24,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 731/6186 [34:37<2:56:23,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 732/6186 [34:39<2:56:56,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 733/6186 [34:41<2:57:20,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 734/6186 [34:43<2:54:28,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 735/6186 [34:45<2:55:23,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 736/6186 [34:46<2:54:00,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 737/6186 [34:49<2:58:48,  1.97s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 738/6186 [34:50<2:58:10,  1.96s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 739/6186 [34:52<2:58:21,  1.96s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 740/6186 [34:54<2:56:12,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 741/6186 [34:56<2:53:46,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 742/6186 [34:58<2:56:18,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 743/6186 [35:00<2:55:39,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 744/6186 [35:02<2:53:52,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 745/6186 [35:04<2:57:38,  1.96s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 746/6186 [35:06<2:59:09,  1.98s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 747/6186 [35:08<3:09:21,  2.09s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 748/6186 [35:11<3:10:42,  2.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 749/6186 [35:12<3:05:03,  2.04s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 750/6186 [35:14<3:03:29,  2.03s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 751/6186 [35:16<2:59:32,  1.98s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 752/6186 [35:18<3:00:36,  1.99s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 753/6186 [35:20<2:57:56,  1.97s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 754/6186 [35:22<2:59:04,  1.98s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 755/6186 [35:24<2:56:50,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 756/6186 [35:26<2:55:20,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 757/6186 [35:28<2:53:24,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 758/6186 [35:30<2:54:13,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 759/6186 [35:32<2:56:31,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 760/6186 [35:34<2:53:54,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 761/6186 [35:36<2:52:44,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 762/6186 [35:38<2:54:27,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 763/6186 [35:40<2:55:21,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 764/6186 [35:42<2:57:30,  1.96s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 765/6186 [35:44<2:58:58,  1.98s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 766/6186 [35:46<2:58:00,  1.97s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 767/6186 [35:47<2:54:17,  1.93s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 768/6186 [35:49<2:56:52,  1.96s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 769/6186 [35:52<3:09:18,  2.10s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 770/6186 [35:54<3:02:04,  2.02s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 771/6186 [35:56<2:58:54,  1.98s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 772/6186 [35:57<2:55:45,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 773/6186 [35:59<2:57:26,  1.97s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 774/6186 [36:01<2:55:49,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 775/6186 [36:03<2:53:36,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 776/6186 [36:05<2:52:25,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 777/6186 [36:07<2:51:08,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 778/6186 [36:09<2:52:57,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 779/6186 [36:11<2:55:12,  1.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 780/6186 [36:13<2:53:06,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 781/6186 [36:15<2:51:41,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 782/6186 [36:17<2:53:36,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 783/6186 [36:19<2:54:27,  1.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 784/6186 [36:21<2:55:09,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 785/6186 [36:22<2:53:05,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 786/6186 [36:24<2:55:22,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 787/6186 [36:26<2:53:46,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 788/6186 [36:28<2:55:08,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 789/6186 [36:30<2:56:39,  1.96s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 790/6186 [36:32<2:54:02,  1.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 791/6186 [36:34<2:56:00,  1.96s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 792/6186 [36:36<2:53:31,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 793/6186 [36:38<2:56:14,  1.96s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 794/6186 [36:40<2:56:21,  1.96s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 795/6186 [36:42<2:56:26,  1.96s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 796/6186 [36:45<3:11:53,  2.14s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 797/6186 [36:47<3:07:13,  2.08s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 798/6186 [36:49<3:05:28,  2.07s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 799/6186 [36:50<3:02:25,  2.03s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 800/6186 [36:52<3:00:15,  2.01s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 801/6186 [36:54<2:59:13,  2.00s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 802/6186 [36:56<2:56:56,  1.97s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 803/6186 [36:58<2:57:46,  1.98s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 804/6186 [37:00<2:57:46,  1.98s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 805/6186 [37:02<2:57:46,  1.98s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 806/6186 [37:04<2:57:42,  1.98s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 807/6186 [37:06<2:57:02,  1.97s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 808/6186 [37:08<2:53:03,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 809/6186 [37:10<2:52:02,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 810/6186 [37:12<2:51:00,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 811/6186 [37:14<2:53:19,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 812/6186 [37:16<2:54:17,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 813/6186 [37:18<2:52:27,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 814/6186 [37:20<2:51:28,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 815/6186 [37:21<2:50:48,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 816/6186 [37:23<2:53:32,  1.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 817/6186 [37:26<2:55:47,  1.96s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 818/6186 [37:27<2:52:45,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 819/6186 [37:29<2:53:56,  1.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 820/6186 [37:31<2:54:42,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 821/6186 [37:33<2:56:10,  1.97s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 822/6186 [37:35<2:56:00,  1.97s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 823/6186 [37:37<2:56:10,  1.97s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 824/6186 [37:39<2:53:54,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 825/6186 [37:41<2:54:30,  1.95s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 826/6186 [37:43<2:52:42,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 827/6186 [37:45<2:51:51,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 828/6186 [37:47<2:53:18,  1.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 829/6186 [37:49<2:50:57,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 830/6186 [37:51<2:52:41,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 831/6186 [37:53<2:49:59,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 832/6186 [37:54<2:48:07,  1.88s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 833/6186 [37:56<2:48:06,  1.88s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 834/6186 [37:58<2:50:41,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 835/6186 [38:00<2:52:16,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 836/6186 [38:02<2:50:12,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 837/6186 [38:04<2:51:31,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 838/6186 [38:06<2:50:34,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 839/6186 [38:08<2:49:28,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 840/6186 [38:10<2:51:36,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 841/6186 [38:12<2:55:58,  1.98s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 842/6186 [38:14<2:53:15,  1.95s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 843/6186 [38:16<2:52:24,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 844/6186 [38:18<2:53:02,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 845/6186 [38:20<2:54:57,  1.97s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 846/6186 [38:22<2:52:55,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 847/6186 [38:23<2:51:51,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 848/6186 [38:25<2:53:15,  1.95s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 849/6186 [38:27<2:53:42,  1.95s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 850/6186 [38:29<2:54:01,  1.96s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 851/6186 [38:31<2:51:31,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 852/6186 [38:33<2:48:54,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 853/6186 [38:35<2:46:59,  1.88s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 854/6186 [38:37<2:49:30,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 855/6186 [38:39<2:51:27,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 856/6186 [38:41<2:49:09,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 857/6186 [38:43<2:50:35,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 858/6186 [38:45<2:49:27,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 859/6186 [38:46<2:48:44,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 860/6186 [38:48<2:51:59,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 861/6186 [38:50<2:52:47,  1.95s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 862/6186 [38:52<2:53:19,  1.95s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 863/6186 [38:54<2:53:27,  1.96s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 864/6186 [38:56<2:51:17,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 865/6186 [38:58<2:53:08,  1.95s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 866/6186 [39:00<2:50:04,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 867/6186 [39:02<2:51:38,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 868/6186 [39:04<2:48:46,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 869/6186 [39:06<2:48:08,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 870/6186 [39:08<2:47:49,  1.89s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 871/6186 [39:10<2:50:42,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 872/6186 [39:12<2:50:02,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 873/6186 [39:13<2:48:38,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 874/6186 [39:15<2:49:55,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 875/6186 [39:17<2:48:33,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 876/6186 [39:19<2:47:43,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 877/6186 [39:21<2:46:56,  1.89s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 878/6186 [39:23<2:49:51,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 879/6186 [39:25<2:51:01,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 880/6186 [39:27<2:52:57,  1.96s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 881/6186 [39:29<2:49:52,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 882/6186 [39:31<2:47:58,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 883/6186 [39:33<2:48:59,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 884/6186 [39:34<2:47:03,  1.89s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 885/6186 [39:37<2:55:49,  1.99s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 886/6186 [39:39<2:53:13,  1.96s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 887/6186 [39:40<2:50:58,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 888/6186 [39:42<2:51:35,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 889/6186 [39:44<2:50:06,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 890/6186 [39:46<2:51:10,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 891/6186 [39:48<2:53:46,  1.97s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 892/6186 [39:50<2:53:34,  1.97s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 893/6186 [39:52<2:54:24,  1.98s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 894/6186 [39:54<2:50:29,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 895/6186 [39:56<2:51:25,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 896/6186 [39:58<2:49:11,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 897/6186 [40:00<2:48:01,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 898/6186 [40:02<2:47:41,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 899/6186 [40:04<2:49:41,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 900/6186 [40:06<2:48:31,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 901/6186 [40:07<2:48:45,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 902/6186 [40:09<2:48:24,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 903/6186 [40:11<2:49:15,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 904/6186 [40:13<2:48:39,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 905/6186 [40:15<2:49:08,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 906/6186 [40:17<2:49:09,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 907/6186 [40:19<2:49:11,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 908/6186 [40:21<2:50:56,  1.94s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 909/6186 [40:23<2:49:06,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 910/6186 [40:25<2:47:25,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 911/6186 [40:27<2:46:45,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 912/6186 [40:28<2:44:49,  1.88s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 913/6186 [40:30<2:48:15,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 914/6186 [40:32<2:45:53,  1.89s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 915/6186 [40:34<2:44:40,  1.87s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 916/6186 [40:36<2:43:45,  1.86s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 917/6186 [40:38<2:44:34,  1.87s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 918/6186 [40:40<2:44:59,  1.88s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 919/6186 [40:42<2:45:05,  1.88s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 920/6186 [40:43<2:45:07,  1.88s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 921/6186 [40:45<2:47:13,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 922/6186 [40:47<2:49:19,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 923/6186 [40:49<2:47:45,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 924/6186 [40:51<2:46:54,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 925/6186 [40:53<2:46:39,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 926/6186 [40:55<2:46:01,  1.89s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 927/6186 [40:57<2:48:20,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 928/6186 [40:59<2:47:11,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 929/6186 [41:01<2:46:36,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 930/6186 [41:03<2:45:45,  1.89s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 931/6186 [41:05<2:47:41,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 932/6186 [41:06<2:45:43,  1.89s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 933/6186 [41:08<2:50:16,  1.94s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 934/6186 [41:10<2:51:02,  1.95s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 935/6186 [41:12<2:48:49,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 936/6186 [41:14<2:47:27,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 937/6186 [41:16<2:46:19,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 938/6186 [41:18<2:48:33,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 939/6186 [41:20<2:46:26,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 940/6186 [41:22<2:45:46,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 941/6186 [41:24<2:44:48,  1.89s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 942/6186 [41:26<2:48:05,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 943/6186 [41:28<2:47:08,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 944/6186 [41:29<2:45:47,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 945/6186 [41:31<2:49:53,  1.94s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 946/6186 [41:33<2:48:11,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 947/6186 [41:35<2:50:43,  1.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 948/6186 [41:37<2:51:00,  1.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 949/6186 [41:39<2:51:09,  1.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 950/6186 [41:41<2:50:14,  1.95s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 951/6186 [41:43<2:47:55,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 952/6186 [41:45<2:56:29,  2.02s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 953/6186 [41:47<2:55:24,  2.01s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 954/6186 [41:49<2:54:58,  2.01s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 955/6186 [41:51<2:54:22,  2.00s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 956/6186 [41:53<2:51:12,  1.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 957/6186 [41:55<2:48:29,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 958/6186 [41:57<2:49:13,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 959/6186 [41:59<2:49:40,  1.95s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 960/6186 [42:01<2:47:33,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 961/6186 [42:03<2:49:14,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 962/6186 [42:05<2:51:30,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 963/6186 [42:07<2:50:58,  1.96s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 964/6186 [42:09<2:48:41,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 965/6186 [42:11<2:49:14,  1.95s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 966/6186 [42:13<2:49:47,  1.95s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 967/6186 [42:14<2:47:45,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 968/6186 [42:16<2:46:50,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 969/6186 [42:18<2:49:08,  1.95s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 970/6186 [42:20<2:47:50,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 971/6186 [42:22<2:49:02,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 972/6186 [42:24<2:52:37,  1.99s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 973/6186 [42:26<2:53:42,  2.00s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 974/6186 [42:28<2:50:32,  1.96s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 975/6186 [42:30<2:51:43,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 976/6186 [42:32<2:50:43,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 977/6186 [42:34<2:55:05,  2.02s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 978/6186 [42:36<2:51:32,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 979/6186 [42:38<2:52:05,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 980/6186 [42:40<2:51:57,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 981/6186 [42:42<2:48:59,  1.95s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 982/6186 [42:44<2:49:40,  1.96s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 983/6186 [42:46<2:47:57,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 984/6186 [42:48<2:46:21,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 985/6186 [42:50<2:46:21,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 986/6186 [42:52<2:45:45,  1.91s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 987/6186 [42:53<2:45:02,  1.90s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 988/6186 [42:55<2:47:08,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 989/6186 [42:57<2:47:33,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 990/6186 [42:59<2:46:38,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 991/6186 [43:01<2:49:17,  1.96s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 992/6186 [43:03<2:47:49,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 993/6186 [43:05<2:46:12,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 994/6186 [43:07<2:47:09,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 995/6186 [43:09<2:45:26,  1.91s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 996/6186 [43:11<2:46:46,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 997/6186 [43:13<2:44:56,  1.91s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 998/6186 [43:15<2:47:42,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 999/6186 [43:17<2:56:20,  2.04s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 1000/6186 [43:19<3:00:44,  2.09s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 1001/6186 [43:21<2:57:27,  2.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 1002/6186 [43:23<2:56:32,  2.04s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 1003/6186 [43:25<2:55:54,  2.04s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 1004/6186 [43:27<2:55:18,  2.03s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 1005/6186 [43:29<2:54:44,  2.02s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1006/6186 [43:31<2:51:02,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1007/6186 [43:33<2:52:20,  2.00s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1008/6186 [43:35<2:53:12,  2.01s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1009/6186 [43:37<2:52:10,  2.00s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1010/6186 [43:39<2:49:27,  1.96s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1011/6186 [43:41<2:50:57,  1.98s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1012/6186 [43:43<2:47:54,  1.95s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1013/6186 [43:45<2:46:37,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1014/6186 [43:47<2:45:30,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1015/6186 [43:49<2:43:46,  1.90s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1016/6186 [43:51<2:46:26,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1017/6186 [43:53<2:48:15,  1.95s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1018/6186 [43:55<2:50:02,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1019/6186 [43:57<2:49:19,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 1020/6186 [43:59<2:50:28,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1021/6186 [44:01<2:50:31,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1022/6186 [44:03<2:50:35,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1023/6186 [44:05<2:48:36,  1.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  17%|█▋        | 1024/6186 [55:59<309:20:25, 215.74s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1025/6186 [56:01<217:22:07, 151.62s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1026/6186 [56:03<152:58:11, 106.72s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1027/6186 [56:05<107:53:54, 75.29s/it] \u001b[A\n",
      "Iteration:  17%|█▋        | 1028/6186 [56:07<76:21:38, 53.30s/it] \u001b[A\n",
      "Iteration:  17%|█▋        | 1029/6186 [56:09<54:18:36, 37.91s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1030/6186 [56:11<38:56:36, 27.19s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1031/6186 [56:13<28:05:35, 19.62s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1032/6186 [56:15<20:29:45, 14.32s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1033/6186 [56:17<15:12:55, 10.63s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1034/6186 [56:19<11:38:20,  8.13s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1035/6186 [56:21<8:59:53,  6.29s/it] \u001b[A\n",
      "Iteration:  17%|█▋        | 1036/6186 [56:23<7:07:47,  4.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1037/6186 [56:25<5:51:13,  4.09s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1038/6186 [56:27<4:54:04,  3.43s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1039/6186 [56:29<4:13:37,  2.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1040/6186 [56:31<3:45:20,  2.63s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1041/6186 [56:33<3:26:05,  2.40s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1042/6186 [56:35<3:16:47,  2.30s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1043/6186 [56:37<3:07:13,  2.18s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1044/6186 [56:39<3:01:03,  2.11s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1045/6186 [56:41<2:59:21,  2.09s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1046/6186 [56:43<2:57:48,  2.08s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1047/6186 [56:45<2:53:26,  2.02s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1048/6186 [56:47<2:52:39,  2.02s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1049/6186 [56:49<2:52:35,  2.02s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1050/6186 [56:51<2:49:58,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1051/6186 [56:53<2:49:38,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1052/6186 [56:54<2:46:21,  1.94s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1053/6186 [56:56<2:45:26,  1.93s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1054/6186 [56:58<2:47:04,  1.95s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1055/6186 [57:00<2:47:19,  1.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1056/6186 [57:02<2:45:57,  1.94s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1057/6186 [57:04<2:44:56,  1.93s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1058/6186 [57:06<2:46:31,  1.95s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1059/6186 [57:08<2:48:39,  1.97s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1060/6186 [57:10<2:47:05,  1.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1061/6186 [57:12<2:49:23,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1062/6186 [57:14<2:48:03,  1.97s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1063/6186 [57:16<2:50:08,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1064/6186 [57:18<2:48:10,  1.97s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1065/6186 [57:20<2:48:46,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1066/6186 [57:22<2:48:25,  1.97s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1067/6186 [57:24<2:49:00,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1068/6186 [57:26<2:49:24,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1069/6186 [57:28<2:49:17,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1070/6186 [57:30<2:56:46,  2.07s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1071/6186 [57:32<2:51:23,  2.01s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1072/6186 [57:34<2:47:52,  1.97s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1073/6186 [57:36<2:48:51,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1074/6186 [57:38<2:46:20,  1.95s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1075/6186 [57:40<2:48:06,  1.97s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1076/6186 [57:42<2:48:23,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1077/6186 [57:44<2:49:42,  1.99s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1078/6186 [57:46<2:51:15,  2.01s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1079/6186 [57:48<2:48:13,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1080/6186 [57:50<2:49:52,  2.00s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1081/6186 [57:52<2:47:06,  1.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 1082/6186 [57:54<2:47:28,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1083/6186 [57:56<2:49:01,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1084/6186 [57:58<2:46:48,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1085/6186 [58:00<2:48:24,  1.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1086/6186 [58:02<2:49:16,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1087/6186 [58:04<2:47:08,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1088/6186 [58:06<2:47:14,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1089/6186 [58:08<2:47:08,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1090/6186 [58:09<2:45:29,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1091/6186 [58:11<2:44:29,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1092/6186 [58:13<2:45:04,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1093/6186 [58:15<2:44:25,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1094/6186 [58:17<2:46:13,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1095/6186 [58:19<2:46:21,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1096/6186 [58:21<2:47:02,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1097/6186 [58:23<2:45:01,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1098/6186 [58:25<2:45:43,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1099/6186 [58:27<2:46:47,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1100/6186 [58:29<2:48:03,  1.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1101/6186 [58:31<2:49:37,  2.00s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1102/6186 [58:33<2:50:11,  2.01s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1103/6186 [58:36<2:59:05,  2.11s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1104/6186 [58:38<2:56:45,  2.09s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1105/6186 [58:40<2:55:30,  2.07s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1106/6186 [58:42<3:13:37,  2.29s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1107/6186 [58:44<3:03:55,  2.17s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1108/6186 [58:46<2:59:48,  2.12s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1109/6186 [58:48<2:56:39,  2.09s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1110/6186 [58:50<2:52:22,  2.04s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1111/6186 [58:52<2:52:20,  2.04s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1112/6186 [58:54<2:52:15,  2.04s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1113/6186 [58:56<2:53:40,  2.05s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1114/6186 [58:58<2:50:19,  2.01s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1115/6186 [59:00<2:47:44,  1.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1116/6186 [59:02<2:44:56,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1117/6186 [59:04<2:45:20,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1118/6186 [59:06<2:45:42,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1119/6186 [59:08<2:45:59,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1120/6186 [59:10<2:46:24,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1121/6186 [59:12<2:46:54,  1.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1122/6186 [59:14<2:47:03,  1.98s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1123/6186 [59:16<2:48:19,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1124/6186 [59:18<2:47:44,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1125/6186 [59:20<2:48:13,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1126/6186 [59:22<2:47:59,  1.99s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1127/6186 [59:24<2:49:15,  2.01s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1128/6186 [59:26<2:46:20,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1129/6186 [59:28<2:46:19,  1.97s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1130/6186 [59:30<2:44:21,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1131/6186 [59:32<2:45:21,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1132/6186 [59:34<2:43:38,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1133/6186 [59:36<2:42:06,  1.92s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1134/6186 [59:38<2:44:11,  1.95s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1135/6186 [59:39<2:42:50,  1.93s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1136/6186 [59:41<2:43:15,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1137/6186 [59:43<2:42:16,  1.93s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1138/6186 [59:45<2:42:37,  1.93s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1139/6186 [59:47<2:44:42,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1140/6186 [59:49<2:42:49,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1141/6186 [59:51<2:44:27,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1142/6186 [59:53<2:45:09,  1.96s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1143/6186 [59:55<2:42:43,  1.94s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 1144/6186 [59:57<2:44:31,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1145/6186 [59:59<2:42:08,  1.93s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1146/6186 [1:00:01<2:40:06,  1.91s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1147/6186 [1:00:03<2:40:35,  1.91s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1148/6186 [1:00:05<2:44:14,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1149/6186 [1:00:07<2:42:40,  1.94s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1150/6186 [1:00:09<2:44:44,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1151/6186 [1:00:11<2:46:23,  1.98s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1152/6186 [1:00:13<2:45:09,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1153/6186 [1:00:15<2:45:43,  1.98s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1154/6186 [1:00:17<2:45:47,  1.98s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1155/6186 [1:00:19<2:44:38,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1156/6186 [1:00:21<2:47:14,  2.00s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1157/6186 [1:00:23<2:47:35,  2.00s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1158/6186 [1:00:25<2:46:38,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 1159/6186 [1:00:27<2:45:09,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1160/6186 [1:00:28<2:43:39,  1.95s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1161/6186 [1:00:30<2:42:40,  1.94s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1162/6186 [1:00:32<2:45:11,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1163/6186 [1:00:34<2:45:08,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1164/6186 [1:00:36<2:45:15,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1165/6186 [1:00:38<2:43:22,  1.95s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1166/6186 [1:00:40<2:42:38,  1.94s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1167/6186 [1:00:42<2:43:57,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1168/6186 [1:00:44<2:43:01,  1.95s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1169/6186 [1:00:46<2:46:32,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1170/6186 [1:00:48<2:47:24,  2.00s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1171/6186 [1:00:50<2:45:49,  1.98s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1172/6186 [1:00:52<2:46:36,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1173/6186 [1:00:54<2:44:20,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1174/6186 [1:00:56<2:46:17,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1175/6186 [1:00:58<2:55:53,  2.11s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1176/6186 [1:01:00<2:52:18,  2.06s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1177/6186 [1:01:02<2:51:08,  2.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1178/6186 [1:01:05<2:50:45,  2.05s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1179/6186 [1:01:06<2:47:24,  2.01s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1180/6186 [1:01:08<2:44:30,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1181/6186 [1:01:10<2:45:11,  1.98s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1182/6186 [1:01:12<2:46:23,  2.00s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1183/6186 [1:01:14<2:48:59,  2.03s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1184/6186 [1:01:16<2:48:14,  2.02s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1185/6186 [1:01:18<2:48:45,  2.02s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1186/6186 [1:01:20<2:48:01,  2.02s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1187/6186 [1:01:22<2:47:46,  2.01s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1188/6186 [1:01:24<2:47:44,  2.01s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1189/6186 [1:01:26<2:45:34,  1.99s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1190/6186 [1:01:28<2:42:51,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1191/6186 [1:01:30<2:41:36,  1.94s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1192/6186 [1:01:32<2:42:29,  1.95s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1193/6186 [1:01:34<2:43:57,  1.97s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1194/6186 [1:01:36<2:41:13,  1.94s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1195/6186 [1:01:38<2:40:08,  1.93s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1196/6186 [1:01:40<2:42:07,  1.95s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1197/6186 [1:01:42<2:39:53,  1.92s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1198/6186 [1:01:44<2:40:11,  1.93s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1199/6186 [1:01:46<2:41:33,  1.94s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1200/6186 [1:01:48<2:40:10,  1.93s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1201/6186 [1:01:50<2:40:31,  1.93s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1202/6186 [1:01:52<2:42:31,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1203/6186 [1:01:53<2:40:38,  1.93s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1204/6186 [1:01:55<2:39:37,  1.92s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1205/6186 [1:01:57<2:42:09,  1.95s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 1206/6186 [1:01:59<2:41:16,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1207/6186 [1:02:01<2:43:31,  1.97s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1208/6186 [1:02:03<2:40:41,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1209/6186 [1:02:05<2:44:57,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1210/6186 [1:02:07<2:41:42,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1211/6186 [1:02:09<2:40:17,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1212/6186 [1:02:11<2:42:47,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1213/6186 [1:02:13<2:44:18,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1214/6186 [1:02:15<2:44:19,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1215/6186 [1:02:17<2:43:07,  1.97s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1216/6186 [1:02:19<2:44:39,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1217/6186 [1:02:21<2:44:54,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1218/6186 [1:02:23<2:42:49,  1.97s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1219/6186 [1:02:25<2:44:35,  1.99s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1220/6186 [1:02:27<2:46:28,  2.01s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1221/6186 [1:02:29<2:43:34,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1222/6186 [1:02:31<2:41:36,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1223/6186 [1:02:33<2:40:29,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1224/6186 [1:02:35<2:40:09,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1225/6186 [1:02:37<2:39:20,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1226/6186 [1:02:39<2:42:17,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1227/6186 [1:02:41<2:42:05,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1228/6186 [1:02:43<2:40:12,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1229/6186 [1:02:44<2:41:09,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1230/6186 [1:02:46<2:39:57,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1231/6186 [1:02:48<2:41:34,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1232/6186 [1:02:50<2:42:37,  1.97s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1233/6186 [1:02:52<2:40:10,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1234/6186 [1:02:54<2:38:11,  1.92s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1235/6186 [1:02:56<2:40:38,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1236/6186 [1:02:58<2:41:47,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 1237/6186 [1:03:00<2:42:45,  1.97s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1238/6186 [1:03:02<2:39:36,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1239/6186 [1:03:04<2:39:22,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1240/6186 [1:03:06<2:40:47,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1241/6186 [1:03:08<2:38:32,  1.92s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1242/6186 [1:03:10<2:39:21,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1243/6186 [1:03:12<2:39:05,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1244/6186 [1:03:14<2:41:46,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1245/6186 [1:03:16<2:41:19,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1246/6186 [1:03:18<2:40:21,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1247/6186 [1:03:20<2:41:06,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1248/6186 [1:03:22<2:40:55,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1249/6186 [1:03:23<2:41:07,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1250/6186 [1:03:25<2:41:16,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1251/6186 [1:03:27<2:41:50,  1.97s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1252/6186 [1:03:29<2:42:25,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1253/6186 [1:03:31<2:42:44,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1254/6186 [1:03:33<2:42:33,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1255/6186 [1:03:35<2:42:51,  1.98s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1256/6186 [1:03:37<2:39:56,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1257/6186 [1:03:39<2:40:34,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1258/6186 [1:03:41<2:41:11,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1259/6186 [1:03:43<2:40:22,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1260/6186 [1:03:45<2:41:08,  1.96s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1261/6186 [1:03:47<2:39:21,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1262/6186 [1:03:49<2:38:14,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1263/6186 [1:03:51<2:39:55,  1.95s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1264/6186 [1:03:53<2:38:13,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1265/6186 [1:03:55<2:37:03,  1.91s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1266/6186 [1:03:57<2:38:57,  1.94s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1267/6186 [1:03:59<2:38:26,  1.93s/it]\u001b[A\n",
      "Iteration:  20%|██        | 1268/6186 [1:04:01<2:38:33,  1.93s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1269/6186 [1:04:02<2:37:49,  1.93s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1270/6186 [1:04:04<2:37:35,  1.92s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1271/6186 [1:04:06<2:37:17,  1.92s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1272/6186 [1:04:08<2:39:42,  1.95s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1273/6186 [1:04:10<2:43:01,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1274/6186 [1:04:12<2:42:49,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1275/6186 [1:04:14<2:42:50,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1276/6186 [1:04:16<2:39:41,  1.95s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1277/6186 [1:04:18<2:40:46,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1278/6186 [1:04:20<2:40:57,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1279/6186 [1:04:22<2:41:58,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1280/6186 [1:04:24<2:42:03,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1281/6186 [1:04:26<2:42:07,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1282/6186 [1:04:28<2:42:05,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1283/6186 [1:04:30<2:40:39,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1284/6186 [1:04:32<2:40:32,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1285/6186 [1:04:34<2:40:52,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1286/6186 [1:04:36<2:41:24,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1287/6186 [1:04:38<2:38:24,  1.94s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1288/6186 [1:04:40<2:37:07,  1.92s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1289/6186 [1:04:42<2:38:57,  1.95s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1290/6186 [1:04:44<2:38:00,  1.94s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1291/6186 [1:04:46<2:39:57,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1292/6186 [1:04:48<2:40:44,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1293/6186 [1:04:50<2:43:39,  2.01s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1294/6186 [1:04:52<2:39:38,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1295/6186 [1:04:53<2:36:34,  1.92s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1296/6186 [1:04:55<2:38:06,  1.94s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1297/6186 [1:04:57<2:39:49,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1298/6186 [1:04:59<2:40:22,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1299/6186 [1:05:01<2:39:11,  1.95s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1300/6186 [1:05:03<2:41:08,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1301/6186 [1:05:05<2:39:32,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1302/6186 [1:05:07<2:38:51,  1.95s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1303/6186 [1:05:09<2:39:16,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1304/6186 [1:05:11<2:40:42,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1305/6186 [1:05:13<2:41:32,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1306/6186 [1:05:15<2:43:22,  2.01s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1307/6186 [1:05:17<2:42:43,  2.00s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1308/6186 [1:05:19<2:46:00,  2.04s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1309/6186 [1:05:21<2:43:15,  2.01s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1310/6186 [1:05:23<2:40:58,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1311/6186 [1:05:25<2:39:54,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1312/6186 [1:05:27<2:38:59,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1313/6186 [1:05:29<2:38:05,  1.95s/it]\u001b[A\n",
      "Iteration:  21%|██        | 1314/6186 [1:05:31<2:40:23,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1315/6186 [1:05:33<2:38:45,  1.96s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1316/6186 [1:05:35<2:39:38,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1317/6186 [1:05:37<2:39:56,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1318/6186 [1:05:39<2:41:18,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1319/6186 [1:05:41<2:40:55,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1320/6186 [1:05:43<2:41:14,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1321/6186 [1:05:45<2:41:30,  1.99s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1322/6186 [1:05:47<2:40:52,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1323/6186 [1:05:49<2:40:36,  1.98s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1324/6186 [1:05:51<2:39:26,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1325/6186 [1:05:53<2:42:39,  2.01s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1326/6186 [1:05:55<2:39:20,  1.97s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1327/6186 [1:05:57<2:48:15,  2.08s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1328/6186 [1:05:59<2:43:10,  2.02s/it]\u001b[A\n",
      "Iteration:  21%|██▏       | 1329/6186 [1:06:01<2:40:45,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1330/6186 [1:06:03<2:39:06,  1.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1331/6186 [1:06:05<2:38:13,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1332/6186 [1:06:07<2:37:39,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1333/6186 [1:06:09<2:38:27,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1334/6186 [1:06:11<2:38:45,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1335/6186 [1:06:13<2:39:59,  1.98s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1336/6186 [1:06:15<2:40:38,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1337/6186 [1:06:17<2:43:29,  2.02s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1338/6186 [1:06:19<2:40:30,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1339/6186 [1:06:21<2:37:59,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1340/6186 [1:06:23<2:38:39,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1341/6186 [1:06:25<2:39:02,  1.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1342/6186 [1:06:27<2:40:44,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1343/6186 [1:06:28<2:38:37,  1.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1344/6186 [1:06:30<2:36:39,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1345/6186 [1:06:32<2:37:13,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1346/6186 [1:06:34<2:34:40,  1.92s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1347/6186 [1:06:36<2:33:13,  1.90s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1348/6186 [1:06:38<2:32:49,  1.90s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1349/6186 [1:06:40<2:34:59,  1.92s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1350/6186 [1:06:42<2:36:49,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1351/6186 [1:06:44<2:37:47,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1352/6186 [1:06:46<2:36:58,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1353/6186 [1:06:48<2:38:41,  1.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1354/6186 [1:06:50<2:37:34,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1355/6186 [1:06:52<2:35:39,  1.93s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1356/6186 [1:06:54<2:35:07,  1.93s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1357/6186 [1:06:55<2:34:49,  1.92s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1358/6186 [1:06:57<2:36:11,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1359/6186 [1:06:59<2:36:33,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1360/6186 [1:07:01<2:35:48,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1361/6186 [1:07:03<2:37:34,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1362/6186 [1:07:05<2:35:36,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1363/6186 [1:07:07<2:36:44,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1364/6186 [1:07:09<2:37:38,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1365/6186 [1:07:11<2:35:30,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1366/6186 [1:07:13<2:33:55,  1.92s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1367/6186 [1:07:15<2:36:14,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1368/6186 [1:07:17<2:36:51,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1369/6186 [1:07:19<2:38:55,  1.98s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1370/6186 [1:07:21<2:36:16,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1371/6186 [1:07:23<2:35:19,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1372/6186 [1:07:25<2:35:08,  1.93s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1373/6186 [1:07:27<2:35:36,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1374/6186 [1:07:29<2:34:43,  1.93s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1375/6186 [1:07:31<2:36:47,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1376/6186 [1:07:33<2:39:11,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1377/6186 [1:07:35<2:39:49,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1378/6186 [1:07:37<2:40:00,  2.00s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1379/6186 [1:07:39<2:39:29,  1.99s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1380/6186 [1:07:40<2:37:14,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1381/6186 [1:07:42<2:35:37,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1382/6186 [1:07:44<2:35:20,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1383/6186 [1:07:46<2:37:00,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1384/6186 [1:07:48<2:34:20,  1.93s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1385/6186 [1:07:50<2:36:11,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1386/6186 [1:07:52<2:35:14,  1.94s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1387/6186 [1:07:54<2:33:54,  1.92s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1388/6186 [1:07:56<2:36:40,  1.96s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1389/6186 [1:07:58<2:37:21,  1.97s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1390/6186 [1:08:00<2:36:07,  1.95s/it]\u001b[A\n",
      "Iteration:  22%|██▏       | 1391/6186 [1:08:02<2:34:59,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1392/6186 [1:08:04<2:34:42,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1393/6186 [1:08:06<2:35:40,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1394/6186 [1:08:08<2:36:25,  1.96s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1395/6186 [1:08:10<2:35:35,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1396/6186 [1:08:12<2:37:24,  1.97s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1397/6186 [1:08:14<2:34:57,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1398/6186 [1:08:16<2:35:25,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1399/6186 [1:08:17<2:33:11,  1.92s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1400/6186 [1:08:19<2:32:19,  1.91s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1401/6186 [1:08:21<2:34:25,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1402/6186 [1:08:23<2:36:07,  1.96s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1403/6186 [1:08:25<2:37:13,  1.97s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1404/6186 [1:08:27<2:34:58,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1405/6186 [1:08:29<2:34:59,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1406/6186 [1:08:31<2:35:04,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1407/6186 [1:08:33<2:35:33,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1408/6186 [1:08:35<2:33:56,  1.93s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1409/6186 [1:08:37<2:33:30,  1.93s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1410/6186 [1:08:39<2:34:31,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1411/6186 [1:08:41<2:34:51,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1412/6186 [1:08:43<2:36:56,  1.97s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1413/6186 [1:08:45<2:35:20,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1414/6186 [1:08:47<2:33:56,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1415/6186 [1:08:49<2:36:35,  1.97s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1416/6186 [1:08:51<2:35:20,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1417/6186 [1:08:53<2:35:50,  1.96s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1418/6186 [1:08:55<2:37:32,  1.98s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1419/6186 [1:08:56<2:35:37,  1.96s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1420/6186 [1:08:58<2:34:41,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1421/6186 [1:09:00<2:34:09,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1422/6186 [1:09:02<2:34:11,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1423/6186 [1:09:04<2:32:57,  1.93s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1424/6186 [1:09:06<2:32:05,  1.92s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1425/6186 [1:09:08<2:33:39,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1426/6186 [1:09:10<2:35:00,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1427/6186 [1:09:12<2:34:53,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1428/6186 [1:09:14<2:37:11,  1.98s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1429/6186 [1:09:16<2:36:52,  1.98s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1430/6186 [1:09:18<2:35:27,  1.96s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1431/6186 [1:09:20<2:33:42,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1432/6186 [1:09:22<2:33:31,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1433/6186 [1:09:24<2:34:14,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1434/6186 [1:09:26<2:33:37,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1435/6186 [1:09:28<2:32:17,  1.92s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1436/6186 [1:09:29<2:33:36,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1437/6186 [1:09:31<2:32:01,  1.92s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1438/6186 [1:09:33<2:30:47,  1.91s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1439/6186 [1:09:35<2:32:27,  1.93s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1440/6186 [1:09:37<2:33:30,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1441/6186 [1:09:39<2:32:29,  1.93s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1442/6186 [1:09:41<2:33:23,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1443/6186 [1:09:43<2:36:07,  1.98s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1444/6186 [1:09:45<2:36:15,  1.98s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1445/6186 [1:09:47<2:34:06,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1446/6186 [1:09:49<2:32:53,  1.94s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1447/6186 [1:09:51<2:32:02,  1.93s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1448/6186 [1:09:53<2:31:20,  1.92s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1449/6186 [1:09:55<2:30:53,  1.91s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1450/6186 [1:09:57<2:33:33,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1451/6186 [1:09:59<2:35:26,  1.97s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1452/6186 [1:10:01<2:34:05,  1.95s/it]\u001b[A\n",
      "Iteration:  23%|██▎       | 1453/6186 [1:10:03<2:35:17,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1454/6186 [1:10:04<2:33:52,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1455/6186 [1:10:06<2:33:13,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1456/6186 [1:10:08<2:32:08,  1.93s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1457/6186 [1:10:10<2:34:04,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1458/6186 [1:10:12<2:33:25,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1459/6186 [1:10:14<2:32:59,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1460/6186 [1:10:16<2:33:17,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1461/6186 [1:10:18<2:33:17,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1462/6186 [1:10:20<2:35:21,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1463/6186 [1:10:22<2:32:39,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1464/6186 [1:10:24<2:34:46,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1465/6186 [1:10:26<2:34:03,  1.96s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1466/6186 [1:10:28<2:32:27,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1467/6186 [1:10:30<2:31:58,  1.93s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1468/6186 [1:10:32<2:33:09,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▎       | 1469/6186 [1:10:34<2:32:33,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1470/6186 [1:10:36<2:34:50,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1471/6186 [1:10:38<2:34:59,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1472/6186 [1:10:40<2:35:04,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1473/6186 [1:10:42<2:33:08,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1474/6186 [1:10:43<2:32:34,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1475/6186 [1:10:45<2:34:37,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1476/6186 [1:10:48<2:37:01,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1477/6186 [1:10:50<2:37:32,  2.01s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1478/6186 [1:10:52<2:35:38,  1.98s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1479/6186 [1:10:54<2:36:49,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1480/6186 [1:10:56<2:35:48,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1481/6186 [1:10:58<2:37:08,  2.00s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1482/6186 [1:11:00<2:37:35,  2.01s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1483/6186 [1:11:01<2:34:51,  1.98s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1484/6186 [1:11:03<2:33:04,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1485/6186 [1:11:05<2:34:49,  1.98s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1486/6186 [1:11:07<2:36:08,  1.99s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1487/6186 [1:11:09<2:33:59,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1488/6186 [1:11:11<2:32:15,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1489/6186 [1:11:13<2:30:42,  1.93s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1490/6186 [1:11:15<2:30:05,  1.92s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1491/6186 [1:11:17<2:29:38,  1.91s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1492/6186 [1:11:19<2:28:34,  1.90s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1493/6186 [1:11:21<2:33:27,  1.96s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1494/6186 [1:11:23<2:34:04,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1495/6186 [1:11:25<2:34:42,  1.98s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1496/6186 [1:11:27<2:33:36,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1497/6186 [1:11:29<2:34:20,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1498/6186 [1:11:31<2:32:15,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1499/6186 [1:11:33<2:34:09,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1500/6186 [1:11:35<2:34:04,  1.97s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1501/6186 [1:11:37<2:34:41,  1.98s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1502/6186 [1:11:39<2:32:50,  1.96s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1503/6186 [1:11:41<2:32:13,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1504/6186 [1:11:42<2:30:26,  1.93s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1505/6186 [1:11:44<2:32:20,  1.95s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1506/6186 [1:11:46<2:29:46,  1.92s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1507/6186 [1:11:48<2:28:26,  1.90s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1508/6186 [1:11:50<2:29:57,  1.92s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1509/6186 [1:11:52<2:31:03,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1510/6186 [1:11:54<2:30:24,  1.93s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1511/6186 [1:11:56<2:31:12,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1512/6186 [1:11:58<2:34:08,  1.98s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1513/6186 [1:12:00<2:31:14,  1.94s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1514/6186 [1:12:02<2:30:18,  1.93s/it]\u001b[A\n",
      "Iteration:  24%|██▍       | 1515/6186 [1:12:04<2:31:57,  1.95s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1516/6186 [1:12:06<2:32:49,  1.96s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1517/6186 [1:12:08<2:33:19,  1.97s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1518/6186 [1:12:10<2:33:35,  1.97s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1519/6186 [1:12:12<2:33:27,  1.97s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1520/6186 [1:12:14<2:37:58,  2.03s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1521/6186 [1:12:16<2:37:15,  2.02s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1522/6186 [1:12:18<2:33:44,  1.98s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1523/6186 [1:12:20<2:33:02,  1.97s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1524/6186 [1:12:22<2:31:54,  1.96s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1525/6186 [1:12:24<2:33:59,  1.98s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1526/6186 [1:12:26<2:32:30,  1.96s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1527/6186 [1:12:28<2:31:39,  1.95s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1528/6186 [1:12:29<2:30:16,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1529/6186 [1:12:31<2:31:46,  1.96s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1530/6186 [1:12:33<2:30:34,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1531/6186 [1:12:35<2:31:27,  1.95s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1532/6186 [1:12:37<2:32:12,  1.96s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1533/6186 [1:12:39<2:30:03,  1.93s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1534/6186 [1:12:41<2:28:46,  1.92s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1535/6186 [1:12:43<2:28:19,  1.91s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1536/6186 [1:12:45<2:28:06,  1.91s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1537/6186 [1:12:47<2:30:37,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1538/6186 [1:12:49<2:29:32,  1.93s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1539/6186 [1:12:51<2:28:44,  1.92s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1540/6186 [1:12:53<2:30:57,  1.95s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1541/6186 [1:12:55<2:29:53,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1542/6186 [1:12:57<2:30:25,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1543/6186 [1:12:58<2:29:08,  1.93s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1544/6186 [1:13:00<2:31:03,  1.95s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1545/6186 [1:13:02<2:31:19,  1.96s/it]\u001b[A\n",
      "Iteration:  25%|██▍       | 1546/6186 [1:13:04<2:29:24,  1.93s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1547/6186 [1:13:06<2:30:17,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1548/6186 [1:13:08<2:29:40,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1549/6186 [1:13:10<2:29:04,  1.93s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1550/6186 [1:13:12<2:30:26,  1.95s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1551/6186 [1:13:14<2:32:56,  1.98s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1552/6186 [1:13:16<2:33:32,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1553/6186 [1:13:18<2:34:20,  2.00s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1554/6186 [1:13:20<2:34:09,  2.00s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1555/6186 [1:13:22<2:33:56,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1556/6186 [1:13:24<2:33:15,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1557/6186 [1:13:26<2:32:43,  1.98s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1558/6186 [1:13:28<2:29:56,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1559/6186 [1:13:30<2:27:55,  1.92s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1560/6186 [1:13:32<2:29:51,  1.94s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1561/6186 [1:13:34<2:31:18,  1.96s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1562/6186 [1:13:36<2:38:21,  2.05s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1563/6186 [1:13:38<2:33:25,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1564/6186 [1:13:40<2:30:30,  1.95s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1565/6186 [1:13:42<2:32:44,  1.98s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1566/6186 [1:13:44<2:32:22,  1.98s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1567/6186 [1:13:46<2:35:02,  2.01s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1568/6186 [1:13:48<2:35:10,  2.02s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1569/6186 [1:13:50<2:34:57,  2.01s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1570/6186 [1:13:52<2:35:02,  2.02s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1571/6186 [1:13:54<2:34:25,  2.01s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1572/6186 [1:13:56<2:35:09,  2.02s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1573/6186 [1:13:58<2:37:43,  2.05s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1574/6186 [1:14:00<2:35:42,  2.03s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1575/6186 [1:14:02<2:34:25,  2.01s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1576/6186 [1:14:04<2:33:04,  1.99s/it]\u001b[A\n",
      "Iteration:  25%|██▌       | 1577/6186 [1:14:06<2:29:28,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1578/6186 [1:14:08<2:27:54,  1.93s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1579/6186 [1:14:10<2:30:08,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1580/6186 [1:14:12<2:31:01,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1581/6186 [1:14:14<2:31:32,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1582/6186 [1:14:16<2:31:40,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1583/6186 [1:14:18<2:31:54,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1584/6186 [1:14:20<2:32:24,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1585/6186 [1:14:22<2:32:24,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1586/6186 [1:14:24<2:32:44,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1587/6186 [1:14:26<2:31:07,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1588/6186 [1:14:28<2:31:00,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1589/6186 [1:14:30<2:30:32,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1590/6186 [1:14:31<2:29:59,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1591/6186 [1:14:33<2:30:26,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1592/6186 [1:14:35<2:30:56,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1593/6186 [1:14:37<2:31:25,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1594/6186 [1:14:39<2:28:32,  1.94s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1595/6186 [1:14:41<2:28:23,  1.94s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1596/6186 [1:14:43<2:28:45,  1.94s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1597/6186 [1:14:45<2:30:23,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1598/6186 [1:14:47<2:31:45,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1599/6186 [1:14:49<2:31:08,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1600/6186 [1:14:51<2:28:45,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1601/6186 [1:14:53<2:28:31,  1.94s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1602/6186 [1:14:55<2:30:35,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1603/6186 [1:14:57<2:28:44,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1604/6186 [1:14:59<2:27:58,  1.94s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1605/6186 [1:15:01<2:29:45,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1606/6186 [1:15:03<2:28:38,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1607/6186 [1:15:05<2:28:30,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1608/6186 [1:15:07<2:30:31,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1609/6186 [1:15:09<2:30:59,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1610/6186 [1:15:11<2:31:56,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1611/6186 [1:15:13<2:32:27,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1612/6186 [1:15:15<2:32:07,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1613/6186 [1:15:17<2:29:37,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1614/6186 [1:15:19<2:28:16,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1615/6186 [1:15:21<2:28:07,  1.94s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1616/6186 [1:15:22<2:29:05,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1617/6186 [1:15:24<2:26:54,  1.93s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1618/6186 [1:15:26<2:29:43,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1619/6186 [1:15:28<2:28:28,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1620/6186 [1:15:31<2:35:19,  2.04s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1621/6186 [1:15:32<2:32:17,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1622/6186 [1:15:34<2:29:50,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▌       | 1623/6186 [1:15:36<2:28:14,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1624/6186 [1:15:38<2:28:23,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1625/6186 [1:15:40<2:28:11,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1626/6186 [1:15:42<2:30:28,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1627/6186 [1:15:44<2:30:45,  1.98s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1628/6186 [1:15:46<2:28:43,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1629/6186 [1:15:48<2:29:49,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1630/6186 [1:15:50<2:29:05,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1631/6186 [1:15:52<2:28:00,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1632/6186 [1:15:54<2:28:28,  1.96s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1633/6186 [1:15:56<2:26:41,  1.93s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1634/6186 [1:15:58<2:27:44,  1.95s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1635/6186 [1:16:00<2:33:14,  2.02s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1636/6186 [1:16:02<2:31:23,  2.00s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1637/6186 [1:16:04<2:31:00,  1.99s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1638/6186 [1:16:06<2:28:59,  1.97s/it]\u001b[A\n",
      "Iteration:  26%|██▋       | 1639/6186 [1:16:08<2:30:13,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1640/6186 [1:16:10<2:28:39,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1641/6186 [1:16:12<2:30:04,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1642/6186 [1:16:14<2:29:53,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1643/6186 [1:16:16<2:28:49,  1.97s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1644/6186 [1:16:18<2:30:06,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1645/6186 [1:16:20<2:29:46,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1646/6186 [1:16:22<2:29:52,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1647/6186 [1:16:24<2:29:40,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1648/6186 [1:16:26<2:31:41,  2.01s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1649/6186 [1:16:28<2:28:57,  1.97s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1650/6186 [1:16:30<2:27:41,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1651/6186 [1:16:32<2:28:16,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1652/6186 [1:16:33<2:28:45,  1.97s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1653/6186 [1:16:35<2:29:09,  1.97s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1654/6186 [1:16:37<2:26:16,  1.94s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1655/6186 [1:16:39<2:27:08,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1656/6186 [1:16:41<2:25:46,  1.93s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1657/6186 [1:16:43<2:27:09,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1658/6186 [1:16:45<2:25:16,  1.92s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1659/6186 [1:16:47<2:24:48,  1.92s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1660/6186 [1:16:49<2:26:04,  1.94s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1661/6186 [1:16:51<2:25:21,  1.93s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1662/6186 [1:16:53<2:23:55,  1.91s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1663/6186 [1:16:55<2:23:54,  1.91s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1664/6186 [1:16:57<2:25:11,  1.93s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1665/6186 [1:16:58<2:24:07,  1.91s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1666/6186 [1:17:01<2:27:19,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1667/6186 [1:17:03<2:29:30,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1668/6186 [1:17:05<2:30:38,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1669/6186 [1:17:07<2:31:17,  2.01s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1670/6186 [1:17:09<2:30:09,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1671/6186 [1:17:11<2:28:50,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1672/6186 [1:17:12<2:26:44,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1673/6186 [1:17:14<2:25:52,  1.94s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1674/6186 [1:17:16<2:23:30,  1.91s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1675/6186 [1:17:18<2:23:21,  1.91s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1676/6186 [1:17:20<2:23:25,  1.91s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1677/6186 [1:17:22<2:25:34,  1.94s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1678/6186 [1:17:24<2:23:18,  1.91s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1679/6186 [1:17:26<2:25:15,  1.93s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1680/6186 [1:17:28<2:26:51,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1681/6186 [1:17:30<2:28:24,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1682/6186 [1:17:32<2:30:05,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1683/6186 [1:17:34<2:30:04,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1684/6186 [1:17:36<2:30:03,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1685/6186 [1:17:38<2:29:13,  1.99s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1686/6186 [1:17:40<2:29:44,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1687/6186 [1:17:42<2:27:04,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1688/6186 [1:17:44<2:28:15,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1689/6186 [1:17:46<2:26:14,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1690/6186 [1:17:48<2:25:58,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1691/6186 [1:17:50<2:26:29,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1692/6186 [1:17:52<2:28:11,  1.98s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1693/6186 [1:17:54<2:27:01,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1694/6186 [1:17:55<2:26:04,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1695/6186 [1:17:57<2:26:29,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1696/6186 [1:18:00<2:29:36,  2.00s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1697/6186 [1:18:01<2:26:26,  1.96s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1698/6186 [1:18:03<2:25:37,  1.95s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1699/6186 [1:18:05<2:24:28,  1.93s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1700/6186 [1:18:07<2:23:32,  1.92s/it]\u001b[A\n",
      "Iteration:  27%|██▋       | 1701/6186 [1:18:09<2:26:59,  1.97s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1702/6186 [1:18:11<2:30:28,  2.01s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1703/6186 [1:18:13<2:30:47,  2.02s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1704/6186 [1:18:15<2:28:19,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1705/6186 [1:18:17<2:29:42,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1706/6186 [1:18:19<2:32:08,  2.04s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1707/6186 [1:18:21<2:28:00,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1708/6186 [1:18:23<2:26:30,  1.96s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1709/6186 [1:18:25<2:26:57,  1.97s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1710/6186 [1:18:27<2:26:32,  1.96s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1711/6186 [1:18:29<2:27:36,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1712/6186 [1:18:31<2:27:42,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1713/6186 [1:18:33<2:29:04,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1714/6186 [1:18:35<2:29:24,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1715/6186 [1:18:37<2:27:53,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1716/6186 [1:18:39<2:28:41,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1717/6186 [1:18:41<2:28:02,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1718/6186 [1:18:43<2:25:04,  1.95s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1719/6186 [1:18:45<2:23:26,  1.93s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1720/6186 [1:18:47<2:24:51,  1.95s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1721/6186 [1:18:49<2:26:50,  1.97s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1722/6186 [1:18:51<2:31:37,  2.04s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1723/6186 [1:18:53<2:27:33,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1724/6186 [1:18:55<2:25:29,  1.96s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1725/6186 [1:18:57<2:26:39,  1.97s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1726/6186 [1:18:59<2:25:15,  1.95s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1727/6186 [1:19:01<2:27:38,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1728/6186 [1:19:03<2:29:22,  2.01s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1729/6186 [1:19:05<2:29:17,  2.01s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1730/6186 [1:19:07<2:27:07,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1731/6186 [1:19:09<2:28:04,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1732/6186 [1:19:11<2:28:03,  1.99s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1733/6186 [1:19:13<2:25:30,  1.96s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1734/6186 [1:19:15<2:26:41,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1735/6186 [1:19:17<2:26:13,  1.97s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1736/6186 [1:19:19<2:28:13,  2.00s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1737/6186 [1:19:21<2:29:47,  2.02s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1738/6186 [1:19:23<2:28:57,  2.01s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1739/6186 [1:19:25<2:26:31,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1740/6186 [1:19:27<2:29:22,  2.02s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1741/6186 [1:19:29<2:26:44,  1.98s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1742/6186 [1:19:31<2:23:46,  1.94s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1743/6186 [1:19:32<2:21:20,  1.91s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1744/6186 [1:19:34<2:22:41,  1.93s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1745/6186 [1:19:36<2:20:38,  1.90s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1746/6186 [1:19:38<2:22:09,  1.92s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1747/6186 [1:19:40<2:20:34,  1.90s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1748/6186 [1:19:42<2:19:48,  1.89s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1749/6186 [1:19:44<2:20:17,  1.90s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1750/6186 [1:19:46<2:19:55,  1.89s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1751/6186 [1:19:48<2:20:15,  1.90s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1752/6186 [1:19:49<2:20:44,  1.90s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1753/6186 [1:19:51<2:20:00,  1.90s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1754/6186 [1:19:53<2:20:21,  1.90s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1755/6186 [1:19:55<2:21:39,  1.92s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1756/6186 [1:19:57<2:21:43,  1.92s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1757/6186 [1:19:59<2:22:15,  1.93s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1758/6186 [1:20:01<2:21:39,  1.92s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1759/6186 [1:20:03<2:23:09,  1.94s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1760/6186 [1:20:05<2:22:09,  1.93s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1761/6186 [1:20:07<2:21:22,  1.92s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1762/6186 [1:20:09<2:21:55,  1.92s/it]\u001b[A\n",
      "Iteration:  28%|██▊       | 1763/6186 [1:20:11<2:22:13,  1.93s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1764/6186 [1:20:13<2:22:52,  1.94s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1765/6186 [1:20:14<2:20:53,  1.91s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1766/6186 [1:20:17<2:24:24,  1.96s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1767/6186 [1:20:18<2:23:17,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1768/6186 [1:20:21<2:25:29,  1.98s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1769/6186 [1:20:22<2:25:14,  1.97s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1770/6186 [1:20:24<2:26:29,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1771/6186 [1:20:26<2:26:35,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1772/6186 [1:20:28<2:23:55,  1.96s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1773/6186 [1:20:30<2:25:24,  1.98s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1774/6186 [1:20:32<2:26:13,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1775/6186 [1:20:34<2:24:51,  1.97s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1776/6186 [1:20:36<2:23:18,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1777/6186 [1:20:38<2:24:58,  1.97s/it]\u001b[A\n",
      "Iteration:  29%|██▊       | 1778/6186 [1:20:40<2:25:17,  1.98s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1779/6186 [1:20:42<2:22:54,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1780/6186 [1:20:44<2:22:55,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1781/6186 [1:20:46<2:23:44,  1.96s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1782/6186 [1:20:48<2:22:44,  1.94s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1783/6186 [1:20:50<2:21:41,  1.93s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1784/6186 [1:20:52<2:21:33,  1.93s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1785/6186 [1:20:54<2:20:41,  1.92s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1786/6186 [1:20:56<2:22:25,  1.94s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1787/6186 [1:20:58<2:23:15,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1788/6186 [1:21:00<2:21:44,  1.93s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1789/6186 [1:21:02<2:22:44,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1790/6186 [1:21:03<2:21:50,  1.94s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1791/6186 [1:21:05<2:23:10,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1792/6186 [1:21:07<2:24:01,  1.97s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1793/6186 [1:21:09<2:21:36,  1.93s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1794/6186 [1:21:11<2:22:41,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1795/6186 [1:21:13<2:24:52,  1.98s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1796/6186 [1:21:15<2:25:35,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1797/6186 [1:21:17<2:23:13,  1.96s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1798/6186 [1:21:19<2:21:11,  1.93s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1799/6186 [1:21:21<2:22:42,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1800/6186 [1:21:23<2:20:25,  1.92s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1801/6186 [1:21:25<2:21:38,  1.94s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1802/6186 [1:21:27<2:22:32,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1803/6186 [1:21:29<2:24:19,  1.98s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1804/6186 [1:21:31<2:25:38,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1805/6186 [1:21:33<2:26:48,  2.01s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1806/6186 [1:21:35<2:25:30,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1807/6186 [1:21:37<2:23:26,  1.97s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1808/6186 [1:21:39<2:23:52,  1.97s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1809/6186 [1:21:41<2:25:26,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1810/6186 [1:21:43<2:34:01,  2.11s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1811/6186 [1:21:45<2:31:27,  2.08s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1812/6186 [1:21:47<2:29:26,  2.05s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1813/6186 [1:21:49<2:27:32,  2.02s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1814/6186 [1:21:51<2:25:05,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1815/6186 [1:21:53<2:24:30,  1.98s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1816/6186 [1:21:55<2:22:41,  1.96s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1817/6186 [1:21:57<2:23:16,  1.97s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1818/6186 [1:21:59<2:24:38,  1.99s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1819/6186 [1:22:01<2:22:16,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1820/6186 [1:22:03<2:21:40,  1.95s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1821/6186 [1:22:05<2:20:37,  1.93s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1822/6186 [1:22:07<2:19:06,  1.91s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1823/6186 [1:22:09<2:19:10,  1.91s/it]\u001b[A\n",
      "Iteration:  29%|██▉       | 1824/6186 [1:22:11<2:22:21,  1.96s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1825/6186 [1:22:13<2:21:12,  1.94s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1826/6186 [1:22:14<2:20:21,  1.93s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1827/6186 [1:22:16<2:21:33,  1.95s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1828/6186 [1:22:18<2:23:32,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1829/6186 [1:22:20<2:24:36,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1830/6186 [1:22:22<2:22:25,  1.96s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1831/6186 [1:22:24<2:22:24,  1.96s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1832/6186 [1:22:26<2:22:10,  1.96s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1833/6186 [1:22:28<2:22:56,  1.97s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1834/6186 [1:22:30<2:23:11,  1.97s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1835/6186 [1:22:32<2:21:03,  1.95s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1836/6186 [1:22:34<2:19:37,  1.93s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1837/6186 [1:22:36<2:18:42,  1.91s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1838/6186 [1:22:38<2:18:07,  1.91s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1839/6186 [1:22:40<2:20:54,  1.94s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1840/6186 [1:22:42<2:20:08,  1.93s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1841/6186 [1:22:44<2:19:25,  1.93s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1842/6186 [1:22:46<2:22:13,  1.96s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1843/6186 [1:22:48<2:24:04,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1844/6186 [1:22:50<2:23:16,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1845/6186 [1:22:52<2:23:18,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1846/6186 [1:22:54<2:23:26,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1847/6186 [1:22:56<2:22:56,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1848/6186 [1:22:58<2:21:03,  1.95s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1849/6186 [1:22:59<2:20:10,  1.94s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1850/6186 [1:23:01<2:22:08,  1.97s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1851/6186 [1:23:03<2:20:48,  1.95s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1852/6186 [1:23:05<2:19:54,  1.94s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1853/6186 [1:23:07<2:19:05,  1.93s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1854/6186 [1:23:09<2:19:02,  1.93s/it]\u001b[A\n",
      "Iteration:  30%|██▉       | 1855/6186 [1:23:11<2:20:21,  1.94s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1856/6186 [1:23:13<2:22:46,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1857/6186 [1:23:15<2:22:34,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1858/6186 [1:23:17<2:21:52,  1.97s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1859/6186 [1:23:19<2:20:30,  1.95s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1860/6186 [1:23:21<2:19:50,  1.94s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1861/6186 [1:23:23<2:21:51,  1.97s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1862/6186 [1:23:25<2:23:06,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1863/6186 [1:23:27<2:23:53,  2.00s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1864/6186 [1:23:29<2:23:22,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1865/6186 [1:23:31<2:23:39,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1866/6186 [1:23:33<2:24:44,  2.01s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1867/6186 [1:23:35<2:22:08,  1.97s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1868/6186 [1:23:37<2:32:31,  2.12s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1869/6186 [1:23:39<2:28:34,  2.06s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1870/6186 [1:23:41<2:25:05,  2.02s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1871/6186 [1:23:43<2:22:55,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1872/6186 [1:23:45<2:22:38,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1873/6186 [1:23:47<2:22:34,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1874/6186 [1:23:49<2:22:48,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1875/6186 [1:23:51<2:20:40,  1.96s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1876/6186 [1:23:53<2:22:01,  1.98s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1877/6186 [1:23:55<2:19:32,  1.94s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1878/6186 [1:23:57<2:17:34,  1.92s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1879/6186 [1:23:59<2:16:47,  1.91s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1880/6186 [1:24:00<2:16:17,  1.90s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1881/6186 [1:24:02<2:15:08,  1.88s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1882/6186 [1:24:04<2:14:48,  1.88s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1883/6186 [1:24:06<2:16:55,  1.91s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1884/6186 [1:24:08<2:16:23,  1.90s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1885/6186 [1:24:10<2:22:31,  1.99s/it]\u001b[A\n",
      "Iteration:  30%|███       | 1886/6186 [1:24:12<2:22:13,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1887/6186 [1:24:14<2:20:44,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1888/6186 [1:24:16<2:21:34,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1889/6186 [1:24:18<2:19:17,  1.95s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1890/6186 [1:24:20<2:18:22,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1891/6186 [1:24:22<2:21:05,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1892/6186 [1:24:24<2:20:28,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1893/6186 [1:24:26<2:21:09,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1894/6186 [1:24:28<2:18:56,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1895/6186 [1:24:30<2:21:40,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1896/6186 [1:24:32<2:21:13,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1897/6186 [1:24:34<2:20:28,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1898/6186 [1:24:36<2:20:54,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1899/6186 [1:24:38<2:18:47,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1900/6186 [1:24:40<2:19:30,  1.95s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1901/6186 [1:24:41<2:17:33,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1902/6186 [1:24:43<2:17:11,  1.92s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1903/6186 [1:24:45<2:18:17,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1904/6186 [1:24:47<2:18:21,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1905/6186 [1:24:49<2:18:21,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1906/6186 [1:24:51<2:20:52,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1907/6186 [1:24:53<2:22:19,  2.00s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1908/6186 [1:24:55<2:20:11,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1909/6186 [1:24:57<2:21:45,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1910/6186 [1:24:59<2:21:29,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1911/6186 [1:25:01<2:21:49,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1912/6186 [1:25:03<2:21:58,  1.99s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1913/6186 [1:25:05<2:20:11,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1914/6186 [1:25:07<2:20:31,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1915/6186 [1:25:09<2:18:19,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1916/6186 [1:25:11<2:17:29,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1917/6186 [1:25:13<2:18:11,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1918/6186 [1:25:15<2:19:31,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1919/6186 [1:25:17<2:18:41,  1.95s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1920/6186 [1:25:19<2:17:39,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1921/6186 [1:25:21<2:17:59,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1922/6186 [1:25:23<2:18:55,  1.95s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1923/6186 [1:25:25<2:17:50,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1924/6186 [1:25:27<2:18:23,  1.95s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1925/6186 [1:25:28<2:17:23,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1926/6186 [1:25:30<2:17:22,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1927/6186 [1:25:32<2:16:37,  1.92s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1928/6186 [1:25:34<2:16:57,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1929/6186 [1:25:36<2:17:43,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1930/6186 [1:25:38<2:19:01,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1931/6186 [1:25:40<2:18:20,  1.95s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1932/6186 [1:25:42<2:19:31,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███       | 1933/6186 [1:25:44<2:17:40,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1934/6186 [1:25:46<2:17:08,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1935/6186 [1:25:48<2:16:27,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1936/6186 [1:25:50<2:19:10,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1937/6186 [1:25:52<2:17:52,  1.95s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1938/6186 [1:25:54<2:19:55,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1939/6186 [1:25:56<2:18:31,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1940/6186 [1:25:58<2:20:07,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1941/6186 [1:26:00<2:19:57,  1.98s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1942/6186 [1:26:02<2:18:35,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1943/6186 [1:26:04<2:18:45,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1944/6186 [1:26:06<2:19:21,  1.97s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1945/6186 [1:26:08<2:18:11,  1.96s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1946/6186 [1:26:09<2:16:51,  1.94s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1947/6186 [1:26:11<2:16:10,  1.93s/it]\u001b[A\n",
      "Iteration:  31%|███▏      | 1948/6186 [1:26:13<2:16:48,  1.94s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1949/6186 [1:26:15<2:18:07,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1950/6186 [1:26:17<2:16:41,  1.94s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1951/6186 [1:26:19<2:16:15,  1.93s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1952/6186 [1:26:21<2:15:46,  1.92s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1953/6186 [1:26:23<2:17:19,  1.95s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1954/6186 [1:26:25<2:16:08,  1.93s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1955/6186 [1:26:27<2:16:48,  1.94s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1956/6186 [1:26:29<2:18:11,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1957/6186 [1:26:31<2:20:06,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1958/6186 [1:26:33<2:20:23,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1959/6186 [1:26:35<2:20:04,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1960/6186 [1:26:37<2:19:23,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1961/6186 [1:26:39<2:19:44,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1962/6186 [1:26:41<2:19:29,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1963/6186 [1:26:43<2:19:23,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1964/6186 [1:26:45<2:19:59,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1965/6186 [1:26:47<2:17:47,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1966/6186 [1:26:49<2:17:38,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1967/6186 [1:26:51<2:17:57,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1968/6186 [1:26:53<2:16:55,  1.95s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1969/6186 [1:26:54<2:15:48,  1.93s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1970/6186 [1:26:57<2:18:29,  1.97s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1971/6186 [1:26:58<2:18:07,  1.97s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1972/6186 [1:27:00<2:18:23,  1.97s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1973/6186 [1:27:03<2:20:07,  2.00s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1974/6186 [1:27:04<2:18:08,  1.97s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1975/6186 [1:27:06<2:16:22,  1.94s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1976/6186 [1:27:08<2:18:00,  1.97s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1977/6186 [1:27:10<2:17:29,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1978/6186 [1:27:12<2:16:40,  1.95s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1979/6186 [1:27:14<2:18:40,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1980/6186 [1:27:16<2:17:24,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1981/6186 [1:27:18<2:22:11,  2.03s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1982/6186 [1:27:20<2:21:34,  2.02s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1983/6186 [1:27:22<2:21:33,  2.02s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1984/6186 [1:27:24<2:21:30,  2.02s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1985/6186 [1:27:26<2:23:19,  2.05s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1986/6186 [1:27:28<2:21:01,  2.01s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1987/6186 [1:27:30<2:20:58,  2.01s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1988/6186 [1:27:32<2:20:43,  2.01s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1989/6186 [1:27:34<2:19:08,  1.99s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1990/6186 [1:27:36<2:18:42,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1991/6186 [1:27:38<2:16:47,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1992/6186 [1:27:40<2:18:33,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1993/6186 [1:27:42<2:17:07,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1994/6186 [1:27:44<2:18:04,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1995/6186 [1:27:46<2:18:31,  1.98s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1996/6186 [1:27:48<2:16:00,  1.95s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1997/6186 [1:27:50<2:16:45,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1998/6186 [1:27:52<2:17:48,  1.97s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 1999/6186 [1:27:54<2:22:30,  2.04s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2000/6186 [1:27:57<2:28:10,  2.12s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2001/6186 [1:27:59<2:26:36,  2.10s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2002/6186 [1:28:01<2:24:02,  2.07s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2003/6186 [1:28:02<2:19:43,  2.00s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2004/6186 [1:28:04<2:19:47,  2.01s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2005/6186 [1:28:06<2:16:30,  1.96s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2006/6186 [1:28:08<2:13:59,  1.92s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2007/6186 [1:28:10<2:12:36,  1.90s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2008/6186 [1:28:12<2:12:12,  1.90s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2009/6186 [1:28:14<2:12:21,  1.90s/it]\u001b[A\n",
      "Iteration:  32%|███▏      | 2010/6186 [1:28:16<2:12:20,  1.90s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2011/6186 [1:28:18<2:11:49,  1.89s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2012/6186 [1:28:20<2:14:15,  1.93s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2013/6186 [1:28:22<2:13:39,  1.92s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2014/6186 [1:28:24<2:16:13,  1.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2015/6186 [1:28:26<2:16:06,  1.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2016/6186 [1:28:28<2:17:38,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2017/6186 [1:28:30<2:18:15,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2018/6186 [1:28:32<2:17:13,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2019/6186 [1:28:33<2:16:20,  1.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2020/6186 [1:28:35<2:16:06,  1.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2021/6186 [1:28:37<2:15:08,  1.95s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2022/6186 [1:28:39<2:14:04,  1.93s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2023/6186 [1:28:41<2:15:57,  1.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2024/6186 [1:28:43<2:17:30,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2025/6186 [1:28:45<2:17:38,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2026/6186 [1:28:47<2:18:11,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2027/6186 [1:28:49<2:17:42,  1.99s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2028/6186 [1:28:51<2:15:49,  1.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2029/6186 [1:28:53<2:16:58,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2030/6186 [1:28:55<2:17:25,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2031/6186 [1:28:57<2:15:13,  1.95s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2032/6186 [1:28:59<2:16:41,  1.97s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2033/6186 [1:29:01<2:17:16,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2034/6186 [1:29:03<2:13:54,  1.93s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2035/6186 [1:29:05<2:12:23,  1.91s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2036/6186 [1:29:07<2:12:38,  1.92s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2037/6186 [1:29:09<2:14:46,  1.95s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2038/6186 [1:29:11<2:13:53,  1.94s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2039/6186 [1:29:13<2:15:13,  1.96s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2040/6186 [1:29:15<2:13:52,  1.94s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2041/6186 [1:29:16<2:14:53,  1.95s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2042/6186 [1:29:18<2:14:17,  1.94s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2043/6186 [1:29:20<2:13:20,  1.93s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2044/6186 [1:29:22<2:13:40,  1.94s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2045/6186 [1:29:24<2:14:12,  1.94s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2046/6186 [1:29:26<2:16:37,  1.98s/it]\u001b[A\n",
      "Iteration:  33%|███▎      | 2047/6186 [1:29:28<2:15:05,  1.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,1,2 classes (irrelevant - misleading)\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    #print(outputs,outputs == labels)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "# group data by question -> we want to look at the accuracy and f1 score for each question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "# model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distil_model():\n",
    "    \n",
    "    for name, param in distil_model.named_parameters():\n",
    "#         param.requires_grad = False\n",
    "        ln = 24\n",
    "        if name.startswith('distilbert.encoder'):\n",
    "        \tl = name.split('.')\n",
    "        \tln = int(l[3])\n",
    "      \n",
    "        if name.startswith('distilbert.embeddings') or ln < 6:\n",
    "#         \tprint(name)  \n",
    "        \tparam.requires_grad = False\n",
    "    \n",
    "    distil_model.to(device)\n",
    "    \n",
    "    return distil_model\n",
    "\n",
    "# from utils.tokenization import BertTokenizer tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modeling import BertEmbeddings\n",
    "\n",
    "class RaceDistilBert(DistilBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(RaceDistilBert, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = self.transformer\n",
    "    \n",
    "    def forward(self, input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=None,\n",
    "                output_all_encoded_layers=True,\n",
    "                inputs_embeds=None):\n",
    "        '''overwrite forward method'''\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        P_att = torch.zeros_like(input_ids)\n",
    "        Q_att = torch.zeros_like(input_ids)\n",
    "        A_att = torch.zeros_like(input_ids)\n",
    "        token_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        P_att[token_type_ids==0] = 1\n",
    "        Q_att[token_type_ids==1] = 1\n",
    "        A_att[token_type_ids==2] = 1\n",
    "\n",
    "        token_ids[token_type_ids > 0] = 1\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        P_att = P_att.unsqueeze(1).unsqueeze(2)\n",
    "        P_att = P_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        P_att = (1.0 - P_att) * -10000.0\n",
    "\n",
    "        Q_att = Q_att.unsqueeze(1).unsqueeze(2)\n",
    "        Q_att = Q_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        Q_att = (1.0 - Q_att) * -10000.0\n",
    "\n",
    "        A_att = A_att.unsqueeze(1).unsqueeze(2)\n",
    "        A_att = A_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        A_att = (1.0 - A_att) * -10000.0\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            embedding_output = self.embeddings(input_ids, token_ids)\n",
    "        else:\n",
    "            embedding_output = inputs_embeds\n",
    "#             print(f'shape inputs_embeds: {len(inputs_embeds)}, and shape embeddings: {len(self.embeddings(input_ids, token_ids))}')\n",
    "\n",
    "#         self.noise = self.generate_noise(embedding_output, attention_mask[0], epsilon=1e-5)\n",
    "        \n",
    "        print(f'embedding output: {embedding_output}')\n",
    "        return embedding_output, extended_attention_mask,output_all_encoded_layers\n",
    "        try:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers=output_all_encoded_layers)\n",
    "        except TypeError as e:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_hidden_states=output_all_encoded_layers)\n",
    "        \n",
    "#         self.adv_encoded_layers = self.encoder(embedding_output+self.noise,\n",
    "#                                       extended_attention_mask,\n",
    "#                                       output_all_encoded_layers=output_all_encoded_layers)\n",
    "        \n",
    "        \n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "\n",
    "        return extended_attention_mask,P_att,Q_att,A_att,sequence_output\n",
    "    \n",
    "class RaceDistilBertMultipleChoice(DistilBertForMultipleChoice):\n",
    "    def __init__(self, config):\n",
    "        super(RaceDistilBertMultipleChoice, self).__init__(config)\n",
    "        self.distilbert = dmodel\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = RaceDistilBert.from_pretrained('distilbert-base-uncased')\n",
    "dmodel_mc = RaceDistilBertMultipleChoice.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel_mc.distilbert.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMultipleChoice, BertModel\n",
    "from utils.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "distil_model = RaceDistilBertMultipleChoice.from_pretrained('distilbert-base-uncased',\n",
    "                                              cache_dir=os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtrain():\n",
    "    default_config = {\n",
    "             'multi_gpu_on':False,\n",
    "             'max_seq_length': 64,\n",
    "             'adam_eps': 6, \n",
    "             'adv_epsilon': 1e-6,\n",
    "             'adv_train': 1,\n",
    "             'adv_noise_var': 1e-5,\n",
    "             'adv_norm_level': 0,\n",
    "             'adv_step_size': 1e-3,\n",
    "             'bin_on': False,\n",
    "             'cuda': 1,\n",
    "             'encoder_type': None,\n",
    "             'fp16': True,\n",
    "             'fp16_opt_level': 'O1',\n",
    "             'global_grad_clipping': 1.0,\n",
    "             'grad_accumulation_step': 1,\n",
    "             'grad_clipping': 0,\n",
    "             'local_rank': -1,\n",
    "             'mkd_opt': 0,\n",
    "             'scheduler_type': 'ms',\n",
    "             'task_def_list': None,\n",
    "             'warmup': 0.1,\n",
    "             'warmup_schedule': 'warmup_linear',\n",
    "             'weight_decay': 0,\n",
    "             'weighted_on': False,\n",
    "             'state_dict': None,\n",
    "             'loss': 'LossCriterion.CeCriterion',\n",
    "             'epochs': 3,\n",
    "             'batch_size': 2,\n",
    "             'adv_k': 1,\n",
    "             'learning_rate':5e-5,\n",
    "             'adv_p_norm': 'inf',\n",
    "             'adv_alpha': 1,\n",
    "             'optimizer': 'radam',\n",
    "             'adv_loss': 'LossCriterion.SymKlCriterion',\n",
    "             'weight': 0\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"master-thesis\", config=default_config)\n",
    "    \n",
    "    device = default_config['cuda']\n",
    "    \n",
    "    config = wandb.config\n",
    "    \n",
    "    # build_dataset\n",
    "    train_dataloader, eval_dataloader = build_dataset(config)\n",
    "    \n",
    "    # create model\n",
    "    model = create_distil_model()\n",
    "    \n",
    "    # initialize model and losses\n",
    "    [mnetwork,\n",
    "     task_loss_criterion,\n",
    "     adv_task_loss_criterion,\n",
    "     adv_teacher,\n",
    "     optimizer_parameters,\n",
    "     optimizer,\n",
    "     scheduler] = _model_init(config=config, model=model, num_train_step=len(train_dataloader))\n",
    "    \n",
    "    return [config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler]\n",
    "\n",
    "    # training mode ON\n",
    "    model.train()\n",
    "    \n",
    "    # wandb watch\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # train/eval\n",
    "    training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dtrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=training_loop(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = params[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.distilbert.embeddings(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.distilbert.embeddings(input_ids[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_untrained = DistilBertForMultipleChoice(net.config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_untrained.distilbert.embeddings(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(net.distilbert.embeddings.forward), help(network.distilbert.embeddings.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(distil_model.to(device).distilbert.embeddings(input_ids[0]).cpu().detach().numpy() - net_untrained.distilbert.embeddings(input_ids[0]).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.distilbert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'large_models'\n",
    "\n",
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_train_loss, test_train_accuracy = 0, 0\n",
    "nb_test_train_steps, nb_test_train_examples = 0, 0\n",
    "test_train_total_logits = []\n",
    "test_train_total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(train_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            test_train_total_logits.append(logits)\n",
    "            test_train_total_labels.append(label_ids)\n",
    "\t\n",
    "test_train_total_logits = np.concatenate(total_logits)\n",
    "test_train_total_labels = np.concatenate(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "# np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"train_acc_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_examples = read_race(\"./RACE/dev\")\n",
    "# eval_features = convert_examples_to_features(\n",
    "#             eval_examples, tokenizer, args.max_seq_length, True)\n",
    "#         logger.info(\"***** Running evaluation *****\")\n",
    "#         logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "#         logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "#         all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n",
    "#         all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n",
    "#         all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n",
    "\n",
    "#         all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
    "#         eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
    "        # Run prediction for full data\n",
    "\n",
    "if True:\n",
    "    dev_examples = read_data(data_location,'dev')\n",
    "    test_examples = read_data(data_location,'test')\n",
    "\n",
    "if True:\n",
    "    test_features = convert_examples_to_features(\n",
    "                test_examples, tokenizer, max_seq_length, True)\n",
    "    dev_features = convert_examples_to_features(\n",
    "            dev_examples, tokenizer, max_seq_length, True)\n",
    "                \n",
    "if True:\n",
    "    test_data = build_tensor(test_features)\n",
    "    dev_data = build_tensor(dev_features)\n",
    "eval_sampler = SequentialSampler(dev_data)\n",
    "eval_dataloader = DataLoader(dev_data, sampler=eval_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "total_logits = np.concatenate(total_logits)\n",
    "total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/logits.npy\",total_logits)\n",
    "np.save(output_dir+\"/labels.npy\",total_labels)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy}\n",
    "\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_total_logits = [items for labels in total_logits for items in labels]\n",
    "assembled_total_logits = np.array(assembled_total_logits)\n",
    "\n",
    "assembled_total_labels = [items for labels in total_labels for items in labels]\n",
    "assembled_total_labels = np.array(assembled_total_labels)\n",
    "assembled_total_labels, assembled_total_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report as cls_report\n",
    "print(cls_report(test_train_total_labels, np.argmax(test_train_total_logits, axis=1)))\n",
    "print(cls_report(assembled_total_labels, np.argmax(assembled_total_logits, axis=1)))\n",
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('identification fails'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "chosen_features = convert_examples_to_features(chosen_examples, tokenizer, max_seq_length, is_training=False, debug=False)\n",
    "\n",
    "chosen_data = build_tensor(chosen_features)\n",
    "\n",
    "chosen_sampler = SequentialSampler(chosen_data)\n",
    "chosen_dataloader = DataLoader(chosen_data, sampler=chosen_sampler, batch_size=32)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(chosen_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "# output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "# with open(output_test_file, \"w\") as writer:\n",
    "# #             logger.info(\"***** Test results *****\")\n",
    "#             for key in sorted(result.keys()):\n",
    "# #                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                 writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_total_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(train_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('i cannot find my ope'):\n",
    "        chosen_examples.append(example)\n",
    "        print(example.endings[example.label])\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('i cannot find my ope'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = convert_examples_to_features(chosen_examples, tokenizer, max_seq_length, is_training=False, debug=False)\n",
    "\n",
    "chosen_data = build_tensor(chosen_features)\n",
    "\n",
    "chosen_sampler = SequentialSampler(chosen_data)\n",
    "chosen_dataloader = DataLoader(chosen_data, sampler=chosen_sampler, batch_size=32)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(chosen_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "# output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "# with open(output_test_file, \"w\") as writer:\n",
    "# #             logger.info(\"***** Test results *****\")\n",
    "#             for key in sorted(result.keys()):\n",
    "# #                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                 writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_total_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('The {FASTA}'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [example.context_sentence for example in chosen_examples]\n",
    "distribution = [example.label for example in chosen_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(contexts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
