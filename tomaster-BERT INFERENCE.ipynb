{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Training Script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# %pip install bert_race/requirements.txt -qqq\n",
    "%pip install transformers==4.11.3 -qqq\n",
    "!pip install pytorch_pretrained_bert==0.4.0 -qqq\n",
    "# %conda install -c conda-forge ipywidgets\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %pip install transformers --upgrade\n",
    "\n",
    "!pip install wandb -qqq\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformers \n",
    "print(transformers.__version__)\n",
    "\n",
    "from loss import LossCriterion, LOSS_REGISTRY\n",
    "\n",
    "from module.bert_optim import RAdam\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "# del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pickle\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from utils.tokenization_utils import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'support-bert-data'\n",
    "data_location = f's3://{bucket}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r bert_race/requirements.txt\n",
    "# %cp bert_race/pytorch_pretrained_bert/tokenization.py utils/tokenization.py\n",
    "# %cp bert_race/pytorch_pretrained_bert/file_utils.py utils/file_utils.py\n",
    "\n",
    "from utils.tokenization import BertTokenizer \n",
    "from utils.tokenization_utils import convert_examples_to_features\n",
    "from data_utils.utils import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "do_lower_case = True\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# %cp bert_race/pytorch_pretrained_bert/modeling.py utils/modeling.py\n",
    "# %cp bert_race/pytorch_pretrained_bert/optimization.py utils/optimization.py\n",
    "\n",
    "import torch\n",
    "from utils.modeling import BertForMultipleChoice, BertConfig, BertEmbeddings\n",
    "from utils.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from utils.optimization import BertAdam, WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset, IterableDataset)\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tokenization_utils import build_tensor\n",
    "def select_field(features, field):\n",
    "    return [\n",
    "        [\n",
    "            choice[field]\n",
    "            for choice in feature.choices_features\n",
    "        ]\n",
    "        for feature in features\n",
    "    ]\n",
    "\n",
    "def build_tensor(features):\n",
    "    all_input_ids = torch.tensor(select_field(features, 'input_ids'),\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(select_field(features, 'input_mask'),\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(select_field(features, 'segment_ids'),\n",
    "                                   dtype=torch.long)\n",
    "    all_label = torch.tensor([f.label for f in features],\n",
    "                             dtype=torch.long)\n",
    "    return TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args\n",
    "seed = 30\n",
    "do_lower_case = True\n",
    "bert_model = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "print(n_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After sorted: support-bert-data/dev/0.txt\n",
      "After sorted: support-bert-data/train/0.txt\n"
     ]
    }
   ],
   "source": [
    "val_examples = read_data(data_location,'dev')\n",
    "train_examples = read_data(data_location,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3289 race_id: support-bert-data/train/0:0, context_sentence: when tech opens info session and when he tries to close it, it hangs and will not close. i have included a screen shot as to what process  hangs in the device manager. i have tried to uninstall and reinstall the pdf application but it did not help, start_ending: what kind of issue occurred?, ending_0: a performance issue., ending_1: [NOA], ending_2: a protocol issue., label: 0\n",
      "0\n",
      "557 race_id: support-bert-data/dev/0:0, context_sentence: [AN56] data is showing as queued, almost all to [AN16], [AN31] seems to transmit okay. please see screen shot and [AN25] requirements log. i also submitted error report from admin client on device., start_ending: how would you describe your issue?, ending_0: the {[AN56]} protocols were transferred to {[AN31]} but could not be saved on your device (pc/{[AN1]})., ending_1: the {[AN56]} protocols are not sent., ending_2: [NOA], label: 1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# test_examples = read_data(data_location,'test')\n",
    "\n",
    "def build_dataset(config, examples):\n",
    "    \n",
    "    test_features = convert_examples_to_features(\n",
    "                examples, tokenizer, config['max_seq_length'], True)\n",
    "    test_data = build_tensor(test_features)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data,\n",
    "                                 sampler=test_sampler,\n",
    "                                 batch_size=4)\n",
    "    \n",
    "    return test_dataloader\n",
    "\n",
    "test_dataloader = build_dataset({'max_seq_length': 128}, examples = train_examples)\n",
    "\n",
    "val_datalaoder = build_dataset({'max_seq_length': 128}, examples = val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'can other devices connetc via your wlan network?',\n",
       " 'is the affetced {[an13]} in the same sub-net as the {[an25]} device that is unable to find it?',\n",
       " 'is the affetced {[an21]} visible in the {[an25]} \"connetcion manager\"?'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions = set([sample.start_ending.lower() for sample in train_examples])\n",
    "val_questions = set([sample.start_ending.lower() for sample in val_examples])\n",
    "test_questions = set([sample.start_ending.lower() for sample in train_examples])\n",
    "unseen_questions = train_questions - val_questions\n",
    "unseen_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unseen_samples = [example for example in val_examples if example.start_ending.lower()=='which {itool} is affected?']\n",
    "len(unseen_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 race_id: support-bert-data/dev/94:10, context_sentence: Dear Support,\n",
      "\n",
      "the ICOM  is not visible in the ISTA?s Connection Manager when it?s connected via WLAN.\n",
      "But, if we trying to using LAN Cable Network it's working.\n",
      "\n",
      "Whether we should arrange schedule for remote session [DAT] is ok? what do you think?\n",
      "\n",
      "Best Regards,\n",
      "\n",
      "\n",
      "Langga Pradana Putra, start_ending: Which {ITOOL} is affected?, ending_0: [NOA], ending_1: {ICOM}, ending_2: {IMIB R2}, label: 1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "unseen_datalaoder = build_dataset({'max_seq_length': 128}, examples = unseen_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which {itool} is affected?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[q for q in val_questions if q not in train_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'can other devices connect via your wlan network?',\n",
       " 'can other devices connetc via your wlan network?',\n",
       " 'did the issue occur for the first time immediately after performing a firmware update?',\n",
       " 'how can we help you?',\n",
       " 'how would you describe your issue?',\n",
       " 'is an error message displayed?',\n",
       " 'is the affected {itool} visible in the {ista} \"connection manager\"?',\n",
       " 'is the affetced {[an13]} in the same sub-net as the {[an25]} device that is unable to find it?',\n",
       " 'is the affetced {[an21]} visible in the {[an25]} \"connetcion manager\"?',\n",
       " 'the newest {[an25]} version in the \"software catalogue\" and the previous one are supported. older versions are not. are you using a supported version?',\n",
       " 'the newest {ista} version in the \"software catalogue\" and the previous one are supported. older versions are not. are you using a supported version?',\n",
       " 'what kind of issue occurred?',\n",
       " 'when does the issue occur?',\n",
       " 'which error message is displayed?',\n",
       " 'which {[an21]} is affetced?'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    if device.type=='cpu':\n",
    "        checkpoint = torch.load(load_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "             'multi_gpu_on':False,\n",
    "             'max_seq_length': 64,\n",
    "             'adam_eps': 6, \n",
    "             'adv_epsilon': 1e-6,\n",
    "             'adv_train': 1,\n",
    "             'adv_noise_var': 1e-5,\n",
    "             'adv_norm_level': 0,\n",
    "             'adv_step_size': 1e-3,\n",
    "             'bin_on': False,\n",
    "             'cuda': 1,\n",
    "             'encoder_type': None,\n",
    "             'fp16': True,\n",
    "             'fp16_opt_level': 'O1',\n",
    "             'global_grad_clipping': 1.0,\n",
    "             'grad_accumulation_step': 1,\n",
    "             'grad_clipping': 1,\n",
    "             'local_rank': -1,\n",
    "             'mkd_opt': 0,\n",
    "             'scheduler_type': 'ms',\n",
    "             'task_def_list': None,\n",
    "             'warmup': 0.1,\n",
    "             'warmup_schedule': 'warmup_linear',\n",
    "             'weight_decay': 0,\n",
    "             'weighted_on': False,\n",
    "             'state_dict': None,\n",
    "             'loss': 'LossCriterion.CeCriterion',\n",
    "             'epochs': 5,\n",
    "             'batch_size': 2,\n",
    "             'adv_k': 1,\n",
    "             'learning_rate':5e-5,\n",
    "             'adv_p_norm': 'inf',\n",
    "             'adv_alpha': 1,\n",
    "             'optimizer': 'adam',\n",
    "             'adv_loss': 'LossCriterion.SymKlCriterion',\n",
    "             'weight': 0,\n",
    "             'log_per_updates': 128,\n",
    "             'have_lr_scheduler': True,\n",
    "             'patience': 2,\n",
    "             'early_stopping_loss': 'train_loss'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def calc_metrics(predictions, labels):\n",
    "    return {'acc': np.round(100*accuracy_score(labels, predictions)),\n",
    "            'f1': np.round(100*f1_score(labels, predictions, average=\"weighted\"), 2)}\n",
    "     \n",
    "\n",
    "def evaluation(model, dataloader, device, val_loss_meter=None, mode='val'):\n",
    "    assert mode in ['test', 'val', 'dev', 'train']\n",
    "    with torch.no_grad():\n",
    "        [metrics,\n",
    "         predictions,\n",
    "         loss,\n",
    "         batch_size,\n",
    "         scores] = eval_model(model, dataloader, device)\n",
    "    if mode in ['val', 'dev']:\n",
    "        assert val_loss_meter is not None\n",
    "        val_loss_meter.update(loss.item(), batch_size)\n",
    "    return val_loss_meter, metrics, scores\n",
    "                         \n",
    "def eval_model(model, dataloader, device):\n",
    "    overall_predictions = []\n",
    "    overall_labels = []\n",
    "    golds = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    overall_metrics = {}\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss, logits = model(input_ids,\n",
    "                             segment_ids,\n",
    "                             input_mask,\n",
    "                             label_ids,\n",
    "                             return_logits=True)\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        prediction_confidence = np.max(logits, axis=1)\n",
    "        metrics = calc_metrics(predictions, list(label_ids))\n",
    "        \n",
    "        correct_label_prediction = prediction_confidence[predictions==label_ids]\n",
    "        wrong_label_prediction = prediction_confidence[predictions!=label_ids]\n",
    "        \n",
    "        prediction_confidence = {'correct_confidence': correct_label_prediction,\n",
    "                                'wrong_confidence': wrong_label_prediction}\n",
    "        overall_predictions.extend(predictions)\n",
    "        overall_labels.extend(label_ids)\n",
    "        \n",
    "    return metrics, predictions, overall_labels, overall_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "def calc_metrics(predictions, labels):\n",
    "    return {'acc': np.round(100*accuracy_score(labels, predictions)),\n",
    "            'f1': np.round(100*f1_score(labels, predictions, average=\"weighted\"), 2)}\n",
    "     \n",
    "\n",
    "def evaluation(model, dataloader, device, mode='val'):\n",
    "    assert mode in ['test', 'val', 'dev', 'train']\n",
    "    with torch.no_grad():\n",
    "        [metrics,\n",
    "         predictions,\n",
    "         loss,\n",
    "         batch_size,\n",
    "         scores] = eval_model(model, dataloader, device)\n",
    "    return loss.item(), batch_size, metrics, scores\n",
    "                         \n",
    "def eval_model(model, dataloader, device):\n",
    "    overall_predictions = []\n",
    "    overall_labels = []\n",
    "    golds = []\n",
    "    scores = []\n",
    "    ids = []\n",
    "    overall_metrics = {}\n",
    "    \n",
    "    \n",
    "    prediction_confidence_summary = {'correct_confidence': [],\n",
    "                            'wrong_confidence': []}\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss, logits = model(input_ids,\n",
    "                             segment_ids,\n",
    "                             input_mask,\n",
    "                             label_ids,\n",
    "                             return_logits=True)\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = label_ids.to('cpu').numpy()\n",
    "        \n",
    "        predictions = np.argmax(logits, axis=1)\n",
    "        \n",
    "        normalized_confidence = softmax(logits)\n",
    "        prediction_confidence = np.max(normalized_confidence, axis=1)\n",
    "        \n",
    "        correct_label_prediction = prediction_confidence[predictions==label_ids]\n",
    "        wrong_label_prediction = prediction_confidence[predictions!=label_ids]\n",
    "        \n",
    "        \n",
    "        prediction_confidence_summary['correct_confidence'].extend(list(correct_label_prediction))\n",
    "        prediction_confidence_summary['wrong_confidence'].extend((wrong_label_prediction))\n",
    "        overall_predictions.extend(predictions)\n",
    "        overall_labels.extend(label_ids)\n",
    "    \n",
    "    metrics = calc_metrics(overall_predictions, list(overall_labels))\n",
    "        \n",
    "    return metrics, predictions, loss, len(predictions), prediction_confidence_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_param_groups(network):\n",
    "        param_optimizer = [n for n in list(network.named_parameters()) if 'pooler' not in n[0]]\n",
    "        no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultipleChoice.from_pretrained(bert_model,\n",
    "                                              cache_dir=os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(-1)),\n",
    "                                              num_choices=3)\n",
    "\n",
    "from pytorch_pretrained_bert import BertAdam as Adam\n",
    "optimizer_parameters = _get_param_groups(model)\n",
    "optimizer = Adam(optimizer_parameters,\n",
    "                                  lr=config['learning_rate'],\n",
    "                                  warmup=config['warmup'],\n",
    "                                  t_total=len(test_dataloader),\n",
    "                                  max_grad_norm=config['grad_clipping'],\n",
    "                                  schedule=config['warmup_schedule'],\n",
    "                                  weight_decay=config['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultipleChoice(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Linear(in_features=3072, out_features=1, bias=True)\n",
       "  (decoder): BertDirectedAttention(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertDeLayer(\n",
       "        (attention): BertDeAttention(\n",
       "          (self): BertMultiAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qkey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qvalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (aquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (akey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (avalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cdropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "            (adropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertDeSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qLayerNorm): BertLayerNorm()\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertDeIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (qdense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertDeOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (qdense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (qLayerNorm): BertLayerNorm()\n",
       "          (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          (adense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (aLayerNorm): BertLayerNorm()\n",
       "          (adropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertDeLayer(\n",
       "        (attention): BertDeAttention(\n",
       "          (self): BertMultiAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qkey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qvalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (aquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (akey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (avalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cdropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "            (adropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertDeSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qLayerNorm): BertLayerNorm()\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertDeIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (qdense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertDeOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (qdense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (qLayerNorm): BertLayerNorm()\n",
       "          (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          (adense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (aLayerNorm): BertLayerNorm()\n",
       "          (adropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertDeLayer(\n",
       "        (attention): BertDeAttention(\n",
       "          (self): BertMultiAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qkey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qvalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (aquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (akey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (avalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cdropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "            (adropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertDeSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qLayerNorm): BertLayerNorm()\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertDeIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (qdense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertDeOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (qdense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (qLayerNorm): BertLayerNorm()\n",
       "          (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          (adense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (aLayerNorm): BertLayerNorm()\n",
       "          (adropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertDeLayer(\n",
       "        (attention): BertDeAttention(\n",
       "          (self): BertMultiAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qkey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qvalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (aquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (akey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (avalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cdropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "            (adropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertDeSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qLayerNorm): BertLayerNorm()\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertDeIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (qdense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertDeOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (qdense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (qLayerNorm): BertLayerNorm()\n",
       "          (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          (adense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (aLayerNorm): BertLayerNorm()\n",
       "          (adropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertDeLayer(\n",
       "        (attention): BertDeAttention(\n",
       "          (self): BertMultiAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qkey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qvalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (query_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_n): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (aquery): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (akey): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (avalue): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cdropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "            (adropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertDeSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qdense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (qLayerNorm): BertLayerNorm()\n",
       "            (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertDeIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (qdense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertDeOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (qdense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (qLayerNorm): BertLayerNorm()\n",
       "          (qdropout): Dropout(p=0.1, inplace=False)\n",
       "          (adense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (aLayerNorm): BertLayerNorm()\n",
       "          (adropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (LayerNorm): BertLayerNorm()\n",
       "  (spa): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (sa): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (spq): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (sq): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = 'model_ckppm4xbdzl.pt' # good model (test acc ~80)\n",
    "load_path = 'model_ckp3jqfdmfc.pt' # very bad test-model (test acc ~35%)\n",
    "load_path = 'model_ckp1u95gxlg.pt' # seems good (test ~75%)\n",
    "load_path = 'model_ckp1qvsjzqf.pt' # seems good (test ~92%)\n",
    "load_path = 'model_ckpe0s55geq.pt' # seems bad (test ~46%)\n",
    "model, optimizer, epoch = load_checkpoint(model, optimizer, load_path) \n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   0%|          | 0/823 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-10-23 22:22:40.869 pytorch-1-6-gpu-py3-ml-g4dn-xlarge-c630bdb4e3ad8d68ab6e5727a214:2618 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-10-23 22:22:40.892 pytorch-1-6-gpu-py3-ml-g4dn-xlarge-c630bdb4e3ad8d68ab6e5727a214:2618 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 823/823 [03:05<00:00,  4.44it/s]\n"
     ]
    }
   ],
   "source": [
    "output = eval_model(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.000e+01, 5.200e+01, 1.350e+02, 1.125e+03, 3.130e+02, 9.100e+01,\n",
       "        3.200e+01, 2.000e+00, 1.000e+00, 1.000e+00]),\n",
       " array([0.08333286, 0.083333  , 0.08333314, 0.08333329, 0.08333343,\n",
       "        0.08333357, 0.08333372, 0.08333386, 0.08333401, 0.08333415,\n",
       "        0.08333429], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(output[4]['correct_confidence'])\n",
    "plt.hist(output[4]['wrong_confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527, 1762)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[4]['correct_confidence']), len(output[4]['wrong_confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'acc': 46.0, 'f1': 46.43},\n",
       " array([2]),\n",
       " tensor(1.0986, device='cuda:0', grad_fn=<NllLossBackward>),\n",
       " 1,\n",
       " {'correct_confidence': [0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333254,\n",
       "   0.083333254,\n",
       "   0.083333336,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333309,\n",
       "   0.08333377,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333924,\n",
       "   0.083333455,\n",
       "   0.08333373,\n",
       "   0.083333254,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333327,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333351,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333298,\n",
       "   0.083333336,\n",
       "   0.08333309,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333309,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333309,\n",
       "   0.083333075,\n",
       "   0.08333397,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333365,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333924,\n",
       "   0.08333317,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333349,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.083333194,\n",
       "   0.08333381,\n",
       "   0.08333321,\n",
       "   0.08333381,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.0833333,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333351,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.083333254,\n",
       "   0.083333254,\n",
       "   0.083333254,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.083333254,\n",
       "   0.08333337,\n",
       "   0.083333686,\n",
       "   0.08333349,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333321,\n",
       "   0.08333391,\n",
       "   0.083333254,\n",
       "   0.08333323,\n",
       "   0.08333323,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333343,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333361,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333305,\n",
       "   0.08333305,\n",
       "   0.08333409,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333336,\n",
       "   0.0833333,\n",
       "   0.08333337,\n",
       "   0.08333377,\n",
       "   0.08333363,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333366,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333135,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333316,\n",
       "   0.08333385,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333321,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333305,\n",
       "   0.08333385,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333361,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333336,\n",
       "   0.08333397,\n",
       "   0.08333316,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.08333385,\n",
       "   0.08333316,\n",
       "   0.08333316,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333331,\n",
       "   0.08333331,\n",
       "   0.08333361,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.0833341,\n",
       "   0.08333321,\n",
       "   0.08333321,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333401,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333254,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333304,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.083333455,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.08333343,\n",
       "   0.0833333,\n",
       "   0.08333321,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333377,\n",
       "   0.083333135,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333389,\n",
       "   0.08333305,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333321,\n",
       "   0.08333349,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333353,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333323,\n",
       "   0.08333373,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333327,\n",
       "   0.083333574,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333015,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333574,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333361,\n",
       "   0.083333455,\n",
       "   0.08333366,\n",
       "   0.08333323,\n",
       "   0.08333351,\n",
       "   0.08333351,\n",
       "   0.08333351,\n",
       "   0.08333351,\n",
       "   0.08333381,\n",
       "   0.08333321,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333337,\n",
       "   0.08333361,\n",
       "   0.08333327,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333254,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333574,\n",
       "   0.083333254,\n",
       "   0.083333254,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333317,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333397,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333321,\n",
       "   0.08333351,\n",
       "   0.08333347,\n",
       "   0.08333347,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333309,\n",
       "   0.08333309,\n",
       "   0.08333371,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333317,\n",
       "   0.08333317,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333351,\n",
       "   0.08333343,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.083333254,\n",
       "   0.083333254,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333331,\n",
       "   0.08333331,\n",
       "   0.08333331,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333331,\n",
       "   0.08333331,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333349,\n",
       "   0.08333327,\n",
       "   0.08333327,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   ...],\n",
       "  'wrong_confidence': [0.08333349,\n",
       "   0.08333342,\n",
       "   0.083333574,\n",
       "   0.083333254,\n",
       "   0.083333336,\n",
       "   0.083333574,\n",
       "   0.08333349,\n",
       "   0.08333317,\n",
       "   0.08333351,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333385,\n",
       "   0.08333293,\n",
       "   0.08333353,\n",
       "   0.08333323,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333353,\n",
       "   0.08333365,\n",
       "   0.08333286,\n",
       "   0.0833333,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333353,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.083333395,\n",
       "   0.083333254,\n",
       "   0.08333321,\n",
       "   0.08333316,\n",
       "   0.08333316,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333323,\n",
       "   0.083333254,\n",
       "   0.08333353,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333347,\n",
       "   0.08333309,\n",
       "   0.08333365,\n",
       "   0.083333075,\n",
       "   0.08333359,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.08333331,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.083333395,\n",
       "   0.083333455,\n",
       "   0.0833333,\n",
       "   0.08333385,\n",
       "   0.08333331,\n",
       "   0.083333835,\n",
       "   0.08333317,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333365,\n",
       "   0.083333336,\n",
       "   0.08333321,\n",
       "   0.083333254,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333337,\n",
       "   0.08333343,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.083333455,\n",
       "   0.08333343,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.083333336,\n",
       "   0.08333347,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333347,\n",
       "   0.083333336,\n",
       "   0.08333321,\n",
       "   0.08333353,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.083333254,\n",
       "   0.083333574,\n",
       "   0.08333335,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.083333336,\n",
       "   0.08333349,\n",
       "   0.083333336,\n",
       "   0.0833333,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333347,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.08333361,\n",
       "   0.08333309,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333331,\n",
       "   0.083333254,\n",
       "   0.08333342,\n",
       "   0.08333353,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.08333321,\n",
       "   0.083333254,\n",
       "   0.08333365,\n",
       "   0.08333361,\n",
       "   0.08333337,\n",
       "   0.08333375,\n",
       "   0.08333305,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333353,\n",
       "   0.08333331,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333254,\n",
       "   0.083333336,\n",
       "   0.08333361,\n",
       "   0.08333286,\n",
       "   0.083333455,\n",
       "   0.08333335,\n",
       "   0.083333395,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.0833333,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.0833333,\n",
       "   0.08333347,\n",
       "   0.08333337,\n",
       "   0.08333347,\n",
       "   0.08333337,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333347,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333254,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333353,\n",
       "   0.08333323,\n",
       "   0.08333342,\n",
       "   0.083333395,\n",
       "   0.08333377,\n",
       "   0.08333361,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333321,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.083333254,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333317,\n",
       "   0.08333321,\n",
       "   0.08333377,\n",
       "   0.083333254,\n",
       "   0.083333194,\n",
       "   0.083333254,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.083333336,\n",
       "   0.08333377,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333323,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333361,\n",
       "   0.083333336,\n",
       "   0.0833333,\n",
       "   0.08333337,\n",
       "   0.083333574,\n",
       "   0.083333135,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333353,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333363,\n",
       "   0.08333317,\n",
       "   0.08333327,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333353,\n",
       "   0.08333323,\n",
       "   0.08333321,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.0833329,\n",
       "   0.083333254,\n",
       "   0.08333337,\n",
       "   0.08333353,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.083333336,\n",
       "   0.0833333,\n",
       "   0.08333317,\n",
       "   0.08333321,\n",
       "   0.0833333,\n",
       "   0.08333361,\n",
       "   0.08333353,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333015,\n",
       "   0.08333349,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.08333335,\n",
       "   0.08333349,\n",
       "   0.08333353,\n",
       "   0.08333317,\n",
       "   0.083333254,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083332874,\n",
       "   0.08333349,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.083333254,\n",
       "   0.08333317,\n",
       "   0.08333373,\n",
       "   0.08333343,\n",
       "   0.08333342,\n",
       "   0.08333343,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333361,\n",
       "   0.08333353,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.083333336,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333574,\n",
       "   0.08333321,\n",
       "   0.083333455,\n",
       "   0.083333395,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333373,\n",
       "   0.08333351,\n",
       "   0.08333347,\n",
       "   0.08333331,\n",
       "   0.083333135,\n",
       "   0.083333135,\n",
       "   0.08333321,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333321,\n",
       "   0.08333365,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333377,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333337,\n",
       "   0.08333327,\n",
       "   0.08333381,\n",
       "   0.08333337,\n",
       "   0.083333574,\n",
       "   0.083333574,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.083333574,\n",
       "   0.083333135,\n",
       "   0.08333365,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333343,\n",
       "   0.08333335,\n",
       "   0.08333343,\n",
       "   0.08333335,\n",
       "   0.08333377,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333351,\n",
       "   0.08333347,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.08333327,\n",
       "   0.08333321,\n",
       "   0.083333455,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333343,\n",
       "   0.08333349,\n",
       "   0.08333377,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.08333349,\n",
       "   0.083333455,\n",
       "   0.08333351,\n",
       "   0.08333361,\n",
       "   0.08333335,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.08333343,\n",
       "   0.083333455,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.08333309,\n",
       "   0.08333361,\n",
       "   0.08333361,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333349,\n",
       "   0.083333135,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333361,\n",
       "   0.08333321,\n",
       "   0.08333337,\n",
       "   0.08333361,\n",
       "   0.08333305,\n",
       "   0.083333336,\n",
       "   0.08333353,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.08333377,\n",
       "   0.08333321,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.083333135,\n",
       "   0.083333336,\n",
       "   0.083333254,\n",
       "   0.083333574,\n",
       "   0.083333574,\n",
       "   0.083333574,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.08333321,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333373,\n",
       "   0.083333254,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.08333305,\n",
       "   0.083333574,\n",
       "   0.083333254,\n",
       "   0.08333321,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333353,\n",
       "   0.08333343,\n",
       "   0.08333349,\n",
       "   0.08333343,\n",
       "   0.08333321,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333135,\n",
       "   0.08333323,\n",
       "   0.083333574,\n",
       "   0.083333574,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333359,\n",
       "   0.083333574,\n",
       "   0.08333321,\n",
       "   0.08333327,\n",
       "   0.083333395,\n",
       "   0.083333455,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333349,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.08333353,\n",
       "   0.08333317,\n",
       "   0.083333686,\n",
       "   0.08333335,\n",
       "   0.083333254,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333365,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.0833333,\n",
       "   0.08333342,\n",
       "   0.08333361,\n",
       "   0.08333349,\n",
       "   0.083333455,\n",
       "   0.08333349,\n",
       "   0.083333455,\n",
       "   0.08333349,\n",
       "   0.08333353,\n",
       "   0.08333342,\n",
       "   0.08333363,\n",
       "   0.08333327,\n",
       "   0.083333,\n",
       "   0.08333385,\n",
       "   0.08333321,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333349,\n",
       "   0.08333327,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333335,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333135,\n",
       "   0.083333574,\n",
       "   0.08333337,\n",
       "   0.08333321,\n",
       "   0.08333347,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333321,\n",
       "   0.08333321,\n",
       "   0.083333336,\n",
       "   0.08333379,\n",
       "   0.083333455,\n",
       "   0.08333335,\n",
       "   0.083333336,\n",
       "   0.083333455,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333347,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.08333337,\n",
       "   0.083333135,\n",
       "   0.08333351,\n",
       "   0.083333015,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333293,\n",
       "   0.08333347,\n",
       "   0.08333365,\n",
       "   0.083333336,\n",
       "   0.08333321,\n",
       "   0.083333574,\n",
       "   0.083333336,\n",
       "   0.08333385,\n",
       "   0.08333321,\n",
       "   0.08333321,\n",
       "   0.08333327,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.083333336,\n",
       "   0.08333337,\n",
       "   0.083333336,\n",
       "   0.083333395,\n",
       "   0.08333385,\n",
       "   0.083333135,\n",
       "   0.083333135,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.083333395,\n",
       "   0.08333342,\n",
       "   0.08333377,\n",
       "   0.08333337,\n",
       "   0.08333353,\n",
       "   0.08333397,\n",
       "   0.08333321,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.083333455,\n",
       "   0.08333353,\n",
       "   0.08333316,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333335,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333335,\n",
       "   0.083333455,\n",
       "   0.083333336,\n",
       "   0.08333321,\n",
       "   0.083333395,\n",
       "   0.083333395,\n",
       "   0.08333343,\n",
       "   0.08333327,\n",
       "   0.08333361,\n",
       "   0.08333317,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.0833333,\n",
       "   0.08333361,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333365,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333349,\n",
       "   0.08333343,\n",
       "   0.08333335,\n",
       "   0.08333335,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333336,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333455,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333317,\n",
       "   0.08333347,\n",
       "   0.083333395,\n",
       "   0.08333327,\n",
       "   0.083333574,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.083333254,\n",
       "   0.08333343,\n",
       "   0.083333574,\n",
       "   0.08333317,\n",
       "   0.08333377,\n",
       "   0.08333305,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333337,\n",
       "   0.08333304,\n",
       "   0.083333455,\n",
       "   0.08333365,\n",
       "   0.08333371,\n",
       "   0.08333335,\n",
       "   0.083333015,\n",
       "   0.08333337,\n",
       "   0.08333342,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.083333455,\n",
       "   0.08333347,\n",
       "   0.08333349,\n",
       "   0.08333342,\n",
       "   0.08333349,\n",
       "   0.08333379,\n",
       "   0.083333254,\n",
       "   ...]})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which error message is displayed?'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-aa0a80c38cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correct_confidence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(output[4]['correct_confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        ln = 24\n",
    "        if name.startswith('bert.encoder'):\n",
    "        \tl = name.split('.')\n",
    "        \tln = int(l[3])\n",
    "      \n",
    "        if name.startswith('bert.embeddings') or ln < 6:\n",
    "#         \tprint(name)  \n",
    "        \tparam.requires_grad = False\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def _model_init(config, model, state_dict=None, num_train_step=-1):\n",
    "        total_param = sum([p.nelement() for p in model.parameters() if p.requires_grad])\n",
    "        if config['cuda']:\n",
    "            if config['local_rank'] != -1:\n",
    "                model = model.to(device)\n",
    "            else:\n",
    "                model = model.to(device)\n",
    "        network = model\n",
    "        if state_dict:\n",
    "            missing_keys, unexpected_keys = network.load_state_dict(state_dict['state'], strict=False)\n",
    "\n",
    "        optimizer_parameters = _get_param_groups(network)\n",
    "#         try:\n",
    "        optimizer, scheduler = _setup_optim(config,optimizer_parameters, state_dict, num_train_step, network)\n",
    "#         except Exceptio#work: {network}')\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        #if self.config[\"local_rank\"] not in [-1, 0]:\n",
    "        #    torch.distributed.barrier()\n",
    "\n",
    "        if config['local_rank'] != -1:\n",
    "            mnetwork = torch.nn.parallel.DistributedDataParallel(network, device_ids=[self.config[\"local_rank\"]], output_device=self.config[\"local_rank\"], find_unused_parameters=True)\n",
    "        elif config['multi_gpu_on']:\n",
    "            mnetwork = torch.nn.DataParallel(network, device_ids=[0, 1, 2])\n",
    "        else:\n",
    "            mnetwork = network\n",
    "        task_loss_criterion = _setup_lossmap(config)\n",
    "        adv_task_loss_criterion = _setup_adv_lossmap(config)\n",
    "        adv_teacher = _setup_adv_training(config, adv_task_loss_criterion)\n",
    "        \n",
    "        return [mnetwork,\n",
    "                task_loss_criterion,\n",
    "                adv_task_loss_criterion,\n",
    "                adv_teacher,\n",
    "                optimizer_parameters,\n",
    "                optimizer,\n",
    "                scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perturbation import SmartPerturbation\n",
    "\n",
    "def _setup_adv_training(config, adv_task_loss_criterion):\n",
    "        adv_teacher = None\n",
    "        if config.get('adv_train', False):\n",
    "            adv_teacher = SmartPerturbation(config['adv_epsilon'],\n",
    "                    config['multi_gpu_on'],\n",
    "                    config['adv_step_size'],\n",
    "                    config['adv_noise_var'],\n",
    "                    config['adv_p_norm'],\n",
    "                    config['adv_k'],\n",
    "                    config['fp16'],\n",
    "                    config['encoder_type'],\n",
    "                    loss_map=adv_task_loss_criterion,\n",
    "                    norm_level=config['adv_norm_level'])\n",
    "        return adv_teacher\n",
    "            \n",
    "def _setup_adv_lossmap(config):\n",
    "        task_def_list: List[TaskDef] = config['task_def_list']\n",
    "        adv_task_loss_criterion = []\n",
    "        if config.get('adv_train', False):\n",
    "            cs = config['adv_loss']\n",
    "            assert cs in ['LossCriterion.SymKlCriterion', 'LossCriterion.KlCriterion']\n",
    "            if cs == 'LossCriterion.SymKlCriterion':\n",
    "                lc = LOSS_REGISTRY[LossCriterion.SymKlCriterion](name='Adv Loss func of task {}: {}'.format(0, cs))\n",
    "                adv_task_loss_criterion.append(lc)\n",
    "            else:\n",
    "                lc = LOSS_REGISTRY[LossCriterion.KlCriterion](name='Adv Loss func of task {}: {}'.format(0, cs))\n",
    "                adv_task_loss_criterion.append(lc)\n",
    "            return adv_task_loss_criterion\n",
    "            \n",
    "def _setup_lossmap(config):\n",
    "        task_def_list: List[TaskDef] = config['task_def_list']\n",
    "        task_loss_criterion = []\n",
    "        cs = config['loss'] # this loss has later to be passed through config file\n",
    "        if cs=='LossCriterion.CeCriterion':\n",
    "            lc = LOSS_REGISTRY[LossCriterion.CeCriterion](name='Loss func of task {}: {}'.format(0, cs)) \n",
    "            task_loss_criterion.append(lc)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return task_loss_criterion\n",
    "            \n",
    "def _get_param_groups(network):\n",
    "        param_optimizer = [n for n in list(network.named_parameters()) if 'pooler' not in n[0]]\n",
    "        no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    \n",
    "from pytorch_pretrained_bert import BertAdam as Adam\n",
    "\n",
    "def _setup_optim(config, optimizer_parameters, state_dict=None, num_train_step=-1, network=None):\n",
    "    if config['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(optimizer_parameters, config['learning_rate'],\n",
    "                                   weight_decay=config['weight_decay'])\n",
    "\n",
    "    elif config['optimizer'] == 'adamax':\n",
    "        optimizer = Adamax(optimizer_parameters,\n",
    "                                config['learning_rate'],\n",
    "                                warmup=config['warmup'],\n",
    "                                t_total=num_train_step,\n",
    "                                max_grad_norm=config['grad_clipping'],\n",
    "                                schedule=config['warmup_schedule'],\n",
    "                                weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "    elif config['optimizer'] == 'radam':\n",
    "        optimizer = RAdam(optimizer_parameters,\n",
    "                                config['learning_rate'],\n",
    "                                warmup=config['warmup'],\n",
    "                                t_total=num_train_step,\n",
    "                                max_grad_norm=config['grad_clipping'],\n",
    "                                schedule=config['warmup_schedule'],\n",
    "                                eps=config['adam_eps'],\n",
    "                                weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "        # The current radam does not support FP16.\n",
    "        config['fp16'] = False\n",
    "    elif config['optimizer'] == 'adam':\n",
    "        optimizer = Adam(optimizer_parameters,\n",
    "                              lr=config['learning_rate'],\n",
    "                              warmup=config['warmup'],\n",
    "                              t_total=num_train_step,\n",
    "                              max_grad_norm=config['grad_clipping'],\n",
    "                              schedule=config['warmup_schedule'],\n",
    "                              weight_decay=config['weight_decay'])\n",
    "        if config.get('have_lr_scheduler', False): config['have_lr_scheduler'] = False\n",
    "    else:\n",
    "        raise RuntimeError('Unsupported optimizer: %s' % opt['optimizer'])\n",
    "\n",
    "    if state_dict and 'optimizer' in state_dict:\n",
    "        optimizer.load_state_dict(state_dict['optimizer'])\n",
    "\n",
    "    if config['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "            global amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(network, optimizer, opt_level=config['fp16_opt_level'])\n",
    "        network = model\n",
    "        optimizer = optimizer\n",
    "\n",
    "    if config.get('have_lr_scheduler', False):\n",
    "        if config.get('scheduler_type', 'rop') == 'rop':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=config['lr_gamma'], patience=3)\n",
    "        elif config.get('scheduler_type', 'rop') == 'exp':\n",
    "            scheduler = ExponentialLR(optimizer, gamma=config.get('lr_gamma', 0.95))\n",
    "        else:\n",
    "            milestones = [int(step) for step in config.get('multi_step_lr', '10,20,30').split(',')]\n",
    "            scheduler = MultiStepLR(optimizer, milestones=milestones, gamma=config.get('lr_gamma'))\n",
    "    else:\n",
    "        scheduler = None\n",
    "        \n",
    "    return optimizer, scheduler\n",
    "\n",
    "def perturbated_loss():\n",
    "    smartPerturbation = SmartPerturbation(epsilon=1e-6,\n",
    "                    multi_gpu_on=False,\n",
    "                    step_size=1e-3,\n",
    "                    noise_var=1e-5,\n",
    "                    norm_p='inf',\n",
    "                    k=1,\n",
    "                    fp16=False,\n",
    "                    encoder_type=EncoderModelType.BERT,\n",
    "                    loss_map=[SymKlCriterion],\n",
    "                    norm_level=0)\n",
    "    \n",
    "    return smartPerturbation\n",
    "\n",
    "def _norm_grad(grad, norm_p, epsilon, eff_grad=None, sentence_level=False):\n",
    "        eff_direction = 0\n",
    "        if norm_p == 'l2':\n",
    "            if sentence_level:\n",
    "                direction = grad / (torch.norm(grad, dim=(-2, -1), keepdim=True) + epsilon)\n",
    "            else:\n",
    "                direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + epsilon)\n",
    "        elif norm_p == 'l1':\n",
    "            direction = grad.sign()\n",
    "        else:\n",
    "            if sentence_level:\n",
    "                direction = grad / (grad.abs().max((-2, -1), keepdim=True)[0] + epsilon)\n",
    "            else:\n",
    "                direction = grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
    "                eff_direction = eff_grad / (grad.abs().max(-1, keepdim=True)[0] + epsilon)\n",
    "        return direction, eff_direction\n",
    "    \n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    #print(outputs,outputs == labels)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "from loss import stable_kl\n",
    "\n",
    "def generate_noise(embed, mask, epsilon=1e-5):\n",
    "    noise = embed.data.new(embed.size()).normal_(0, 1) *  epsilon\n",
    "    noise.detach()\n",
    "    noise.requires_grad_()\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_save_ckp(metric, best_metric, model, optimizer, save_path, epoch, metric_name='train_loss'):\n",
    "    if metric < best_metric:\n",
    "        wandb.summary[f\"best_{metric_name}\"] = metric\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }, save_path)\n",
    "        return metric\n",
    "    return best_metric\n",
    "    \n",
    "def load_checkpoint(model, optimizer, load_path):\n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    return model, optimizer, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  model,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  run_id,\n",
    "                  save_path=''):\n",
    "    \n",
    "    # preliminaries for training loop\n",
    "    loss_scale=0\n",
    "    output_dir=\"large_models\"\n",
    "    %mkdir large_models\n",
    "    output_train_file = os.path.join(output_dir, \"train_results.txt\")\n",
    "    output_smart_train_file = os.path.join(output_dir, \"smart_train_results.txt\")\n",
    "    loss_writer = open(output_train_file, \"w\",1)\n",
    "    smart_loss_writer = open(output_smart_train_file, \"w\",1)\n",
    "\n",
    "    train_loss_meter = AverageMeter()\n",
    "    adv_loss_meter = AverageMeter()\n",
    "    emb_val_meter = AverageMeter()\n",
    "    eff_perturb_meter = AverageMeter()\n",
    "    val_loss_meter = AverageMeter()\n",
    "    \n",
    "    local_updates = 0\n",
    "    updates =0\n",
    "    best_loss = np.inf\n",
    "    last_es_criterion = np.inf\n",
    "    accumulated_tr_loss = 0\n",
    "    accumulated_std_tr_loss = 0\n",
    "    overall_wrong_confidence_scores = []\n",
    "    overall_correct_confidence_scores = []\n",
    "    trigger_times = 0\n",
    "    stopping = False\n",
    "    best_metrics = {}\n",
    "\n",
    "    for epoch in trange(int(config['epochs']), desc=\"Epoch\"):\n",
    "                tr_loss = 0\n",
    "                last_tr_loss = 0\n",
    "                std_loss = 0\n",
    "\n",
    "                nb_tr_examples, nb_tr_steps = 0, 0\n",
    "                for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "                    batch = tuple(t.to(device) for t in batch)\n",
    "                    input_ids, input_mask, segment_ids, label_ids = batch\n",
    "                    logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = task_loss_criterion[0](logits, label_ids, config['weight'], ignore_index=-1)\n",
    "                    \n",
    "                    if config['adv_train']:\n",
    "                        if config['multi_gpu_on']:\n",
    "                            embed = torch.stack([model.module.bert.embeddings(ids, mask) for ids, mask in zip(input_ids, input_mask)]) # recover batch-embeddings\n",
    "                        else:\n",
    "                            embed = torch.stack([model.bert.embeddings(ids, mask) for ids, mask in zip(input_ids, input_mask)]) # recover batch-embeddings\n",
    "                        noise = generate_noise(embed, input_mask, config['adv_noise_var'])\n",
    "                        for step in range(config['adv_k']):\n",
    "                            adv_logits = torch.stack([model(input_ids=ids,\n",
    "                                                               token_type_ids=mask,\n",
    "                                                               attention_mask=segment,\n",
    "                                                               inputs_embeds=in_embed)[-1] for ids, mask, segment, in_embed in zip(input_ids,\n",
    "                                                                                                                                   input_mask,\n",
    "                                                                                                                                   segment_ids,\n",
    "                                                                                                                                   noise+embed)\n",
    "                                                     ]\n",
    "                                                    )\n",
    "                            adv_loss = stable_kl(adv_logits, logits.detach(), reduce=False)\n",
    "                            delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True, retain_graph=False)\n",
    "                            norm = delta_grad.norm()\n",
    "                            if (torch.isnan(norm) or torch.isinf(norm)):\n",
    "                                adv_loss = emb_val = eff_perturbation = 0\n",
    "                                break\n",
    "                            eff_delta_grad = delta_grad * config['adv_step_size']\n",
    "                            delta_grad = noise + delta_grad * config['adv_step_size']\n",
    "                            noise, eff_noise = _norm_grad(delta_grad,\n",
    "                                                          norm_p=config['adv_p_norm'],\n",
    "                                                          epsilon=config['adv_epsilon'],\n",
    "                                                          eff_grad=eff_delta_grad,\n",
    "                                                          sentence_level=config['adv_norm_level'])\n",
    "                            noise = noise.detach()\n",
    "                            noise.requires_grad_()\n",
    "                        adv_logits = torch.stack([model(input_ids=ids,\n",
    "                                               token_type_ids=mask,\n",
    "                                               attention_mask=segment,\n",
    "                                               inputs_embeds=in_embed)[-1] for ids, mask, segment, in_embed in zip(input_ids,\n",
    "                                                                                                                   input_mask,\n",
    "                                                                                                                   segment_ids,\n",
    "                                                                                                                   noise+embed)\n",
    "                                                 ]\n",
    "                                                )\n",
    "                        adv_loss = adv_task_loss_criterion[-1](logits, adv_logits, ignore_index=-1)\n",
    "                        emb_val = embed.detach().abs().mean()\n",
    "                        try:\n",
    "                            eff_perturb = eff_noise.detach().abs().mean()\n",
    "                            eff_perturb_meter.update(eff_perturb.item(), config['batch_size'])\n",
    "                        except AttributeError:\n",
    "                            eff_perturb = 0\n",
    "                            eff_perturb_meter.update(0, config['batch_size'])\n",
    "\n",
    "                        loss += config['adv_alpha']*adv_loss\n",
    "                        adv_loss_meter.update(adv_loss.item(), config['batch_size'])\n",
    "                        emb_val_meter.update(emb_val.item(), config['batch_size'])\n",
    "                    \n",
    "                    else:\n",
    "                        adv_loss = 0\n",
    "                        emb_val = 0\n",
    "                        adv_loss_meter.update(adv_loss, config['batch_size'])\n",
    "                        emb_val_meter.update(emb_val, config['batch_size'])\n",
    "                    train_loss_meter.update(loss.item(), config['batch_size'])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    loss = loss /config.get('grad_accumulation_step', 1)\n",
    "                    if config['fp16']:\n",
    "                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                            scaled_loss.backward()\n",
    "\n",
    "                    local_updates += 1\n",
    "                    if local_updates % config.get('grad_accumulation_step', 1) == 0:\n",
    "                        if config['global_grad_clipping'] > 0:\n",
    "                            if config['fp16']:\n",
    "                                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                                               config['global_grad_clipping'])\n",
    "                            else:\n",
    "                                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                                               config['global_grad_clipping'])\n",
    "                        updates += 1\n",
    "                        # reset number of the grad accumulation\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad() \n",
    "                    \n",
    "    \n",
    "#                     accumulated_tr_loss+=train_loss_meter.val\n",
    "#                     accumulated_std_tr_loss+=train_loss_meter.val-adv_loss_meter.val\n",
    "                    if (updates) % (config['log_per_updates']) == 0 or updates == 1:\n",
    "        \n",
    "                        print('evaluating model......')\n",
    "                        wandb.log({\"avg standard loss\": train_loss_meter.avg - adv_loss_meter.avg})\n",
    "                        wandb.log({\"avg regularized loss\": train_loss_meter.avg})\n",
    "                                                \n",
    "#                         wandb.log({\"moving avg standard batch loss\": accumulated_std_tr_loss/config['log_per_updates']})\n",
    "#                         wandb.log({\"moving avg regularized batch loss\": accumulated_tr_loss/config['log_per_updates']})\n",
    "                        \n",
    "                        # Early stopping\n",
    "                        if config['early_stopping_loss']=='train_loss':\n",
    "                            es_criterion = train_loss_meter.avg\n",
    "                        \n",
    "#                         accumulated_tr_loss = 0\n",
    "#                         accumulated_std_tr_loss = 0\n",
    "                        \n",
    "                        best_loss=check_save_ckp(train_loss_meter.val,\n",
    "                                                 best_loss,\n",
    "                                                 model,\n",
    "                                                 optimizer,\n",
    "                                                 f'model_ckp{run_id}.pt',\n",
    "                                                 epoch,\n",
    "                                                 'train_loss')\n",
    "                        \n",
    "                        _, metrics, _ = evaluation(model,\n",
    "                                             train_dataloader,\n",
    "                                             device,\n",
    "                                             mode='train')\n",
    "                \n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'train {key}': val})\n",
    "                            if val < best_metrics.get(f'train {key}', np.inf):\n",
    "                                wandb.summary[f'train {key}'] = val\n",
    "                                best_metrics[f'train {key}'] = val\n",
    "                            \n",
    "                        val_loss_meter, metrics, scores = evaluation(model,\n",
    "                                                                     eval_dataloader,\n",
    "                                                                     device,\n",
    "                                                                     val_loss_meter,\n",
    "                                                                     mode='val')\n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'val {key}': val})\n",
    "                            if val < best_metrics.get(f'val {key}', np.inf):\n",
    "                                wandb.summary[f'val {key}'] = val\n",
    "                                best_metrics[f'val {key}'] = val\n",
    "                        \n",
    "                        wandb.log({\"avg eval loss\": val_loss_meter.avg})\n",
    "                        if val_loss_meter.avg < best_metrics.get('best_avg_val_loss', np.inf):\n",
    "                            wandb.summary[f\"best eval loss\"] = val_loss_meter.avg\n",
    "                            best_metrics['best_avg_val_loss'] = val_loss_meter.avg\n",
    "                        _, metrics, scores = evaluation(model,\n",
    "                                                        test_dataloader,\n",
    "                                                        device,\n",
    "                                                        mode='test')\n",
    "                        for key, val in metrics.items():\n",
    "                            wandb.log({f'test {key}': val})\n",
    "                            if val < best_metrics.get(f'test {key}', np.inf):\n",
    "                                wandb.summary[f'test {key}'] = val\n",
    "                                best_metrics[f'test {key}'] = val\n",
    "                            \n",
    "                        if config['early_stopping_loss']!='train_loss':\n",
    "                            es_criterion = val_loss_meter.avg\n",
    "                        \n",
    "                        \n",
    "                        if es_criterion > last_es_criterion:\n",
    "                            trigger_times += 1\n",
    "\n",
    "                            if trigger_times >= config['patience']:\n",
    "                                stopping=True\n",
    "\n",
    "                        else:\n",
    "                            trigger_times = 0\n",
    "\n",
    "                        last_es_criterion = es_criterion\n",
    "                    \n",
    "                        overall_correct_confidence_scores.extend(scores['correct_confidence'])\n",
    "                        overall_wrong_confidence_scores.extend(scores['wrong_confidence'])\n",
    "                        \n",
    "                    \n",
    "                    overall_correct_confidence_scores = [[sigmoid(s)] for s in overall_correct_confidence_scores]\n",
    "                    overall_wrong_confidence_scores = [[sigmoid(s)] for s in overall_wrong_confidence_scores]\n",
    "                    correct_table = wandb.Table(data=overall_correct_confidence_scores, columns=[\"confidence scores\"])\n",
    "                    wrong_table = wandb.Table(data=overall_wrong_confidence_scores, columns=[\"confidence scores\"])\n",
    "                    \n",
    "                    wandb.log({f'correct_pred_scores epoch {epoch}': wandb.plot.histogram(correct_table,\n",
    "                                                                    \"scores\",\n",
    "                                                                    title=\"(Correct) Prediction Score Distribution\")})\n",
    "                    wandb.log({f'wrong_pred_scores epoch {epoch}': wandb.plot.histogram(wrong_table,\n",
    "                                                                    \"scores\",\n",
    "                                                                    title=\"(Wrong) Prediction Score Distribution\")})\n",
    "                    \n",
    "                    overall_wrong_confidence_scores = []\n",
    "                    overall_correct_confidence_scores = []\n",
    "                    \n",
    "                    wandb.log({'batch adv_loss': train_loss_meter.avg})\n",
    "                    wandb.log({'batch std_loss': train_loss_meter.avg - adv_loss_meter.avg})\n",
    "                    if stopping==True:\n",
    "                        print(f'STOPPING at epoch {epoch}')\n",
    "                        return model\n",
    "#                     if nb_tr_examples % 512 == 0 or False:\n",
    "            \n",
    "            \n",
    "\n",
    "                        \n",
    "                        \n",
    "#                         loss_log = (train_loss_meter.val - last_tr_loss)*1.0/512\n",
    "#                         print(nb_tr_examples,loss_log)\n",
    "#                         loss_writer.write(\"%d %f \\n\" % (nb_tr_examples,loss_log))\n",
    "#                         smart_loss_writer.write((\"%d %f \\n\" % (nb_tr_examples,train_loss_meter.val)))\n",
    "#                         last_tr_loss = tr_loss\n",
    "\n",
    "#                         # evaluate model\n",
    "#                         model.eval()\n",
    "#                         tr_val_loss = 0\n",
    "#                         eval_loss, eval_accuracy = 0, 0\n",
    "#                         nb_eval_steps, nb_eval_examples = 0, 0\n",
    "#                         total_logits = []\n",
    "#                         total_labels = []\n",
    "#                         for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "#                             input_ids = input_ids.to(device)\n",
    "#                             input_mask = input_mask.to(device)\n",
    "#                             segment_ids = segment_ids.to(device)\n",
    "#                             label_ids = label_ids.to(device)\n",
    "\n",
    "#                             with torch.no_grad():\n",
    "#                                 tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "#                                 logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "#                             logits = logits.detach().cpu().numpy()\n",
    "#                             label_ids = label_ids.to('cpu').numpy()\n",
    "#                             tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "#                             eval_loss += tmp_eval_loss.mean().item()\n",
    "#                             eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#                             nb_eval_examples += input_ids.size(0)\n",
    "#                             nb_eval_steps += 1\n",
    "\n",
    "#                             total_logits.append(logits)\n",
    "#                             total_labels.append(label_ids)\n",
    "\n",
    "#                         total_logits = np.concatenate(total_logits)\n",
    "#                         total_labels = np.concatenate(total_labels)\n",
    "\n",
    "#                         eval_loss = eval_loss / nb_eval_steps\n",
    "#                         eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "#                         wandb.log({'eval_loss': eval_loss,\n",
    "#                                   'eval_accuracy': eval_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset\n",
    "# train_dataloader, eval_dataloader = build_dataset(wandb.config)\n",
    "\n",
    "def train():\n",
    "    default_config = {\n",
    "             'multi_gpu_on':False,\n",
    "             'max_seq_length': 64,\n",
    "             'adam_eps': 6, \n",
    "             'adv_epsilon': 1e-6,\n",
    "             'adv_train': 1,\n",
    "             'adv_noise_var': 1e-5,\n",
    "             'adv_norm_level': 0,\n",
    "             'adv_step_size': 1e-3,\n",
    "             'bin_on': False,\n",
    "             'cuda': 1,\n",
    "             'encoder_type': None,\n",
    "             'fp16': True,\n",
    "             'fp16_opt_level': 'O1',\n",
    "             'global_grad_clipping': 1.0,\n",
    "             'grad_accumulation_step': 1,\n",
    "             'grad_clipping': 1,\n",
    "             'local_rank': -1,\n",
    "             'mkd_opt': 0,\n",
    "             'scheduler_type': 'ms',\n",
    "             'task_def_list': None,\n",
    "             'warmup': 0.1,\n",
    "             'warmup_schedule': 'warmup_linear',\n",
    "             'weight_decay': 0,\n",
    "             'weighted_on': False,\n",
    "             'state_dict': None,\n",
    "             'loss': 'LossCriterion.CeCriterion',\n",
    "             'epochs': 5,\n",
    "             'batch_size': 4,\n",
    "             'adv_k': 1,\n",
    "             'learning_rate':5e-5,\n",
    "             'adv_p_norm': 'inf',\n",
    "             'adv_alpha': 1,\n",
    "             'optimizer': 'adam',\n",
    "             'adv_loss': 'LossCriterion.SymKlCriterion',\n",
    "             'weight': 0,\n",
    "             'log_per_updates': 128,\n",
    "             'have_lr_scheduler': True,\n",
    "             'patience': 2,\n",
    "             'early_stopping_loss': 'train_loss'\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"master-thesis\", config=default_config)\n",
    "    \n",
    "    device = default_config['cuda']\n",
    "    \n",
    "    config = wandb.config\n",
    "    run_id = wandb.run.id\n",
    "    \n",
    "#     # build_dataset\n",
    "#     train_dataloader, eval_dataloader = build_dataset(config)\n",
    "    \n",
    "    # create model\n",
    "    model = create_model()\n",
    "    \n",
    "    # initialize model and losses\n",
    "    [mnetwork,\n",
    "     task_loss_criterion,\n",
    "     adv_task_loss_criterion,\n",
    "     adv_teacher,\n",
    "     optimizer_parameters,\n",
    "     optimizer,\n",
    "     scheduler] = _model_init(config=config, model=model, num_train_step=len(train_dataloader))\n",
    "    \n",
    "    # training mode ON\n",
    "    mnetwork.train()\n",
    "    \n",
    "    # wandb watch\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # train/eval\n",
    "    training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler,\n",
    "                  run_id)\n",
    "    \n",
    "    # stop wandb\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3gsgzps8) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 4687... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg eval loss</td><td>█▃▁</td></tr><tr><td>avg regularized loss</td><td>█▂▁</td></tr><tr><td>avg standard loss</td><td>█▁▁</td></tr><tr><td>batch adv_loss</td><td>▇███▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch std_loss</td><td>███▇▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test acc</td><td>▁▇█</td></tr><tr><td>test f1</td><td>▁▇█</td></tr><tr><td>val acc</td><td>▁▁▁▁██</td></tr><tr><td>val f1</td><td>▁▁▁▁█▄</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg eval loss</td><td>36.99938</td></tr><tr><td>avg regularized loss</td><td>13.01055</td></tr><tr><td>avg standard loss</td><td>7.70393</td></tr><tr><td>batch adv_loss</td><td>11.9723</td></tr><tr><td>batch std_loss</td><td>7.10215</td></tr><tr><td>best eval loss</td><td>36.99938</td></tr><tr><td>best_train_loss</td><td>0.94481</td></tr><tr><td>test acc</td><td>53.0</td></tr><tr><td>test f1</td><td>53.35</td></tr><tr><td>val acc</td><td>50.0</td></tr><tr><td>val f1</td><td>33.33</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 562 media file(s), 6 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">revived-field-481</strong>: <a href=\"https://wandb.ai/wassimboubaker/master-thesis/runs/3gsgzps8\" target=\"_blank\">https://wandb.ai/wassimboubaker/master-thesis/runs/3gsgzps8</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211020_134632-3gsgzps8/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3gsgzps8). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/wassimboubaker/master-thesis/runs/1jq4krqc\" target=\"_blank\">cosmic-star-482</a></strong> to <a href=\"https://wandb.ai/wassimboubaker/master-thesis\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "mkdir: cannot create directory ‘large_models’: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/1645 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 1/1645 [00:12<5:37:58, 12.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 2/1645 [00:14<4:10:51,  9.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 3/1645 [00:15<3:10:02,  6.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 4/1645 [00:17<2:27:03,  5.38s/it]\u001b[A\n",
      "Iteration:   0%|          | 5/1645 [00:19<1:58:14,  4.33s/it]\u001b[A\n",
      "Iteration:   0%|          | 6/1645 [00:21<1:38:22,  3.60s/it]\u001b[A\n",
      "Iteration:   0%|          | 7/1645 [00:23<1:24:17,  3.09s/it]\u001b[A\n",
      "Iteration:   0%|          | 8/1645 [00:25<1:14:13,  2.72s/it]\u001b[A\n",
      "Iteration:   1%|          | 9/1645 [00:27<1:07:26,  2.47s/it]\u001b[A\n",
      "Iteration:   1%|          | 10/1645 [00:28<1:03:04,  2.31s/it]\u001b[A\n",
      "Iteration:   1%|          | 11/1645 [00:30<59:44,  2.19s/it]  \u001b[A\n",
      "Iteration:   1%|          | 12/1645 [00:32<57:05,  2.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   1%|          | 13/1645 [00:34<53:59,  1.99s/it]\u001b[A\n",
      "Iteration:   1%|          | 14/1645 [00:36<53:06,  1.95s/it]\u001b[A\n",
      "Iteration:   1%|          | 15/1645 [00:38<52:25,  1.93s/it]\u001b[A\n",
      "Iteration:   1%|          | 16/1645 [00:40<51:56,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|          | 17/1645 [00:41<51:49,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|          | 18/1645 [00:43<51:23,  1.90s/it]\u001b[A\n",
      "Iteration:   1%|          | 19/1645 [00:45<51:14,  1.89s/it]\u001b[A\n",
      "Iteration:   1%|          | 20/1645 [00:47<51:08,  1.89s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 21/1645 [00:49<52:02,  1.92s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 22/1645 [00:51<51:45,  1.91s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 23/1645 [00:53<51:28,  1.90s/it]\u001b[A\n",
      "Iteration:   1%|▏         | 24/1645 [00:55<51:16,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 25/1645 [00:57<51:11,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 26/1645 [00:59<52:02,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 27/1645 [01:01<52:15,  1.94s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 28/1645 [01:03<52:00,  1.93s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 29/1645 [01:04<51:46,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 30/1645 [01:06<51:38,  1.92s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 31/1645 [01:08<51:26,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 32/1645 [01:10<51:06,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 33/1645 [01:12<51:10,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 34/1645 [01:14<51:05,  1.90s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 35/1645 [01:16<51:14,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 36/1645 [01:18<51:09,  1.91s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 37/1645 [01:20<53:39,  2.00s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 38/1645 [01:22<52:36,  1.96s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 39/1645 [01:24<52:33,  1.96s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 40/1645 [01:26<52:26,  1.96s/it]\u001b[A\n",
      "Iteration:   2%|▏         | 41/1645 [01:29<1:00:42,  2.27s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 42/1645 [01:31<57:46,  2.16s/it]  \u001b[A\n",
      "Iteration:   3%|▎         | 43/1645 [01:33<56:46,  2.13s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 44/1645 [01:35<55:05,  2.06s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 45/1645 [01:37<53:48,  2.02s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 46/1645 [01:38<52:34,  1.97s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 47/1645 [01:40<52:15,  1.96s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 48/1645 [01:42<51:48,  1.95s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 49/1645 [01:44<51:20,  1.93s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 50/1645 [01:46<50:46,  1.91s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 51/1645 [01:48<50:33,  1.90s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 52/1645 [01:50<50:30,  1.90s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 53/1645 [01:52<50:19,  1.90s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 54/1645 [01:54<50:03,  1.89s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 55/1645 [01:55<49:53,  1.88s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 56/1645 [01:57<49:58,  1.89s/it]\u001b[A\n",
      "Iteration:   3%|▎         | 57/1645 [01:59<49:53,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 58/1645 [02:01<50:43,  1.92s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 59/1645 [02:03<50:23,  1.91s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 60/1645 [02:05<50:09,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▎         | 61/1645 [02:07<50:13,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 62/1645 [02:09<50:09,  1.90s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 63/1645 [02:11<50:21,  1.91s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 64/1645 [02:13<50:19,  1.91s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 65/1645 [02:15<50:12,  1.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   4%|▍         | 66/1645 [02:16<48:59,  1.86s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 67/1645 [02:18<49:03,  1.87s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 68/1645 [02:20<49:27,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 69/1645 [02:22<49:30,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 70/1645 [02:24<49:40,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 71/1645 [02:26<49:41,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 72/1645 [02:28<49:28,  1.89s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 73/1645 [02:30<49:21,  1.88s/it]\u001b[A\n",
      "Iteration:   4%|▍         | 74/1645 [02:31<49:28,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 75/1645 [02:34<51:43,  1.98s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 76/1645 [02:36<51:07,  1.95s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 77/1645 [02:37<50:37,  1.94s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 78/1645 [02:39<50:20,  1.93s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 79/1645 [02:41<49:55,  1.91s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 80/1645 [02:43<49:42,  1.91s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 81/1645 [02:45<49:28,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▍         | 82/1645 [02:47<49:37,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 83/1645 [02:49<49:39,  1.91s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 84/1645 [02:51<49:19,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 85/1645 [02:53<49:05,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 86/1645 [02:54<49:01,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 87/1645 [02:56<49:08,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 88/1645 [02:58<49:03,  1.89s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 89/1645 [03:00<49:11,  1.90s/it]\u001b[A\n",
      "Iteration:   5%|▌         | 90/1645 [03:02<50:24,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 91/1645 [03:04<49:54,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 92/1645 [03:06<49:40,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 93/1645 [03:08<49:49,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 94/1645 [03:10<50:41,  1.96s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 95/1645 [03:12<50:08,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 96/1645 [03:14<50:05,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 97/1645 [03:16<49:51,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 98/1645 [03:18<49:35,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 99/1645 [03:20<51:11,  1.99s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 100/1645 [03:22<50:47,  1.97s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 101/1645 [03:24<50:18,  1.95s/it]\u001b[A\n",
      "Iteration:   6%|▌         | 102/1645 [03:25<49:57,  1.94s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 103/1645 [03:27<49:28,  1.93s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 104/1645 [03:29<49:18,  1.92s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 105/1645 [03:31<48:59,  1.91s/it]\u001b[A\n",
      "Iteration:   6%|▋         | 106/1645 [03:33<49:04,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 107/1645 [03:35<48:49,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 108/1645 [03:37<48:59,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 109/1645 [03:39<48:47,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 110/1645 [03:41<48:53,  1.91s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 111/1645 [03:43<48:41,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 112/1645 [03:45<48:35,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 113/1645 [03:46<48:32,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 114/1645 [03:48<48:19,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 115/1645 [03:50<48:17,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 116/1645 [03:52<48:23,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 117/1645 [03:54<48:17,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 118/1645 [03:56<48:15,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 119/1645 [03:58<48:14,  1.90s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 120/1645 [04:00<48:03,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 121/1645 [04:02<48:02,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 122/1645 [04:03<48:04,  1.89s/it]\u001b[A\n",
      "Iteration:   7%|▋         | 123/1645 [04:05<48:32,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 124/1645 [04:07<48:17,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 125/1645 [04:09<48:07,  1.90s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 126/1645 [04:11<47:57,  1.89s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 127/1645 [04:13<47:56,  1.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   8%|▊         | 128/1645 [04:16<54:28,  2.15s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 129/1645 [04:18<52:27,  2.08s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 130/1645 [04:20<51:04,  2.02s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 131/1645 [04:21<50:10,  1.99s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 132/1645 [04:23<49:27,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 133/1645 [04:25<49:20,  1.96s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 134/1645 [04:27<49:31,  1.97s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 135/1645 [04:29<49:04,  1.95s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 136/1645 [04:31<48:48,  1.94s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 137/1645 [04:33<48:18,  1.92s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 138/1645 [04:35<47:57,  1.91s/it]\u001b[A\n",
      "Iteration:   8%|▊         | 139/1645 [04:37<48:19,  1.93s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 140/1645 [04:39<48:01,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 141/1645 [04:41<48:07,  1.92s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 142/1645 [04:42<47:43,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▊         | 143/1645 [04:44<47:41,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 144/1645 [04:46<47:35,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 145/1645 [04:48<47:44,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 146/1645 [04:50<47:30,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 147/1645 [04:52<47:19,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 148/1645 [04:54<47:13,  1.89s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 149/1645 [04:56<47:16,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 150/1645 [04:58<47:40,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 151/1645 [05:00<47:27,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 152/1645 [05:02<47:22,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 153/1645 [05:03<47:11,  1.90s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 154/1645 [05:05<47:24,  1.91s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 155/1645 [05:08<49:51,  2.01s/it]\u001b[A\n",
      "Iteration:   9%|▉         | 156/1645 [05:09<48:57,  1.97s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 157/1645 [05:11<48:16,  1.95s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 158/1645 [05:13<47:41,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 159/1645 [05:15<47:18,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 160/1645 [05:17<48:17,  1.95s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 161/1645 [05:19<47:53,  1.94s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 162/1645 [05:21<47:31,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 163/1645 [05:23<47:11,  1.91s/it]\u001b[A\n",
      "Iteration:  10%|▉         | 164/1645 [05:25<46:58,  1.90s/it]\u001b[A\n",
      "Iteration:  10%|█         | 165/1645 [05:27<46:42,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 166/1645 [05:28<46:32,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 167/1645 [05:30<46:30,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 168/1645 [05:32<46:30,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 169/1645 [05:34<46:28,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 170/1645 [05:36<46:30,  1.89s/it]\u001b[A\n",
      "Iteration:  10%|█         | 171/1645 [05:38<47:07,  1.92s/it]\u001b[A\n",
      "Iteration:  10%|█         | 172/1645 [05:40<47:05,  1.92s/it]\u001b[A\n",
      "Iteration:  11%|█         | 173/1645 [05:42<46:43,  1.90s/it]\u001b[A\n",
      "Iteration:  11%|█         | 174/1645 [05:44<46:31,  1.90s/it]\u001b[A\n",
      "Iteration:  11%|█         | 175/1645 [05:46<46:21,  1.89s/it]\u001b[A\n",
      "Iteration:  11%|█         | 176/1645 [05:47<46:43,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 177/1645 [05:49<46:32,  1.90s/it]\u001b[A\n",
      "Iteration:  11%|█         | 178/1645 [05:51<46:28,  1.90s/it]\u001b[A\n",
      "Iteration:  11%|█         | 179/1645 [05:53<46:19,  1.90s/it]\u001b[A\n",
      "Iteration:  11%|█         | 180/1645 [05:55<46:09,  1.89s/it]\u001b[A\n",
      "Iteration:  11%|█         | 181/1645 [05:57<46:26,  1.90s/it]\u001b[A\n",
      "Iteration:  11%|█         | 182/1645 [05:59<46:47,  1.92s/it]\u001b[A\n",
      "Iteration:  11%|█         | 183/1645 [06:01<46:33,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 184/1645 [06:03<46:29,  1.91s/it]\u001b[A\n",
      "Iteration:  11%|█         | 185/1645 [06:05<47:33,  1.95s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 186/1645 [06:07<46:55,  1.93s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 187/1645 [06:09<46:37,  1.92s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 188/1645 [06:10<46:38,  1.92s/it]\u001b[A\n",
      "Iteration:  11%|█▏        | 189/1645 [06:12<46:21,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 190/1645 [06:14<47:22,  1.95s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 191/1645 [06:16<46:58,  1.94s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 192/1645 [06:18<46:25,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 193/1645 [06:20<46:32,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 194/1645 [06:22<46:19,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 195/1645 [06:24<46:09,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 196/1645 [06:26<46:28,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 197/1645 [06:28<46:13,  1.92s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 198/1645 [06:30<45:55,  1.90s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 199/1645 [06:32<45:47,  1.90s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 200/1645 [06:33<45:50,  1.90s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 201/1645 [06:35<45:43,  1.90s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 202/1645 [06:37<45:35,  1.90s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 203/1645 [06:39<45:29,  1.89s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 204/1645 [06:41<45:54,  1.91s/it]\u001b[A\n",
      "Iteration:  12%|█▏        | 205/1645 [06:43<46:15,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 206/1645 [06:45<46:20,  1.93s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 207/1645 [06:47<47:47,  1.99s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 208/1645 [06:49<47:00,  1.96s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 209/1645 [06:51<46:23,  1.94s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 210/1645 [06:53<45:56,  1.92s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 211/1645 [06:55<45:37,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 212/1645 [06:57<45:29,  1.91s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 213/1645 [06:58<45:25,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 214/1645 [07:00<45:23,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 215/1645 [07:02<45:15,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 216/1645 [07:04<45:15,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 217/1645 [07:06<45:10,  1.90s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 218/1645 [07:08<45:02,  1.89s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 219/1645 [07:10<44:53,  1.89s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 220/1645 [07:12<44:46,  1.89s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 221/1645 [07:14<44:46,  1.89s/it]\u001b[A\n",
      "Iteration:  13%|█▎        | 222/1645 [07:15<44:44,  1.89s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 223/1645 [07:17<44:35,  1.88s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 224/1645 [07:19<44:28,  1.88s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 225/1645 [07:21<44:41,  1.89s/it]\u001b[A\n",
      "Iteration:  14%|█▎        | 226/1645 [07:23<44:55,  1.90s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 227/1645 [07:25<44:42,  1.89s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 228/1645 [07:27<45:30,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 229/1645 [07:29<46:08,  1.96s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 230/1645 [07:31<45:41,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 231/1645 [07:33<45:23,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 232/1645 [07:35<45:10,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 233/1645 [07:37<44:52,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 234/1645 [07:38<45:25,  1.93s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 235/1645 [07:40<45:04,  1.92s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 236/1645 [07:42<44:53,  1.91s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 237/1645 [07:44<45:27,  1.94s/it]\u001b[A\n",
      "Iteration:  14%|█▍        | 238/1645 [07:46<45:10,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 239/1645 [07:48<44:57,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 240/1645 [07:50<44:46,  1.91s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 241/1645 [07:52<44:27,  1.90s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 242/1645 [07:54<45:01,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 243/1645 [07:56<45:00,  1.93s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 244/1645 [07:58<44:48,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 245/1645 [08:00<44:42,  1.92s/it]\u001b[A\n",
      "Iteration:  15%|█▍        | 246/1645 [08:02<45:29,  1.95s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 247/1645 [08:04<46:40,  2.00s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 248/1645 [08:06<45:59,  1.98s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 249/1645 [08:08<45:31,  1.96s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 250/1645 [08:09<45:07,  1.94s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 251/1645 [08:11<45:03,  1.94s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 252/1645 [08:14<46:34,  2.01s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 253/1645 [08:16<46:37,  2.01s/it]\u001b[A\n",
      "Iteration:  15%|█▌        | 254/1645 [08:17<45:41,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 255/1645 [08:19<45:07,  1.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:  16%|█▌        | 256/1645 [08:22<50:47,  2.19s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 257/1645 [08:24<49:02,  2.12s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 258/1645 [08:26<47:54,  2.07s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 259/1645 [08:28<46:30,  2.01s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 260/1645 [08:30<46:00,  1.99s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 261/1645 [08:32<45:17,  1.96s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 262/1645 [08:34<44:46,  1.94s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 263/1645 [08:36<44:28,  1.93s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 264/1645 [08:37<44:06,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 265/1645 [08:39<44:04,  1.92s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 266/1645 [08:42<47:08,  2.05s/it]\u001b[A\n",
      "Iteration:  16%|█▌        | 267/1645 [08:44<45:56,  2.00s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 268/1645 [08:46<46:34,  2.03s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 269/1645 [08:48<45:47,  2.00s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 270/1645 [08:50<45:07,  1.97s/it]\u001b[A\n",
      "Iteration:  16%|█▋        | 271/1645 [08:51<44:35,  1.95s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 272/1645 [08:53<45:14,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 273/1645 [08:55<44:30,  1.95s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 274/1645 [08:58<46:03,  2.02s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 275/1645 [08:59<45:15,  1.98s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 276/1645 [09:01<44:39,  1.96s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 277/1645 [09:03<44:01,  1.93s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 278/1645 [09:05<43:48,  1.92s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 279/1645 [09:07<43:36,  1.92s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 280/1645 [09:09<43:17,  1.90s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 281/1645 [09:11<43:07,  1.90s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 282/1645 [09:13<42:53,  1.89s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 283/1645 [09:14<42:51,  1.89s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 284/1645 [09:16<43:01,  1.90s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 285/1645 [09:18<42:48,  1.89s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 286/1645 [09:20<42:49,  1.89s/it]\u001b[A\n",
      "Iteration:  17%|█▋        | 287/1645 [09:22<42:38,  1.88s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 288/1645 [09:24<42:37,  1.89s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 289/1645 [09:26<42:40,  1.89s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 290/1645 [09:28<42:38,  1.89s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 291/1645 [09:30<42:42,  1.89s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 292/1645 [09:32<42:45,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 293/1645 [09:33<42:41,  1.89s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 294/1645 [09:35<42:47,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 295/1645 [09:37<42:50,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 296/1645 [09:39<42:40,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 297/1645 [09:41<42:35,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 298/1645 [09:43<42:36,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 299/1645 [09:45<42:36,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 300/1645 [09:47<42:38,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 301/1645 [09:49<42:35,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 302/1645 [09:51<42:33,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 303/1645 [09:52<42:24,  1.90s/it]\u001b[A\n",
      "Iteration:  18%|█▊        | 304/1645 [09:54<42:10,  1.89s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 305/1645 [09:56<42:00,  1.88s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 306/1645 [09:58<41:58,  1.88s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 307/1645 [10:00<42:02,  1.89s/it]\u001b[A\n",
      "Iteration:  19%|█▊        | 308/1645 [10:02<42:14,  1.90s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 309/1645 [10:04<42:28,  1.91s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 310/1645 [10:06<42:27,  1.91s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 311/1645 [10:08<42:17,  1.90s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 312/1645 [10:09<42:10,  1.90s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 313/1645 [10:11<42:15,  1.90s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 314/1645 [10:13<42:04,  1.90s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 315/1645 [10:15<42:11,  1.90s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 316/1645 [10:17<42:05,  1.90s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 317/1645 [10:19<43:16,  1.96s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 318/1645 [10:21<42:59,  1.94s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 319/1645 [10:23<42:35,  1.93s/it]\u001b[A\n",
      "Iteration:  19%|█▉        | 320/1645 [10:25<42:26,  1.92s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 321/1645 [10:27<42:14,  1.91s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 322/1645 [10:29<42:03,  1.91s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 323/1645 [10:31<41:52,  1.90s/it]\u001b[A\n",
      "Iteration:  20%|█▉        | 324/1645 [10:32<41:41,  1.89s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,1,2 classes (irrelevant - misleading)\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    #print(outputs,outputs == labels)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "# group data by question -> we want to look at the accuracy and f1 score for each question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForMultipleChoice, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "# model = DistilBertForMultipleChoice.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distil_model():\n",
    "    \n",
    "    for name, param in distil_model.named_parameters():\n",
    "#         param.requires_grad = False\n",
    "        ln = 24\n",
    "        if name.startswith('distilbert.encoder'):\n",
    "        \tl = name.split('.')\n",
    "        \tln = int(l[3])\n",
    "      \n",
    "        if name.startswith('distilbert.embeddings') or ln < 6:\n",
    "#         \tprint(name)  \n",
    "        \tparam.requires_grad = False\n",
    "    \n",
    "    distil_model.to(device)\n",
    "    \n",
    "    return distil_model\n",
    "\n",
    "# from utils.tokenization import BertTokenizer tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modeling import BertEmbeddings\n",
    "\n",
    "class RaceDistilBert(DistilBertModel):\n",
    "    def __init__(self, config):\n",
    "        super(RaceDistilBert, self).__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = self.transformer\n",
    "    \n",
    "    def forward(self, input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=None,\n",
    "                output_all_encoded_layers=True,\n",
    "                inputs_embeds=None):\n",
    "        '''overwrite forward method'''\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        P_att = torch.zeros_like(input_ids)\n",
    "        Q_att = torch.zeros_like(input_ids)\n",
    "        A_att = torch.zeros_like(input_ids)\n",
    "        token_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        P_att[token_type_ids==0] = 1\n",
    "        Q_att[token_type_ids==1] = 1\n",
    "        A_att[token_type_ids==2] = 1\n",
    "\n",
    "        token_ids[token_type_ids > 0] = 1\n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        P_att = P_att.unsqueeze(1).unsqueeze(2)\n",
    "        P_att = P_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        P_att = (1.0 - P_att) * -10000.0\n",
    "\n",
    "        Q_att = Q_att.unsqueeze(1).unsqueeze(2)\n",
    "        Q_att = Q_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        Q_att = (1.0 - Q_att) * -10000.0\n",
    "\n",
    "        A_att = A_att.unsqueeze(1).unsqueeze(2)\n",
    "        A_att = A_att.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        A_att = (1.0 - A_att) * -10000.0\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            embedding_output = self.embeddings(input_ids, token_ids)\n",
    "        else:\n",
    "            embedding_output = inputs_embeds\n",
    "#             print(f'shape inputs_embeds: {len(inputs_embeds)}, and shape embeddings: {len(self.embeddings(input_ids, token_ids))}')\n",
    "\n",
    "#         self.noise = self.generate_noise(embedding_output, attention_mask[0], epsilon=1e-5)\n",
    "        \n",
    "        print(f'embedding output: {embedding_output}')\n",
    "        return embedding_output, extended_attention_mask,output_all_encoded_layers\n",
    "        try:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_all_encoded_layers=output_all_encoded_layers)\n",
    "        except TypeError as e:\n",
    "            encoded_layers = self.encoder(embedding_output,\n",
    "                                          extended_attention_mask,\n",
    "                                          output_hidden_states=output_all_encoded_layers)\n",
    "        \n",
    "#         self.adv_encoded_layers = self.encoder(embedding_output+self.noise,\n",
    "#                                       extended_attention_mask,\n",
    "#                                       output_all_encoded_layers=output_all_encoded_layers)\n",
    "        \n",
    "        \n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        if not output_all_encoded_layers:\n",
    "            encoded_layers = encoded_layers[-1]\n",
    "\n",
    "        return extended_attention_mask,P_att,Q_att,A_att,sequence_output\n",
    "    \n",
    "class RaceDistilBertMultipleChoice(DistilBertForMultipleChoice):\n",
    "    def __init__(self, config):\n",
    "        super(RaceDistilBertMultipleChoice, self).__init__(config)\n",
    "        self.distilbert = dmodel\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = RaceDistilBert.from_pretrained('distilbert-base-uncased')\n",
    "dmodel_mc = RaceDistilBertMultipleChoice.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel_mc.distilbert.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbert = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMultipleChoice, BertModel\n",
    "from utils.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "distil_model = RaceDistilBertMultipleChoice.from_pretrained('distilbert-base-uncased',\n",
    "                                              cache_dir=os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtrain():\n",
    "    default_config = {\n",
    "             'multi_gpu_on':False,\n",
    "             'max_seq_length': 64,\n",
    "             'adam_eps': 6, \n",
    "             'adv_epsilon': 1e-6,\n",
    "             'adv_train': 1,\n",
    "             'adv_noise_var': 1e-5,\n",
    "             'adv_norm_level': 0,\n",
    "             'adv_step_size': 1e-3,\n",
    "             'bin_on': False,\n",
    "             'cuda': 1,\n",
    "             'encoder_type': None,\n",
    "             'fp16': True,\n",
    "             'fp16_opt_level': 'O1',\n",
    "             'global_grad_clipping': 1.0,\n",
    "             'grad_accumulation_step': 1,\n",
    "             'grad_clipping': 0,\n",
    "             'local_rank': -1,\n",
    "             'mkd_opt': 0,\n",
    "             'scheduler_type': 'ms',\n",
    "             'task_def_list': None,\n",
    "             'warmup': 0.1,\n",
    "             'warmup_schedule': 'warmup_linear',\n",
    "             'weight_decay': 0,\n",
    "             'weighted_on': False,\n",
    "             'state_dict': None,\n",
    "             'loss': 'LossCriterion.CeCriterion',\n",
    "             'epochs': 3,\n",
    "             'batch_size': 4,\n",
    "             'adv_k': 1,\n",
    "             'learning_rate':5e-5,\n",
    "             'adv_p_norm': 'inf',\n",
    "             'adv_alpha': 1,\n",
    "             'optimizer': 'radam',\n",
    "             'adv_loss': 'LossCriterion.SymKlCriterion',\n",
    "             'weight': 0\n",
    "    }\n",
    "    \n",
    "    wandb.init(project=\"master-thesis\", config=default_config)\n",
    "    \n",
    "    device = default_config['cuda']\n",
    "    \n",
    "    config = wandb.config\n",
    "    \n",
    "    # build_dataset\n",
    "    train_dataloader, eval_dataloader = build_dataset(config)\n",
    "    \n",
    "    # create model\n",
    "    model = create_distil_model()\n",
    "    \n",
    "    # initialize model and losses\n",
    "    [mnetwork,\n",
    "     task_loss_criterion,\n",
    "     adv_task_loss_criterion,\n",
    "     adv_teacher,\n",
    "     optimizer_parameters,\n",
    "     optimizer,\n",
    "     scheduler] = _model_init(config=config, model=model, num_train_step=len(train_dataloader))\n",
    "    \n",
    "    return [config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler]\n",
    "\n",
    "    # training mode ON\n",
    "    model.train()\n",
    "    \n",
    "    # wandb watch\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    # train/eval\n",
    "    training_loop(config,\n",
    "                  train_dataloader,\n",
    "                  eval_dataloader,\n",
    "                  mnetwork,\n",
    "                  task_loss_criterion,\n",
    "                  adv_task_loss_criterion,\n",
    "                  adv_teacher,\n",
    "                  optimizer_parameters,\n",
    "                  optimizer,\n",
    "                  scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dtrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=training_loop(*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = params[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.distilbert.embeddings(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DistilBertForMultipleChoice.from_pretrained('distilbert-base-uncased')\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.distilbert.embeddings(input_ids[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_untrained = DistilBertForMultipleChoice(net.config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_untrained.distilbert.embeddings(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(net.distilbert.embeddings.forward), help(network.distilbert.embeddings.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(distil_model.to(device).distilbert.embeddings(input_ids[0]).cpu().detach().numpy() - net_untrained.distilbert.embeddings(input_ids[0]).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.distilbert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'large_models'\n",
    "\n",
    "# Save a trained model, configuration and tokenizer\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "# If we save using the predefined names, we can load using `from_pretrained`\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_train_loss, test_train_accuracy = 0, 0\n",
    "nb_test_train_steps, nb_test_train_examples = 0, 0\n",
    "test_train_total_logits = []\n",
    "test_train_total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(train_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            test_train_total_logits.append(logits)\n",
    "            test_train_total_labels.append(label_ids)\n",
    "\t\n",
    "test_train_total_logits = np.concatenate(total_logits)\n",
    "test_train_total_labels = np.concatenate(total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "# np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"train_acc_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_examples = read_race(\"./RACE/dev\")\n",
    "# eval_features = convert_examples_to_features(\n",
    "#             eval_examples, tokenizer, args.max_seq_length, True)\n",
    "#         logger.info(\"***** Running evaluation *****\")\n",
    "#         logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "#         logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "#         all_input_ids = torch.tensor(select_field(eval_features, 'input_ids'), dtype=torch.long)\n",
    "#         all_input_mask = torch.tensor(select_field(eval_features, 'input_mask'), dtype=torch.long)\n",
    "#         all_segment_ids = torch.tensor(select_field(eval_features, 'segment_ids'), dtype=torch.long)\n",
    "\n",
    "#         all_label = torch.tensor([f.label for f in eval_features], dtype=torch.long)\n",
    "#         eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
    "        # Run prediction for full data\n",
    "\n",
    "if True:\n",
    "    dev_examples = read_data(data_location,'dev')\n",
    "    test_examples = read_data(data_location,'test')\n",
    "\n",
    "if True:\n",
    "    test_features = convert_examples_to_features(\n",
    "                test_examples, tokenizer, max_seq_length, True)\n",
    "    dev_features = convert_examples_to_features(\n",
    "            dev_examples, tokenizer, max_seq_length, True)\n",
    "                \n",
    "if True:\n",
    "    test_data = build_tensor(test_features)\n",
    "    dev_data = build_tensor(dev_features)\n",
    "eval_sampler = SequentialSampler(dev_data)\n",
    "eval_dataloader = DataLoader(dev_data, sampler=eval_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "total_logits = np.concatenate(total_logits)\n",
    "total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/logits.npy\",total_logits)\n",
    "np.save(output_dir+\"/labels.npy\",total_labels)\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy}\n",
    "\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_total_logits = [items for labels in total_logits for items in labels]\n",
    "assembled_total_logits = np.array(assembled_total_logits)\n",
    "\n",
    "assembled_total_labels = [items for labels in total_labels for items in labels]\n",
    "assembled_total_labels = np.array(assembled_total_labels)\n",
    "assembled_total_labels, assembled_total_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report as cls_report\n",
    "print(cls_report(test_train_total_labels, np.argmax(test_train_total_logits, axis=1)))\n",
    "print(cls_report(assembled_total_labels, np.argmax(assembled_total_logits, axis=1)))\n",
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "with open(output_test_file, \"w\") as writer:\n",
    "#             logger.info(\"***** Test results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "#                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('identification fails'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "chosen_features = convert_examples_to_features(chosen_examples, tokenizer, max_seq_length, is_training=False, debug=False)\n",
    "\n",
    "chosen_data = build_tensor(chosen_features)\n",
    "\n",
    "chosen_sampler = SequentialSampler(chosen_data)\n",
    "chosen_dataloader = DataLoader(chosen_data, sampler=chosen_sampler, batch_size=32)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(chosen_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "# output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "# with open(output_test_file, \"w\") as writer:\n",
    "# #             logger.info(\"***** Test results *****\")\n",
    "#             for key in sorted(result.keys()):\n",
    "# #                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                 writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_total_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(train_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('i cannot find my ope'):\n",
    "        chosen_examples.append(example)\n",
    "        print(example.endings[example.label])\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('i cannot find my ope'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features = convert_examples_to_features(chosen_examples, tokenizer, max_seq_length, is_training=False, debug=False)\n",
    "\n",
    "chosen_data = build_tensor(chosen_features)\n",
    "\n",
    "chosen_sampler = SequentialSampler(chosen_data)\n",
    "chosen_dataloader = DataLoader(chosen_data, sampler=chosen_sampler, batch_size=32)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "tr_loss = 0\n",
    "test_loss, test_accuracy = 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "total_logits = []\n",
    "total_labels = []\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(chosen_dataloader, desc=\"Testing\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tmp_test_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = label_ids.to('cpu').numpy()\n",
    "            tmp_test_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "            test_loss += tmp_test_loss.mean().item()\n",
    "            test_accuracy += tmp_test_accuracy\n",
    "\n",
    "            nb_test_examples += input_ids.size(0)\n",
    "            nb_test_steps += 1\n",
    "            \n",
    "            total_logits.append(logits)\n",
    "            total_labels.append(label_ids)\n",
    "\t\n",
    "test_total_logits = np.concatenate(total_logits)\n",
    "test_total_labels = np.concatenate(total_labels)\n",
    "\n",
    "np.save(output_dir+\"/test_logits.npy\",test_total_logits)\n",
    "np.save(output_dir+\"/test_labels.npy\",test_total_labels)\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_accuracy = test_accuracy / nb_test_examples\n",
    "\n",
    "result = {'test_loss': test_loss,\n",
    "          'test_accuracy': test_accuracy}\n",
    "\n",
    "# output_test_file = os.path.join(output_dir, \"test_results.txt\")\n",
    "# with open(output_test_file, \"w\") as writer:\n",
    "# #             logger.info(\"***** Test results *****\")\n",
    "#             for key in sorted(result.keys()):\n",
    "# #                 logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#                 writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_report(test_total_labels, np.argmax(test_total_logits, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(test_total_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_examples = []\n",
    "for example in tqdm(test_examples, desc=\"Testing\"):\n",
    "    if example.endings[example.label].startswith('The {FASTA}'):\n",
    "        chosen_examples.append(example)\n",
    "\n",
    "len(chosen_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [example.context_sentence for example in chosen_examples]\n",
    "distribution = [example.label for example in chosen_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(contexts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
